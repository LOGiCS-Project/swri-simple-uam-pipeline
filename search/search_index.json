{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SimpleUAM : Tools for SWRi's UAV and UAM Workflow \u00b6 SimpleUAM is a set of python libraries and command-line tools for working with SWRi's pipeline for UAV development. Its main goals are to make the SWRi pipeline easy to deploy and wrap it with convenient interfaces for external tools. See Github Pages for more details... Organization \u00b6 SimpleUAM Component Structure SimpleUAM organizes itself into components, each of which perform some basic task needed to evaluate UAV or UAM designs: Client Nodes : Makes requests for UAV/UAM design analysis. Message Broker : Distributes analysis requests from clients to available workers. Worker Nodes : Analyzes designs with SWRi's pipelines, placing the results into storage. Engineering Corpus : Provides component and design data to the worker nodes during analysis. License Servers : Provides floating Creo licenses to workers. Results Storage : Store the results of design analyses on the file system, whether it's in a local folder or a network drive. These components aren't tied to specific machines and can all happily coexist on a single computer or support multiple servers cooperating on each task. This project provides support for versions of all of these nodes including setup scripts and python libraries for interacting with them. Links \u00b6 Install Instructions Configuration Instructions Repo Organization \u00b6 <repo-root> \u251c\u2500\u2500 LICENSE # License File \u251c\u2500\u2500 README.md # Readme File \u251c\u2500\u2500 pyproject.toml # Project Setup Info. For use with pdm, poetry, or similar. \u251c\u2500\u2500 mkdocs.yml # Documentation Configuration via mkdocs \u2502 \u251c\u2500\u2500 config/ # Config files for type checking, linting, etc... \u251c\u2500\u2500 docs/ # Source for gh-pages documentation. \u2514\u2500\u2500 src/ # Python source root","title":"Overview"},{"location":"#simpleuam-tools-for-swris-uav-and-uam-workflow","text":"SimpleUAM is a set of python libraries and command-line tools for working with SWRi's pipeline for UAV development. Its main goals are to make the SWRi pipeline easy to deploy and wrap it with convenient interfaces for external tools. See Github Pages for more details...","title":"SimpleUAM : Tools for SWRi's UAV and UAM Workflow"},{"location":"#organization","text":"SimpleUAM Component Structure SimpleUAM organizes itself into components, each of which perform some basic task needed to evaluate UAV or UAM designs: Client Nodes : Makes requests for UAV/UAM design analysis. Message Broker : Distributes analysis requests from clients to available workers. Worker Nodes : Analyzes designs with SWRi's pipelines, placing the results into storage. Engineering Corpus : Provides component and design data to the worker nodes during analysis. License Servers : Provides floating Creo licenses to workers. Results Storage : Store the results of design analyses on the file system, whether it's in a local folder or a network drive. These components aren't tied to specific machines and can all happily coexist on a single computer or support multiple servers cooperating on each task. This project provides support for versions of all of these nodes including setup scripts and python libraries for interacting with them.","title":"Organization"},{"location":"#links","text":"Install Instructions Configuration Instructions","title":"Links"},{"location":"#repo-organization","text":"<repo-root> \u251c\u2500\u2500 LICENSE # License File \u251c\u2500\u2500 README.md # Readme File \u251c\u2500\u2500 pyproject.toml # Project Setup Info. For use with pdm, poetry, or similar. \u251c\u2500\u2500 mkdocs.yml # Documentation Configuration via mkdocs \u2502 \u251c\u2500\u2500 config/ # Config files for type checking, linting, etc... \u251c\u2500\u2500 docs/ # Source for gh-pages documentation. \u2514\u2500\u2500 src/ # Python source root","title":"Repo Organization"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Environment setup \u00b6 Nothing easier! Fork and clone the repository, then: cd swri-simple-uam-pipleline make setup Note If it fails for some reason, you'll need to install PDM manually. You can install it with: python3 -m pip install --user pipx pipx install pdm Now you can try running make setup again, or simply pdm install . You now have the dependencies installed. You can run the application with pdm run simple-uam [ARGS...] . Run make help to see all the available actions! Tasks \u00b6 This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with pdm run duty TASK The Makefile detects if a virtual environment is activated, so make will work the same with the virtualenv activated or not. Development \u00b6 As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) if you updated the documentation or the project dependencies: run make docs-serve go to http://localhost:8000 and check that everything looks good follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this. Commit message convention \u00b6 Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15. Pull requests guidelines \u00b6 Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Contributing"},{"location":"contributing/#environment-setup","text":"Nothing easier! Fork and clone the repository, then: cd swri-simple-uam-pipleline make setup Note If it fails for some reason, you'll need to install PDM manually. You can install it with: python3 -m pip install --user pipx pipx install pdm Now you can try running make setup again, or simply pdm install . You now have the dependencies installed. You can run the application with pdm run simple-uam [ARGS...] . Run make help to see all the available actions!","title":"Environment setup"},{"location":"contributing/#tasks","text":"This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with pdm run duty TASK The Makefile detects if a virtual environment is activated, so make will work the same with the virtualenv activated or not.","title":"Tasks"},{"location":"contributing/#development","text":"As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) if you updated the documentation or the project dependencies: run make docs-serve go to http://localhost:8000 and check that everything looks good follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this.","title":"Development"},{"location":"contributing/#commit-message-convention","text":"Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15.","title":"Commit message convention"},{"location":"contributing/#pull-requests-guidelines","text":"Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Pull requests guidelines"},{"location":"license/","text":"MIT License Copyright (c) 2022 LOGiCS-Project Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"reference/SUMMARY/","text":"simple_uam craidl corpus abstract get_corpus gremlin static info_files direct2cad actors manager session workspace tools config_mgr cli craidl cli examples stub_server tasks d2c_client cli tasks d2c_worker cli service worker d2c_workspace cli env manage tasks dev_util cli tasks setup_lin broker cli license_server shared setup_win broker choco cli graph_server helpers license_server shared worker util config craidl_config d2c_worker_config d2c_workspace_config lin_setup_config manager path_config service_config tasks win_setup_config workspace_config invoke program logging logger system backup git nssm pip rsync windows worker broker run_worker workspace manager session workspace","title":"SUMMARY"},{"location":"reference/simple_uam/","text":"SimpleUAM windows node setup scripts.","title":"simple_uam"},{"location":"reference/simple_uam/craidl/","text":"SimpleUAM windows node setup scripts.","title":"craidl"},{"location":"reference/simple_uam/craidl/info_files/","text":"DesignInfoFiles \u00b6 Source code in simple_uam/craidl/info_files.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 @frozen class DesignInfoFiles (): corpus : CorpusReader = field () \"\"\" Corpus being used during generation. \"\"\" design : dict = field () \"\"\" Design rep we're generating info files for. \"\"\" design_to_corpus : dict = field ( init = False , ) \"\"\" Map from design component instances to corpus component classes. \"\"\" @design_to_corpus . default def _cdcm_default ( self ): cdcm = dict () for comp_entry in self . design [ 'components' ]: from_comp = comp_entry [ 'component_instance' ] lib_component = comp_entry [ 'component_choice' ] cdcm [ from_comp ] = lib_component return cdcm component_maps : List [ dict ] = field ( init = False , ) \"\"\" Map of components and cad files. \"\"\" component_maps_file = 'info_componentMapList1.json' @component_maps . default def _cmp_maps_default ( self ): cmp_maps = list () for from_comp , lib_comp in self . design_to_corpus . items (): cad_prt = self . corpus [ lib_comp ] . cad_part new_entry = { 'FROM_COMP' : from_comp , 'LIB_COMPONENT' : lib_comp , 'CAD_PRT' : cad_prt } if any_none ( from_comp , lib_comp , cad_prt ): log . warning ( 'Skipping entry in component_maps due to null values' , ** new_entry , ) else : cmp_maps . append ( new_entry ) return cmp_maps connection_maps : List [ dict ] = field ( init = False , ) \"\"\" List of connections in the design. \"\"\" connection_maps_file = 'info_connectionMap6.json' @connection_maps . default def _conn_maps_def ( self ): conn_maps = list () for conn_entry in self . design [ 'connections' ]: from_comp = conn_entry [ 'from_ci' ] from_conn = conn_entry [ 'from_conn' ] to_comp = conn_entry [ 'to_ci' ] to_conn = conn_entry [ 'to_conn' ] new_entry = { 'FROM_COMP' : from_comp , 'FROM_CONN' : from_conn , 'TO_COMP' : to_comp , 'TO_CONN' : to_conn } if any_none ( from_comp , from_conn , to_comp , to_conn ): log . warning ( 'Skipping entry in connectionMap due to null values' , comp_entry = conn_entry , ** new_entry , ) else : conn_maps . append ( new_entry ) return conn_maps param_maps : List [ dict ] = field ( init = False , ) param_maps_file = \"info_paramMap4.json\" @param_maps . default def _param_maps_default ( self ): param_maps = list () for param_entry in self . design [ 'parameters' ]: design_param = param_entry [ 'parameter_name' ] design_param_val = param_entry [ 'value' ] for target in param_entry [ 'component_properties' ]: component_name = target [ 'component_name' ] component_param = target [ 'component_property' ] new_entry = { 'DESIGN_PARAM' : design_param , 'DESIGN_PARAM_VAL' : design_param_val , 'COMPONENT_NAME' : component_name , 'COMPONENT_PARAM' : component_param } if any_none ( design_param , design_param_val , component_name , component_param ): log . warning ( 'Skipping entry in paramMap due to null values' , comp_entry = comp_entry , ** new_entry , ) else : param_maps . append ( new_entry ) return param_maps cad_properties : List [ dict ] = field ( init = False , ) cad_properties_file = \"info_componentCadProps2.json\" @cad_properties . default def _cad_props_default ( self ): cad_props = list () for entry in self . component_maps : # NOTE : needs component map list comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] cad_part = entry [ 'CAD_PRT' ] prop_vals = self . corpus [ lib_name ] . cad_properties for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'CAD_PART' : cad_part , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_value ): log . warning ( 'Skipping entry in paramMap due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : cad_props . append ( new_entry ) return cad_props cad_connections : List [ dict ] = field ( init = False , ) cad_connections_file = \"info_connectionCADMap3.json\" @cad_connections . default def _cad_conn_default ( self ): cad_conns = list () for entry in self . connection_maps : ## NOTE: need connmap from_comp = entry [ 'FROM_COMP' ] to_comp = entry [ 'TO_COMP' ] from_conn = entry [ 'FROM_CONN' ] to_conn = entry [ 'TO_CONN' ] from_comp_type = self . design_to_corpus [ from_comp ] ## NOTE: needs ci_to_comp to_comp_type = self . design_to_corpus [ to_comp ] from_conn_cs = self . corpus [ from_comp_type ] . cad_connection ( from_conn ) to_conn_cs = self . corpus [ to_comp_type ] . cad_connection ( to_conn ) new_entry = { 'FROM_COMP' : from_comp , 'FROM_CONN_CS' : from_conn_cs , 'TO_COMP' : to_comp , 'TO_CONN_CS' : to_conn_cs } if any_none ( from_conn_cs and to_conn_cs ): log . warning ( 'Skipping entry in connectionCadMap due to null values' , entry = entry , ** new_entry , ) else : cad_conns . append ( new_entry ) return cad_conns component_props : List [ dict ] = field ( init = False , ) component_props_file = \"info_componentProps7.json\" @component_props . default def _comp_props_default ( self ): comp_props = list () for entry in self . component_maps : comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] prop_vals = self . corpus [ lib_name ] . properties for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_val ): log . warning ( 'Skipping entry in componentProps due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : comp_props . append ( new_entry ) return comp_props component_params : List [ dict ] = field ( init = False , ) component_params_file = 'info_componentParams8.json' @component_params . default def _comp_params_default ( self ): comp_params = list () for entry in self . component_maps : comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] prop_vals = self . corpus [ lib_name ] . params for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_value ): log . warning ( 'Skipping entry in componentParams due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : comp_params . append ( new_entry ) return comp_params cad_params : List [ dict ] = field ( init = False , ) cad_params_file = 'info_componentCADParams5.json' @cad_params . default def _cad_params ( self ): cad_params = list () for entry in self . component_maps : ## NOTE: ComponentMapList comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] cad_part = entry [ 'CAD_PRT' ] prop_vals = self . corpus [ lib_name ] . cad_params for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'CAD_PART' : cad_part , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_value ): log . warning ( 'Skipping entry in componentCadParams due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : cad_params . append ( new_entry ) return cad_params @property def info_file_map ( self ): \"\"\" Map from filename to file data. \"\"\" return { self . component_maps_file : self . component_maps , self . connection_maps_file : self . connection_maps , self . param_maps_file : self . param_maps , self . cad_properties_file : self . cad_properties , self . cad_connections_file : self . cad_connections , self . component_props_file : self . component_props , self . component_params_file : self . component_params , self . cad_params_file : self . cad_params , } def write_files ( self , out_dir ): \"\"\" Write all the info files to the output directory. \"\"\" out_dir = Path ( out_dir ) out_dir . mkdir ( parents = True , exist_ok = True ) for filename , content in self . info_file_map . items (): filepath = out_dir / filename with filepath . open ( 'w' ) as fp : json . dump ( content , fp , indent = \" \" ) @classmethod def load_design ( cls , corpus , fp , ** kwargs ): \"\"\" Load from filepointer to design. Arguments: corpus: The corpus accessor to use. fp: the file pointer to load from. **kwargs: same as json.load's kwargs \"\"\" design_rep = json . load ( fp , ** kwargs ) return cls ( corpus , design_rep ) @classmethod def load_design_str ( cls , corpus , s , ** kwargs ): \"\"\" Load from a json string of a design. Arguments: corpus: The corpus accessor to use. s: the json string to load from. **kwargs: same as json.loads's kwargs \"\"\" design_rep = json . loads ( s , ** kwargs ) return cls ( corpus , design_rep ) component_maps : List [ dict ] = field ( init = False ) class-attribute \u00b6 Map of components and cad files. connection_maps : List [ dict ] = field ( init = False ) class-attribute \u00b6 List of connections in the design. corpus : CorpusReader = field () class-attribute \u00b6 Corpus being used during generation. design : dict = field () class-attribute \u00b6 Design rep we're generating info files for. design_to_corpus : dict = field ( init = False ) class-attribute \u00b6 Map from design component instances to corpus component classes. info_file_map () property \u00b6 Map from filename to file data. Source code in simple_uam/craidl/info_files.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @property def info_file_map ( self ): \"\"\" Map from filename to file data. \"\"\" return { self . component_maps_file : self . component_maps , self . connection_maps_file : self . connection_maps , self . param_maps_file : self . param_maps , self . cad_properties_file : self . cad_properties , self . cad_connections_file : self . cad_connections , self . component_props_file : self . component_props , self . component_params_file : self . component_params , self . cad_params_file : self . cad_params , } load_design ( corpus , fp , ** kwargs ) classmethod \u00b6 Load from filepointer to design. Parameters: Name Type Description Default corpus The corpus accessor to use. required fp the file pointer to load from. required **kwargs same as json.load's kwargs {} Source code in simple_uam/craidl/info_files.py 331 332 333 334 335 336 337 338 339 340 341 342 343 @classmethod def load_design ( cls , corpus , fp , ** kwargs ): \"\"\" Load from filepointer to design. Arguments: corpus: The corpus accessor to use. fp: the file pointer to load from. **kwargs: same as json.load's kwargs \"\"\" design_rep = json . load ( fp , ** kwargs ) return cls ( corpus , design_rep ) load_design_str ( corpus , s , ** kwargs ) classmethod \u00b6 Load from a json string of a design. Parameters: Name Type Description Default corpus The corpus accessor to use. required s the json string to load from. required **kwargs same as json.loads's kwargs {} Source code in simple_uam/craidl/info_files.py 345 346 347 348 349 350 351 352 353 354 355 356 357 @classmethod def load_design_str ( cls , corpus , s , ** kwargs ): \"\"\" Load from a json string of a design. Arguments: corpus: The corpus accessor to use. s: the json string to load from. **kwargs: same as json.loads's kwargs \"\"\" design_rep = json . loads ( s , ** kwargs ) return cls ( corpus , design_rep ) write_files ( out_dir ) \u00b6 Write all the info files to the output directory. Source code in simple_uam/craidl/info_files.py 317 318 319 320 321 322 323 324 325 326 327 328 329 def write_files ( self , out_dir ): \"\"\" Write all the info files to the output directory. \"\"\" out_dir = Path ( out_dir ) out_dir . mkdir ( parents = True , exist_ok = True ) for filename , content in self . info_file_map . items (): filepath = out_dir / filename with filepath . open ( 'w' ) as fp : json . dump ( content , fp , indent = \" \" )","title":"info_files"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles","text":"Source code in simple_uam/craidl/info_files.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 @frozen class DesignInfoFiles (): corpus : CorpusReader = field () \"\"\" Corpus being used during generation. \"\"\" design : dict = field () \"\"\" Design rep we're generating info files for. \"\"\" design_to_corpus : dict = field ( init = False , ) \"\"\" Map from design component instances to corpus component classes. \"\"\" @design_to_corpus . default def _cdcm_default ( self ): cdcm = dict () for comp_entry in self . design [ 'components' ]: from_comp = comp_entry [ 'component_instance' ] lib_component = comp_entry [ 'component_choice' ] cdcm [ from_comp ] = lib_component return cdcm component_maps : List [ dict ] = field ( init = False , ) \"\"\" Map of components and cad files. \"\"\" component_maps_file = 'info_componentMapList1.json' @component_maps . default def _cmp_maps_default ( self ): cmp_maps = list () for from_comp , lib_comp in self . design_to_corpus . items (): cad_prt = self . corpus [ lib_comp ] . cad_part new_entry = { 'FROM_COMP' : from_comp , 'LIB_COMPONENT' : lib_comp , 'CAD_PRT' : cad_prt } if any_none ( from_comp , lib_comp , cad_prt ): log . warning ( 'Skipping entry in component_maps due to null values' , ** new_entry , ) else : cmp_maps . append ( new_entry ) return cmp_maps connection_maps : List [ dict ] = field ( init = False , ) \"\"\" List of connections in the design. \"\"\" connection_maps_file = 'info_connectionMap6.json' @connection_maps . default def _conn_maps_def ( self ): conn_maps = list () for conn_entry in self . design [ 'connections' ]: from_comp = conn_entry [ 'from_ci' ] from_conn = conn_entry [ 'from_conn' ] to_comp = conn_entry [ 'to_ci' ] to_conn = conn_entry [ 'to_conn' ] new_entry = { 'FROM_COMP' : from_comp , 'FROM_CONN' : from_conn , 'TO_COMP' : to_comp , 'TO_CONN' : to_conn } if any_none ( from_comp , from_conn , to_comp , to_conn ): log . warning ( 'Skipping entry in connectionMap due to null values' , comp_entry = conn_entry , ** new_entry , ) else : conn_maps . append ( new_entry ) return conn_maps param_maps : List [ dict ] = field ( init = False , ) param_maps_file = \"info_paramMap4.json\" @param_maps . default def _param_maps_default ( self ): param_maps = list () for param_entry in self . design [ 'parameters' ]: design_param = param_entry [ 'parameter_name' ] design_param_val = param_entry [ 'value' ] for target in param_entry [ 'component_properties' ]: component_name = target [ 'component_name' ] component_param = target [ 'component_property' ] new_entry = { 'DESIGN_PARAM' : design_param , 'DESIGN_PARAM_VAL' : design_param_val , 'COMPONENT_NAME' : component_name , 'COMPONENT_PARAM' : component_param } if any_none ( design_param , design_param_val , component_name , component_param ): log . warning ( 'Skipping entry in paramMap due to null values' , comp_entry = comp_entry , ** new_entry , ) else : param_maps . append ( new_entry ) return param_maps cad_properties : List [ dict ] = field ( init = False , ) cad_properties_file = \"info_componentCadProps2.json\" @cad_properties . default def _cad_props_default ( self ): cad_props = list () for entry in self . component_maps : # NOTE : needs component map list comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] cad_part = entry [ 'CAD_PRT' ] prop_vals = self . corpus [ lib_name ] . cad_properties for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'CAD_PART' : cad_part , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_value ): log . warning ( 'Skipping entry in paramMap due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : cad_props . append ( new_entry ) return cad_props cad_connections : List [ dict ] = field ( init = False , ) cad_connections_file = \"info_connectionCADMap3.json\" @cad_connections . default def _cad_conn_default ( self ): cad_conns = list () for entry in self . connection_maps : ## NOTE: need connmap from_comp = entry [ 'FROM_COMP' ] to_comp = entry [ 'TO_COMP' ] from_conn = entry [ 'FROM_CONN' ] to_conn = entry [ 'TO_CONN' ] from_comp_type = self . design_to_corpus [ from_comp ] ## NOTE: needs ci_to_comp to_comp_type = self . design_to_corpus [ to_comp ] from_conn_cs = self . corpus [ from_comp_type ] . cad_connection ( from_conn ) to_conn_cs = self . corpus [ to_comp_type ] . cad_connection ( to_conn ) new_entry = { 'FROM_COMP' : from_comp , 'FROM_CONN_CS' : from_conn_cs , 'TO_COMP' : to_comp , 'TO_CONN_CS' : to_conn_cs } if any_none ( from_conn_cs and to_conn_cs ): log . warning ( 'Skipping entry in connectionCadMap due to null values' , entry = entry , ** new_entry , ) else : cad_conns . append ( new_entry ) return cad_conns component_props : List [ dict ] = field ( init = False , ) component_props_file = \"info_componentProps7.json\" @component_props . default def _comp_props_default ( self ): comp_props = list () for entry in self . component_maps : comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] prop_vals = self . corpus [ lib_name ] . properties for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_val ): log . warning ( 'Skipping entry in componentProps due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : comp_props . append ( new_entry ) return comp_props component_params : List [ dict ] = field ( init = False , ) component_params_file = 'info_componentParams8.json' @component_params . default def _comp_params_default ( self ): comp_params = list () for entry in self . component_maps : comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] prop_vals = self . corpus [ lib_name ] . params for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_value ): log . warning ( 'Skipping entry in componentParams due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : comp_params . append ( new_entry ) return comp_params cad_params : List [ dict ] = field ( init = False , ) cad_params_file = 'info_componentCADParams5.json' @cad_params . default def _cad_params ( self ): cad_params = list () for entry in self . component_maps : ## NOTE: ComponentMapList comp_name = entry [ 'FROM_COMP' ] lib_name = entry [ 'LIB_COMPONENT' ] cad_part = entry [ 'CAD_PRT' ] prop_vals = self . corpus [ lib_name ] . cad_params for prop_val in prop_vals : prop_name = prop_val [ 'PROP_NAME' ] prop_value = prop_val [ 'PROP_VALUE' ] new_entry = { 'COMP_NAME' : comp_name , 'LIB_NAME' : lib_name , 'CAD_PART' : cad_part , 'PROP_NAME' : prop_name , 'PROP_VALUE' : prop_value } if any_none ( prop_name , prop_value ): log . warning ( 'Skipping entry in componentCadParams due to null values' , entry = entry , prop_val = prop_val , ** new_entry , ) else : cad_params . append ( new_entry ) return cad_params @property def info_file_map ( self ): \"\"\" Map from filename to file data. \"\"\" return { self . component_maps_file : self . component_maps , self . connection_maps_file : self . connection_maps , self . param_maps_file : self . param_maps , self . cad_properties_file : self . cad_properties , self . cad_connections_file : self . cad_connections , self . component_props_file : self . component_props , self . component_params_file : self . component_params , self . cad_params_file : self . cad_params , } def write_files ( self , out_dir ): \"\"\" Write all the info files to the output directory. \"\"\" out_dir = Path ( out_dir ) out_dir . mkdir ( parents = True , exist_ok = True ) for filename , content in self . info_file_map . items (): filepath = out_dir / filename with filepath . open ( 'w' ) as fp : json . dump ( content , fp , indent = \" \" ) @classmethod def load_design ( cls , corpus , fp , ** kwargs ): \"\"\" Load from filepointer to design. Arguments: corpus: The corpus accessor to use. fp: the file pointer to load from. **kwargs: same as json.load's kwargs \"\"\" design_rep = json . load ( fp , ** kwargs ) return cls ( corpus , design_rep ) @classmethod def load_design_str ( cls , corpus , s , ** kwargs ): \"\"\" Load from a json string of a design. Arguments: corpus: The corpus accessor to use. s: the json string to load from. **kwargs: same as json.loads's kwargs \"\"\" design_rep = json . loads ( s , ** kwargs ) return cls ( corpus , design_rep )","title":"DesignInfoFiles"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.component_maps","text":"Map of components and cad files.","title":"component_maps"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.connection_maps","text":"List of connections in the design.","title":"connection_maps"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.corpus","text":"Corpus being used during generation.","title":"corpus"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.design","text":"Design rep we're generating info files for.","title":"design"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.design_to_corpus","text":"Map from design component instances to corpus component classes.","title":"design_to_corpus"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.info_file_map","text":"Map from filename to file data. Source code in simple_uam/craidl/info_files.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @property def info_file_map ( self ): \"\"\" Map from filename to file data. \"\"\" return { self . component_maps_file : self . component_maps , self . connection_maps_file : self . connection_maps , self . param_maps_file : self . param_maps , self . cad_properties_file : self . cad_properties , self . cad_connections_file : self . cad_connections , self . component_props_file : self . component_props , self . component_params_file : self . component_params , self . cad_params_file : self . cad_params , }","title":"info_file_map()"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.load_design","text":"Load from filepointer to design. Parameters: Name Type Description Default corpus The corpus accessor to use. required fp the file pointer to load from. required **kwargs same as json.load's kwargs {} Source code in simple_uam/craidl/info_files.py 331 332 333 334 335 336 337 338 339 340 341 342 343 @classmethod def load_design ( cls , corpus , fp , ** kwargs ): \"\"\" Load from filepointer to design. Arguments: corpus: The corpus accessor to use. fp: the file pointer to load from. **kwargs: same as json.load's kwargs \"\"\" design_rep = json . load ( fp , ** kwargs ) return cls ( corpus , design_rep )","title":"load_design()"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.load_design_str","text":"Load from a json string of a design. Parameters: Name Type Description Default corpus The corpus accessor to use. required s the json string to load from. required **kwargs same as json.loads's kwargs {} Source code in simple_uam/craidl/info_files.py 345 346 347 348 349 350 351 352 353 354 355 356 357 @classmethod def load_design_str ( cls , corpus , s , ** kwargs ): \"\"\" Load from a json string of a design. Arguments: corpus: The corpus accessor to use. s: the json string to load from. **kwargs: same as json.loads's kwargs \"\"\" design_rep = json . loads ( s , ** kwargs ) return cls ( corpus , design_rep )","title":"load_design_str()"},{"location":"reference/simple_uam/craidl/info_files/#simple_uam.craidl.info_files.DesignInfoFiles.write_files","text":"Write all the info files to the output directory. Source code in simple_uam/craidl/info_files.py 317 318 319 320 321 322 323 324 325 326 327 328 329 def write_files ( self , out_dir ): \"\"\" Write all the info files to the output directory. \"\"\" out_dir = Path ( out_dir ) out_dir . mkdir ( parents = True , exist_ok = True ) for filename , content in self . info_file_map . items (): filepath = out_dir / filename with filepath . open ( 'w' ) as fp : json . dump ( content , fp , indent = \" \" )","title":"write_files()"},{"location":"reference/simple_uam/craidl/corpus/","text":"SimpleUAM windows node setup scripts. CorpusReader \u00b6 Bases: ABC Represents a corpus that can be read and queried. Source code in simple_uam/craidl/corpus/abstract.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class CorpusReader ( ABC ): \"\"\" Represents a corpus that can be read and queried. \"\"\" @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ... @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ... __getitem__ ( comp ) abstractmethod \u00b6 Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. Source code in simple_uam/craidl/corpus/abstract.py 96 97 98 99 100 101 102 103 104 @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ... components () property abstractmethod \u00b6 Iterates over a list of all components. Source code in simple_uam/craidl/corpus/abstract.py 106 107 108 109 110 111 112 @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ... GremlinCorpus \u00b6 Bases: CorpusReader A corpus that is backed by a gremlin compatible server. Source code in simple_uam/craidl/corpus/gremlin.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @define class GremlinCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a gremlin compatible server. \"\"\" host : Optional [ str ] = field ( default = None , ) port : Optional [ int ] = field ( default = None , ) conn_timeout : int = field ( default = 300000 , ) \"\"\" Connection timeout in ms. \"\"\" eval_timeout : int = field ( default = 120000 , ) \"\"\" Single query timeout in ms. \"\"\" url : str = field ( init = False , ) @url . default def _default_url ( self ): host = self . host or 'localhost' port = self . port or 8182 return f \"ws:// { host } : { port } /gremlin\" conn = field ( init = False , ) @conn . default def init_conn ( self ): log . info ( 'Connecting to Gremlin server...' , server = self . url , ) return traversal () . withRemote ( DriverRemoteConnection ( self . url , 'g' )) start_time : float = field ( init = False , ) @start_time . default def _start_time_default ( self ): return time . monotonic () @property def g ( self ): curr_time = time . monotonic () elapsed = curr_time - self . start_time if elapsed > self . conn_timeout : log . info ( \"Connection timeout met, creating new connection.\" , start_time = self . start_time , curr_time = curr_time , elapsed = elapsed , conn_timeout = self . conn_timeout , ) self . conn = self . init_conn () self . start_time = time . monotonic () return self . conn . with_ ( 'evaluationTimeout' , self . eval_timeout ) def __getitem__ ( self , comp : str ) -> GremlinComponent : return GremlinComponent ( self , comp ) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . g . V () . hasLabel ( '[avm]Component' ) \\ . values ( '[]Name' ) . toList () conn_timeout : int = field ( default = 300000 ) class-attribute \u00b6 Connection timeout in ms. eval_timeout : int = field ( default = 120000 ) class-attribute \u00b6 Single query timeout in ms. StaticCorpus \u00b6 Bases: CorpusReader A corpus that is backed by a static representation. Source code in simple_uam/craidl/corpus/static.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 @frozen class StaticCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a static representation. \"\"\" rep : dict = field ( ) def __getitem__ ( self , comp : str ) -> ComponentReader : return StaticComponent . from_rep ( self . rep [ comp ]) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . rep . keys () @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , )) corpus_rep ( corp , cache_dir = None , cluster_size = 100 ) staticmethod \u00b6 Generates a representation of a corpus. Argument corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. Source code in simple_uam/craidl/corpus/static.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep dump_json ( fp , indent = ' ' , ** kwargs ) \u00b6 Same arguments as json.dump less the first. Source code in simple_uam/craidl/corpus/static.py 219 220 221 def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) dump_json_str ( indent = ' ' , ** kwargs ) \u00b6 Same arguments as json.dumps less the first. Source code in simple_uam/craidl/corpus/static.py 223 224 225 def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) from_corpus ( corp , cache_dir = None , cluster_size = 100 ) classmethod \u00b6 Extract the rep from a component. see corpus_rep for args. Source code in simple_uam/craidl/corpus/static.py 314 315 316 317 318 319 320 321 322 323 324 325 @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , )) from_rep ( rep ) classmethod \u00b6 Load from serializable rep. Source code in simple_uam/craidl/corpus/static.py 200 201 202 203 @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) load_json ( fp , ** kwargs ) classmethod \u00b6 Same arguments as json.load. Source code in simple_uam/craidl/corpus/static.py 209 210 211 212 @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) load_json_str ( s , ** kwargs ) classmethod \u00b6 Same arguments as json.loads. Source code in simple_uam/craidl/corpus/static.py 214 215 216 217 @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) to_rep () \u00b6 Return the rep. Source code in simple_uam/craidl/corpus/static.py 205 206 207 def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep","title":"corpus"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.CorpusReader","text":"Bases: ABC Represents a corpus that can be read and queried. Source code in simple_uam/craidl/corpus/abstract.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class CorpusReader ( ABC ): \"\"\" Represents a corpus that can be read and queried. \"\"\" @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ... @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ...","title":"CorpusReader"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.abstract.CorpusReader.__getitem__","text":"Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. Source code in simple_uam/craidl/corpus/abstract.py 96 97 98 99 100 101 102 103 104 @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ...","title":"__getitem__()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.abstract.CorpusReader.components","text":"Iterates over a list of all components. Source code in simple_uam/craidl/corpus/abstract.py 106 107 108 109 110 111 112 @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ...","title":"components()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.GremlinCorpus","text":"Bases: CorpusReader A corpus that is backed by a gremlin compatible server. Source code in simple_uam/craidl/corpus/gremlin.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @define class GremlinCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a gremlin compatible server. \"\"\" host : Optional [ str ] = field ( default = None , ) port : Optional [ int ] = field ( default = None , ) conn_timeout : int = field ( default = 300000 , ) \"\"\" Connection timeout in ms. \"\"\" eval_timeout : int = field ( default = 120000 , ) \"\"\" Single query timeout in ms. \"\"\" url : str = field ( init = False , ) @url . default def _default_url ( self ): host = self . host or 'localhost' port = self . port or 8182 return f \"ws:// { host } : { port } /gremlin\" conn = field ( init = False , ) @conn . default def init_conn ( self ): log . info ( 'Connecting to Gremlin server...' , server = self . url , ) return traversal () . withRemote ( DriverRemoteConnection ( self . url , 'g' )) start_time : float = field ( init = False , ) @start_time . default def _start_time_default ( self ): return time . monotonic () @property def g ( self ): curr_time = time . monotonic () elapsed = curr_time - self . start_time if elapsed > self . conn_timeout : log . info ( \"Connection timeout met, creating new connection.\" , start_time = self . start_time , curr_time = curr_time , elapsed = elapsed , conn_timeout = self . conn_timeout , ) self . conn = self . init_conn () self . start_time = time . monotonic () return self . conn . with_ ( 'evaluationTimeout' , self . eval_timeout ) def __getitem__ ( self , comp : str ) -> GremlinComponent : return GremlinComponent ( self , comp ) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . g . V () . hasLabel ( '[avm]Component' ) \\ . values ( '[]Name' ) . toList ()","title":"GremlinCorpus"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.gremlin.GremlinCorpus.conn_timeout","text":"Connection timeout in ms.","title":"conn_timeout"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.gremlin.GremlinCorpus.eval_timeout","text":"Single query timeout in ms.","title":"eval_timeout"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.StaticCorpus","text":"Bases: CorpusReader A corpus that is backed by a static representation. Source code in simple_uam/craidl/corpus/static.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 @frozen class StaticCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a static representation. \"\"\" rep : dict = field ( ) def __getitem__ ( self , comp : str ) -> ComponentReader : return StaticComponent . from_rep ( self . rep [ comp ]) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . rep . keys () @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , ))","title":"StaticCorpus"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.corpus_rep","text":"Generates a representation of a corpus. Argument corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. Source code in simple_uam/craidl/corpus/static.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep","title":"corpus_rep()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.dump_json","text":"Same arguments as json.dump less the first. Source code in simple_uam/craidl/corpus/static.py 219 220 221 def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs )","title":"dump_json()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.dump_json_str","text":"Same arguments as json.dumps less the first. Source code in simple_uam/craidl/corpus/static.py 223 224 225 def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs )","title":"dump_json_str()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.from_corpus","text":"Extract the rep from a component. see corpus_rep for args. Source code in simple_uam/craidl/corpus/static.py 314 315 316 317 318 319 320 321 322 323 324 325 @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , ))","title":"from_corpus()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.from_rep","text":"Load from serializable rep. Source code in simple_uam/craidl/corpus/static.py 200 201 202 203 @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep )","title":"from_rep()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.load_json","text":"Same arguments as json.load. Source code in simple_uam/craidl/corpus/static.py 209 210 211 212 @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs ))","title":"load_json()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.load_json_str","text":"Same arguments as json.loads. Source code in simple_uam/craidl/corpus/static.py 214 215 216 217 @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs ))","title":"load_json_str()"},{"location":"reference/simple_uam/craidl/corpus/#simple_uam.craidl.corpus.static.StaticCorpus.to_rep","text":"Return the rep. Source code in simple_uam/craidl/corpus/static.py 205 206 207 def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep","title":"to_rep()"},{"location":"reference/simple_uam/craidl/corpus/abstract/","text":"ComponentReader \u00b6 Bases: ABC Abstract interface representing a single component in a corpus. Source code in simple_uam/craidl/corpus/abstract.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class ComponentReader ( ABC ): \"\"\" Abstract interface representing a single component in a corpus. \"\"\" @property @abstractmethod def name ( self ) -> str : \"\"\" The component name. \"\"\" ... @property @abstractmethod def connections ( self ) -> Iterator [ str ]: \"\"\" The list of connections this component type can have. \"\"\" ... @property @abstractmethod def cad_part ( self ) -> Optional [ str ]: \"\"\" Get a string with the name of the cad file for this component. \"\"\" ... @property @abstractmethod def cad_properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... @property @abstractmethod def cad_params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... @abstractmethod def cad_connection ( self , conn : str ) -> Optional [ str ]: \"\"\" Get the connection specifier as used in this component's cad file. Arguments: conn: Name of connection as string. Returns: String name of connection specifier. \"\"\" ... @property @abstractmethod def properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... @property @abstractmethod def params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... cad_connection ( conn ) abstractmethod \u00b6 Get the connection specifier as used in this component's cad file. Parameters: Name Type Description Default conn str Name of connection as string. required Source code in simple_uam/craidl/corpus/abstract.py 56 57 58 59 60 61 62 63 64 65 66 @abstractmethod def cad_connection ( self , conn : str ) -> Optional [ str ]: \"\"\" Get the connection specifier as used in this component's cad file. Arguments: conn: Name of connection as string. Returns: String name of connection specifier. \"\"\" ... cad_params () property abstractmethod \u00b6 Cad parameters for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 45 46 47 48 49 50 51 52 53 54 @property @abstractmethod def cad_params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... cad_part () property abstractmethod \u00b6 Get a string with the name of the cad file for this component. Source code in simple_uam/craidl/corpus/abstract.py 26 27 28 29 30 31 32 @property @abstractmethod def cad_part ( self ) -> Optional [ str ]: \"\"\" Get a string with the name of the cad file for this component. \"\"\" ... cad_properties () property abstractmethod \u00b6 Cad Properties for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 34 35 36 37 38 39 40 41 42 43 @property @abstractmethod def cad_properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... connections () property abstractmethod \u00b6 The list of connections this component type can have. Source code in simple_uam/craidl/corpus/abstract.py 18 19 20 21 22 23 24 @property @abstractmethod def connections ( self ) -> Iterator [ str ]: \"\"\" The list of connections this component type can have. \"\"\" ... name () property abstractmethod \u00b6 The component name. Source code in simple_uam/craidl/corpus/abstract.py 10 11 12 13 14 15 16 @property @abstractmethod def name ( self ) -> str : \"\"\" The component name. \"\"\" ... params () property abstractmethod \u00b6 Parameters for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 80 81 82 83 84 85 86 87 88 89 @property @abstractmethod def params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... properties () property abstractmethod \u00b6 Properties for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 69 70 71 72 73 74 75 76 77 78 @property @abstractmethod def properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... CorpusReader \u00b6 Bases: ABC Represents a corpus that can be read and queried. Source code in simple_uam/craidl/corpus/abstract.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class CorpusReader ( ABC ): \"\"\" Represents a corpus that can be read and queried. \"\"\" @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ... @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ... __getitem__ ( comp ) abstractmethod \u00b6 Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. Source code in simple_uam/craidl/corpus/abstract.py 96 97 98 99 100 101 102 103 104 @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ... components () property abstractmethod \u00b6 Iterates over a list of all components. Source code in simple_uam/craidl/corpus/abstract.py 106 107 108 109 110 111 112 @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ...","title":"abstract"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader","text":"Bases: ABC Abstract interface representing a single component in a corpus. Source code in simple_uam/craidl/corpus/abstract.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class ComponentReader ( ABC ): \"\"\" Abstract interface representing a single component in a corpus. \"\"\" @property @abstractmethod def name ( self ) -> str : \"\"\" The component name. \"\"\" ... @property @abstractmethod def connections ( self ) -> Iterator [ str ]: \"\"\" The list of connections this component type can have. \"\"\" ... @property @abstractmethod def cad_part ( self ) -> Optional [ str ]: \"\"\" Get a string with the name of the cad file for this component. \"\"\" ... @property @abstractmethod def cad_properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... @property @abstractmethod def cad_params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... @abstractmethod def cad_connection ( self , conn : str ) -> Optional [ str ]: \"\"\" Get the connection specifier as used in this component's cad file. Arguments: conn: Name of connection as string. Returns: String name of connection specifier. \"\"\" ... @property @abstractmethod def properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ... @property @abstractmethod def params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ...","title":"ComponentReader"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.cad_connection","text":"Get the connection specifier as used in this component's cad file. Parameters: Name Type Description Default conn str Name of connection as string. required Source code in simple_uam/craidl/corpus/abstract.py 56 57 58 59 60 61 62 63 64 65 66 @abstractmethod def cad_connection ( self , conn : str ) -> Optional [ str ]: \"\"\" Get the connection specifier as used in this component's cad file. Arguments: conn: Name of connection as string. Returns: String name of connection specifier. \"\"\" ...","title":"cad_connection()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.cad_params","text":"Cad parameters for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 45 46 47 48 49 50 51 52 53 54 @property @abstractmethod def cad_params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ...","title":"cad_params()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.cad_part","text":"Get a string with the name of the cad file for this component. Source code in simple_uam/craidl/corpus/abstract.py 26 27 28 29 30 31 32 @property @abstractmethod def cad_part ( self ) -> Optional [ str ]: \"\"\" Get a string with the name of the cad file for this component. \"\"\" ...","title":"cad_part()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.cad_properties","text":"Cad Properties for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 34 35 36 37 38 39 40 41 42 43 @property @abstractmethod def cad_properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Cad Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ...","title":"cad_properties()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.connections","text":"The list of connections this component type can have. Source code in simple_uam/craidl/corpus/abstract.py 18 19 20 21 22 23 24 @property @abstractmethod def connections ( self ) -> Iterator [ str ]: \"\"\" The list of connections this component type can have. \"\"\" ...","title":"connections()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.name","text":"The component name. Source code in simple_uam/craidl/corpus/abstract.py 10 11 12 13 14 15 16 @property @abstractmethod def name ( self ) -> str : \"\"\" The component name. \"\"\" ...","title":"name()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.params","text":"Parameters for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 80 81 82 83 84 85 86 87 88 89 @property @abstractmethod def params ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Parameters for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ...","title":"params()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.ComponentReader.properties","text":"Properties for this component. Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. Source code in simple_uam/craidl/corpus/abstract.py 69 70 71 72 73 74 75 76 77 78 @property @abstractmethod def properties ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Properties for this component. Returns: A list of dictionaries with 'PROP_NAME' and 'PROP_VALUE' entries. \"\"\" ...","title":"properties()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.CorpusReader","text":"Bases: ABC Represents a corpus that can be read and queried. Source code in simple_uam/craidl/corpus/abstract.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class CorpusReader ( ABC ): \"\"\" Represents a corpus that can be read and queried. \"\"\" @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ... @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ...","title":"CorpusReader"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.CorpusReader.__getitem__","text":"Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. Source code in simple_uam/craidl/corpus/abstract.py 96 97 98 99 100 101 102 103 104 @abstractmethod def __getitem__ ( self , comp : str ) -> ComponentReader : \"\"\" Allows you to get a component by providing its name. Implementations may error on call or defer errors till compoent is used. \"\"\" ...","title":"__getitem__()"},{"location":"reference/simple_uam/craidl/corpus/abstract/#simple_uam.craidl.corpus.abstract.CorpusReader.components","text":"Iterates over a list of all components. Source code in simple_uam/craidl/corpus/abstract.py 106 107 108 109 110 111 112 @property @abstractmethod def components ( self ) -> Iterator [ ComponentReader ]: \"\"\" Iterates over a list of all components. \"\"\" ...","title":"components()"},{"location":"reference/simple_uam/craidl/corpus/get_corpus/","text":"get_corpus ( config , static = None , host = None , port = None ) \u00b6 Given the inputs will create a corpus object, using the configured settings where needed. Parameters: Name Type Description Default config CraidlConfig The CraidlConfig that the defaults are taken from. required static The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured None host The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured None port The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured None Source code in simple_uam/craidl/corpus/get_corpus.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def get_corpus ( config : CraidlConfig , static = None , host = None , port = None ): \"\"\" Given the inputs will create a corpus object, using the configured settings where needed. Arguments: config: The CraidlConfig that the defaults are taken from. static: The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured host: The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured port: The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured \"\"\" mk_static = False if static and host : err = RuntimeError ( \"Cannot have both static corpus and corpus server.\" ) log . exception ( \"Cannot specify 'static' and 'host' at the same time.\" , static = static , host = host , port = port , err = err , ) raise err elif not static and not host : if config . use_static_corpus : mk_static = True static = True else : host = True elif static : mk_static = True if mk_static : if isinstance ( static , bool ) and static : static = config . static_corpus static = Path ( static ) log . info ( \"Loading static corpus.\" , corpus_file = str ( static ), ) with static . open () as fp : return StaticCorpus . load_json ( fp ) else : if isinstance ( host , bool ) and host : host = config . server_host if not port : port = config . server_port log . info ( \"Creating corpus client.\" , host = host , port = port , ) return GremlinCorpus ( host = host , port = port )","title":"get_corpus"},{"location":"reference/simple_uam/craidl/corpus/get_corpus/#simple_uam.craidl.corpus.get_corpus.get_corpus","text":"Given the inputs will create a corpus object, using the configured settings where needed. Parameters: Name Type Description Default config CraidlConfig The CraidlConfig that the defaults are taken from. required static The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured None host The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured None port The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured None Source code in simple_uam/craidl/corpus/get_corpus.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def get_corpus ( config : CraidlConfig , static = None , host = None , port = None ): \"\"\" Given the inputs will create a corpus object, using the configured settings where needed. Arguments: config: The CraidlConfig that the defaults are taken from. static: The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured host: The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured port: The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured \"\"\" mk_static = False if static and host : err = RuntimeError ( \"Cannot have both static corpus and corpus server.\" ) log . exception ( \"Cannot specify 'static' and 'host' at the same time.\" , static = static , host = host , port = port , err = err , ) raise err elif not static and not host : if config . use_static_corpus : mk_static = True static = True else : host = True elif static : mk_static = True if mk_static : if isinstance ( static , bool ) and static : static = config . static_corpus static = Path ( static ) log . info ( \"Loading static corpus.\" , corpus_file = str ( static ), ) with static . open () as fp : return StaticCorpus . load_json ( fp ) else : if isinstance ( host , bool ) and host : host = config . server_host if not port : port = config . server_port log . info ( \"Creating corpus client.\" , host = host , port = port , ) return GremlinCorpus ( host = host , port = port )","title":"get_corpus()"},{"location":"reference/simple_uam/craidl/corpus/gremlin/","text":"GremlinComponent \u00b6 Bases: ComponentReader A wrapper for a component in a gremlin db, will only perform queries lazily. Source code in simple_uam/craidl/corpus/gremlin.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @define class GremlinComponent ( ComponentReader ): \"\"\" A wrapper for a component in a gremlin db, will only perform queries lazily. \"\"\" parent = field () \"\"\" The parent corpus. \"\"\" _name = field () \"\"\" The string name of this component. \"\"\" @property def name ( self ) -> str : return self . _name @property def g ( self ) -> str : return self . parent . g @property def connections ( self ) -> List [ str ]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Connector' ) \\ . values ( '[]Name' ) . toList () @property def cad_part ( self ) -> Optional [ str ]: part = self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . has ( 'VertexLabel' , '[]DomainModel' ) \\ . has ( '[]Format' , 'Creo' ) . values ( '[]Name' ) . toList () return None if len ( part ) == 0 else part [ 0 ] @property def cad_properties ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]DomainModel' ) . has ( '[]Format' , 'Creo' ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Parameter' ) \\ . as_ ( 'PROP' ) . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) \\ . select ( 'PROP' ) . in_ ( 'inside' ) . in_ ( 'inside' ) . out ( 'value_source' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) . in_ ( 'inside' ) . values ( 'value' ) \\ . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () @property def cad_params ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]DomainModel' ) . has ( '[]Format' , 'Creo' ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Parameter' ) . as_ ( 'LIB_PROP' ) \\ . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) \\ . select ( 'LIB_PROP' ) . in_ ( 'inside' ) . in_ ( 'inside' ) . out ( 'value_source' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) \\ . hasLabel ( '[]AssignedValue' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) . values ( 'value' ) . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () def cad_connection ( self , conn : str ) -> Optional [ str ]: conn = self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . has ( '[]Connector' , '[]Name' , conn ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Role' ) . out ( 'port_map' ) . values ( '[]Name' ) \\ . dedup () . toList () return None if len ( conn ) == 0 else conn [ 0 ] @property def properties ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Property' ) . as_ ( 'C_PROP' ) \\ . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) . select ( 'C_PROP' ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Value' ) . in_ ( 'inside' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) . values ( 'value' ) . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () @property def params ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Property' ) . as_ ( 'C_PROP' ) \\ . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) \\ . select ( 'C_PROP' ) . in_ ( 'inside' ) . hasLabel ( '[]Value' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) \\ . hasLabel ( '[]AssignedValue' ) . in_ ( 'inside' ) \\ . in_ ( 'inside' ) . values ( 'value' ) . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () parent = field () class-attribute \u00b6 The parent corpus. GremlinCorpus \u00b6 Bases: CorpusReader A corpus that is backed by a gremlin compatible server. Source code in simple_uam/craidl/corpus/gremlin.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @define class GremlinCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a gremlin compatible server. \"\"\" host : Optional [ str ] = field ( default = None , ) port : Optional [ int ] = field ( default = None , ) conn_timeout : int = field ( default = 300000 , ) \"\"\" Connection timeout in ms. \"\"\" eval_timeout : int = field ( default = 120000 , ) \"\"\" Single query timeout in ms. \"\"\" url : str = field ( init = False , ) @url . default def _default_url ( self ): host = self . host or 'localhost' port = self . port or 8182 return f \"ws:// { host } : { port } /gremlin\" conn = field ( init = False , ) @conn . default def init_conn ( self ): log . info ( 'Connecting to Gremlin server...' , server = self . url , ) return traversal () . withRemote ( DriverRemoteConnection ( self . url , 'g' )) start_time : float = field ( init = False , ) @start_time . default def _start_time_default ( self ): return time . monotonic () @property def g ( self ): curr_time = time . monotonic () elapsed = curr_time - self . start_time if elapsed > self . conn_timeout : log . info ( \"Connection timeout met, creating new connection.\" , start_time = self . start_time , curr_time = curr_time , elapsed = elapsed , conn_timeout = self . conn_timeout , ) self . conn = self . init_conn () self . start_time = time . monotonic () return self . conn . with_ ( 'evaluationTimeout' , self . eval_timeout ) def __getitem__ ( self , comp : str ) -> GremlinComponent : return GremlinComponent ( self , comp ) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . g . V () . hasLabel ( '[avm]Component' ) \\ . values ( '[]Name' ) . toList () conn_timeout : int = field ( default = 300000 ) class-attribute \u00b6 Connection timeout in ms. eval_timeout : int = field ( default = 120000 ) class-attribute \u00b6 Single query timeout in ms.","title":"gremlin"},{"location":"reference/simple_uam/craidl/corpus/gremlin/#simple_uam.craidl.corpus.gremlin.GremlinComponent","text":"Bases: ComponentReader A wrapper for a component in a gremlin db, will only perform queries lazily. Source code in simple_uam/craidl/corpus/gremlin.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @define class GremlinComponent ( ComponentReader ): \"\"\" A wrapper for a component in a gremlin db, will only perform queries lazily. \"\"\" parent = field () \"\"\" The parent corpus. \"\"\" _name = field () \"\"\" The string name of this component. \"\"\" @property def name ( self ) -> str : return self . _name @property def g ( self ) -> str : return self . parent . g @property def connections ( self ) -> List [ str ]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Connector' ) \\ . values ( '[]Name' ) . toList () @property def cad_part ( self ) -> Optional [ str ]: part = self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . has ( 'VertexLabel' , '[]DomainModel' ) \\ . has ( '[]Format' , 'Creo' ) . values ( '[]Name' ) . toList () return None if len ( part ) == 0 else part [ 0 ] @property def cad_properties ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]DomainModel' ) . has ( '[]Format' , 'Creo' ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Parameter' ) \\ . as_ ( 'PROP' ) . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) \\ . select ( 'PROP' ) . in_ ( 'inside' ) . in_ ( 'inside' ) . out ( 'value_source' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) . in_ ( 'inside' ) . values ( 'value' ) \\ . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () @property def cad_params ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]DomainModel' ) . has ( '[]Format' , 'Creo' ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Parameter' ) . as_ ( 'LIB_PROP' ) \\ . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) \\ . select ( 'LIB_PROP' ) . in_ ( 'inside' ) . in_ ( 'inside' ) . out ( 'value_source' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) \\ . hasLabel ( '[]AssignedValue' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) . values ( 'value' ) . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () def cad_connection ( self , conn : str ) -> Optional [ str ]: conn = self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . has ( '[]Connector' , '[]Name' , conn ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Role' ) . out ( 'port_map' ) . values ( '[]Name' ) \\ . dedup () . toList () return None if len ( conn ) == 0 else conn [ 0 ] @property def properties ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Property' ) . as_ ( 'C_PROP' ) \\ . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) . select ( 'C_PROP' ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Value' ) . in_ ( 'inside' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) . values ( 'value' ) . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList () @property def params ( self ) -> List [ Dict [ str , Any ]]: return self . g . V () . has ( '[avm]Component' , '[]Name' , self . name ) \\ . in_ ( 'inside' ) . hasLabel ( '[]Property' ) . as_ ( 'C_PROP' ) \\ . values ( '[]Name' ) . as_ ( 'PROP_NAME' ) \\ . select ( 'C_PROP' ) . in_ ( 'inside' ) . hasLabel ( '[]Value' ) \\ . in_ ( 'inside' ) . in_ ( 'inside' ) \\ . hasLabel ( '[]AssignedValue' ) . in_ ( 'inside' ) \\ . in_ ( 'inside' ) . values ( 'value' ) . as_ ( 'PROP_VALUE' ) \\ . select ( 'PROP_NAME' , 'PROP_VALUE' ) \\ . toList ()","title":"GremlinComponent"},{"location":"reference/simple_uam/craidl/corpus/gremlin/#simple_uam.craidl.corpus.gremlin.GremlinComponent.parent","text":"The parent corpus.","title":"parent"},{"location":"reference/simple_uam/craidl/corpus/gremlin/#simple_uam.craidl.corpus.gremlin.GremlinCorpus","text":"Bases: CorpusReader A corpus that is backed by a gremlin compatible server. Source code in simple_uam/craidl/corpus/gremlin.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @define class GremlinCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a gremlin compatible server. \"\"\" host : Optional [ str ] = field ( default = None , ) port : Optional [ int ] = field ( default = None , ) conn_timeout : int = field ( default = 300000 , ) \"\"\" Connection timeout in ms. \"\"\" eval_timeout : int = field ( default = 120000 , ) \"\"\" Single query timeout in ms. \"\"\" url : str = field ( init = False , ) @url . default def _default_url ( self ): host = self . host or 'localhost' port = self . port or 8182 return f \"ws:// { host } : { port } /gremlin\" conn = field ( init = False , ) @conn . default def init_conn ( self ): log . info ( 'Connecting to Gremlin server...' , server = self . url , ) return traversal () . withRemote ( DriverRemoteConnection ( self . url , 'g' )) start_time : float = field ( init = False , ) @start_time . default def _start_time_default ( self ): return time . monotonic () @property def g ( self ): curr_time = time . monotonic () elapsed = curr_time - self . start_time if elapsed > self . conn_timeout : log . info ( \"Connection timeout met, creating new connection.\" , start_time = self . start_time , curr_time = curr_time , elapsed = elapsed , conn_timeout = self . conn_timeout , ) self . conn = self . init_conn () self . start_time = time . monotonic () return self . conn . with_ ( 'evaluationTimeout' , self . eval_timeout ) def __getitem__ ( self , comp : str ) -> GremlinComponent : return GremlinComponent ( self , comp ) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . g . V () . hasLabel ( '[avm]Component' ) \\ . values ( '[]Name' ) . toList ()","title":"GremlinCorpus"},{"location":"reference/simple_uam/craidl/corpus/gremlin/#simple_uam.craidl.corpus.gremlin.GremlinCorpus.conn_timeout","text":"Connection timeout in ms.","title":"conn_timeout"},{"location":"reference/simple_uam/craidl/corpus/gremlin/#simple_uam.craidl.corpus.gremlin.GremlinCorpus.eval_timeout","text":"Single query timeout in ms.","title":"eval_timeout"},{"location":"reference/simple_uam/craidl/corpus/static/","text":"StaticComponent \u00b6 Bases: ComponentReader A static component defined an an immutable object. Source code in simple_uam/craidl/corpus/static.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @frozen class StaticComponent ( ComponentReader ): \"\"\" A static component defined an an immutable object. \"\"\" rep : dict = field ( ) \"\"\" Internal object representation of a component. \"\"\" @property def name ( self ) -> str : return self . rep [ 'name' ] @property def connections ( self ) -> List [ str ]: return list ( self . rep . get ( 'conns' , dict ()) . keys ()) @property def cad_part ( self ) -> Optional [ str ]: return self . rep . get ( 'cad_part' ) @property def cad_properties ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'cad_props' , dict ()) ) @property def cad_params ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'cad_params' , dict ()) ) def cad_connection ( self , conn : str ) -> Optional [ str ]: return self . rep . get ( 'conns' , dict ()) . get ( conn , dict ()) . get ( 'cad' ) @property def properties ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'props' , dict ()) ) @property def params ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'params' , dict ()) ) @staticmethod def from_prop_list ( prop_list : List [ Dict [ str , str ]]) -> Dict [ str , str ]: prop_dict = dict () for item in prop_list : prop_dict [ item [ 'PROP_NAME' ]] = item [ 'PROP_VALUE' ] return prop_dict @staticmethod def to_prop_list ( prop_dict : Dict [ str , str ]) -> List [ Dict [ str , str ]]: prop_list = list () for name , val in prop_dict . items (): prop_list . append ({ 'PROP_NAME' : name , 'PROP_VALUE' : val }) return prop_list @classmethod def from_rep ( cls , rep ) -> 'StaticComponent' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) @classmethod def component_rep ( cls , comp : ComponentReader ) -> dict : \"\"\" Extract the rep from a component. \"\"\" rep = dict () log . info ( \"Getting component name.\" ) name = comp . name rep [ 'name' ] = name log . info ( \"Getting Connection List.\" ) conns = dict () conn_list = list ( comp . connections ) num = len ( conn_list ) for ind , conn in enumerate ( conn_list ): log . info ( f \"Getting connection data ( { ind + 1 } / { num } )\" , comp = name , conn = conn , ) conns [ conn ] = dict () if cad_conn := comp . cad_connection ( conn ): log . info ( f \"Found connection data.\" , comp = name , conn = conn , cad_conn = cad_conn , ) conns [ conn ][ 'cad' ] = cad_conn rep [ 'conns' ] = conns if cad_part := comp . cad_part : log . info ( \"Getting cad part data.\" , comp = name , cad_part = cad_part , ) rep [ 'cad_part' ] = cad_part cad_props = cls . from_prop_list ( comp . cad_properties ) log . info ( \"Getting cad properties\" , comp = name , cad_props = cad_props , ) if len ( cad_props ) > 0 : rep [ 'cad_props' ] = cad_props cad_params = cls . from_prop_list ( comp . cad_params ) log . info ( \"Getting cad parameters\" , comp = name , cad_params = cad_params , ) if len ( cad_params ) > 0 : rep [ 'cad_params' ] = cad_params props = cls . from_prop_list ( comp . properties ) log . info ( \"Getting properties.\" , comp = name , props = props , ) if len ( props ) > 0 : rep [ 'props' ] = props params = cls . from_prop_list ( comp . params ) log . info ( \"Getting parameters\" , comp = name , params = params , ) if len ( params ) > 0 : rep [ 'params' ] = params return rep @classmethod def from_component ( cls , comp : ComponentReader ) -> 'StaticComponent' : return cls . from_rep ( cls . component_rep ( comp )) rep : dict = field () class-attribute \u00b6 Internal object representation of a component. component_rep ( comp ) classmethod \u00b6 Extract the rep from a component. Source code in simple_uam/craidl/corpus/static.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @classmethod def component_rep ( cls , comp : ComponentReader ) -> dict : \"\"\" Extract the rep from a component. \"\"\" rep = dict () log . info ( \"Getting component name.\" ) name = comp . name rep [ 'name' ] = name log . info ( \"Getting Connection List.\" ) conns = dict () conn_list = list ( comp . connections ) num = len ( conn_list ) for ind , conn in enumerate ( conn_list ): log . info ( f \"Getting connection data ( { ind + 1 } / { num } )\" , comp = name , conn = conn , ) conns [ conn ] = dict () if cad_conn := comp . cad_connection ( conn ): log . info ( f \"Found connection data.\" , comp = name , conn = conn , cad_conn = cad_conn , ) conns [ conn ][ 'cad' ] = cad_conn rep [ 'conns' ] = conns if cad_part := comp . cad_part : log . info ( \"Getting cad part data.\" , comp = name , cad_part = cad_part , ) rep [ 'cad_part' ] = cad_part cad_props = cls . from_prop_list ( comp . cad_properties ) log . info ( \"Getting cad properties\" , comp = name , cad_props = cad_props , ) if len ( cad_props ) > 0 : rep [ 'cad_props' ] = cad_props cad_params = cls . from_prop_list ( comp . cad_params ) log . info ( \"Getting cad parameters\" , comp = name , cad_params = cad_params , ) if len ( cad_params ) > 0 : rep [ 'cad_params' ] = cad_params props = cls . from_prop_list ( comp . properties ) log . info ( \"Getting properties.\" , comp = name , props = props , ) if len ( props ) > 0 : rep [ 'props' ] = props params = cls . from_prop_list ( comp . params ) log . info ( \"Getting parameters\" , comp = name , params = params , ) if len ( params ) > 0 : rep [ 'params' ] = params return rep dump_json ( fp , indent = ' ' , ** kwargs ) \u00b6 Same arguments as json.dump less the first. Source code in simple_uam/craidl/corpus/static.py 94 95 96 def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) dump_json_str ( indent = ' ' , ** kwargs ) \u00b6 Same arguments as json.dumps less the first. Source code in simple_uam/craidl/corpus/static.py 98 99 100 def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) from_rep ( rep ) classmethod \u00b6 Load from serializable rep. Source code in simple_uam/craidl/corpus/static.py 75 76 77 78 @classmethod def from_rep ( cls , rep ) -> 'StaticComponent' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) load_json ( fp , ** kwargs ) classmethod \u00b6 Same arguments as json.load. Source code in simple_uam/craidl/corpus/static.py 84 85 86 87 @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) load_json_str ( s , ** kwargs ) classmethod \u00b6 Same arguments as json.loads. Source code in simple_uam/craidl/corpus/static.py 89 90 91 92 @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) to_rep () \u00b6 Return the rep. Source code in simple_uam/craidl/corpus/static.py 80 81 82 def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep StaticCorpus \u00b6 Bases: CorpusReader A corpus that is backed by a static representation. Source code in simple_uam/craidl/corpus/static.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 @frozen class StaticCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a static representation. \"\"\" rep : dict = field ( ) def __getitem__ ( self , comp : str ) -> ComponentReader : return StaticComponent . from_rep ( self . rep [ comp ]) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . rep . keys () @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , )) corpus_rep ( corp , cache_dir = None , cluster_size = 100 ) staticmethod \u00b6 Generates a representation of a corpus. Argument corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. Source code in simple_uam/craidl/corpus/static.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep dump_json ( fp , indent = ' ' , ** kwargs ) \u00b6 Same arguments as json.dump less the first. Source code in simple_uam/craidl/corpus/static.py 219 220 221 def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) dump_json_str ( indent = ' ' , ** kwargs ) \u00b6 Same arguments as json.dumps less the first. Source code in simple_uam/craidl/corpus/static.py 223 224 225 def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) from_corpus ( corp , cache_dir = None , cluster_size = 100 ) classmethod \u00b6 Extract the rep from a component. see corpus_rep for args. Source code in simple_uam/craidl/corpus/static.py 314 315 316 317 318 319 320 321 322 323 324 325 @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , )) from_rep ( rep ) classmethod \u00b6 Load from serializable rep. Source code in simple_uam/craidl/corpus/static.py 200 201 202 203 @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) load_json ( fp , ** kwargs ) classmethod \u00b6 Same arguments as json.load. Source code in simple_uam/craidl/corpus/static.py 209 210 211 212 @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) load_json_str ( s , ** kwargs ) classmethod \u00b6 Same arguments as json.loads. Source code in simple_uam/craidl/corpus/static.py 214 215 216 217 @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) to_rep () \u00b6 Return the rep. Source code in simple_uam/craidl/corpus/static.py 205 206 207 def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep","title":"static"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent","text":"Bases: ComponentReader A static component defined an an immutable object. Source code in simple_uam/craidl/corpus/static.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @frozen class StaticComponent ( ComponentReader ): \"\"\" A static component defined an an immutable object. \"\"\" rep : dict = field ( ) \"\"\" Internal object representation of a component. \"\"\" @property def name ( self ) -> str : return self . rep [ 'name' ] @property def connections ( self ) -> List [ str ]: return list ( self . rep . get ( 'conns' , dict ()) . keys ()) @property def cad_part ( self ) -> Optional [ str ]: return self . rep . get ( 'cad_part' ) @property def cad_properties ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'cad_props' , dict ()) ) @property def cad_params ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'cad_params' , dict ()) ) def cad_connection ( self , conn : str ) -> Optional [ str ]: return self . rep . get ( 'conns' , dict ()) . get ( conn , dict ()) . get ( 'cad' ) @property def properties ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'props' , dict ()) ) @property def params ( self ) -> List [ Dict [ str , Any ]]: return self . to_prop_list ( self . rep . get ( 'params' , dict ()) ) @staticmethod def from_prop_list ( prop_list : List [ Dict [ str , str ]]) -> Dict [ str , str ]: prop_dict = dict () for item in prop_list : prop_dict [ item [ 'PROP_NAME' ]] = item [ 'PROP_VALUE' ] return prop_dict @staticmethod def to_prop_list ( prop_dict : Dict [ str , str ]) -> List [ Dict [ str , str ]]: prop_list = list () for name , val in prop_dict . items (): prop_list . append ({ 'PROP_NAME' : name , 'PROP_VALUE' : val }) return prop_list @classmethod def from_rep ( cls , rep ) -> 'StaticComponent' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) @classmethod def component_rep ( cls , comp : ComponentReader ) -> dict : \"\"\" Extract the rep from a component. \"\"\" rep = dict () log . info ( \"Getting component name.\" ) name = comp . name rep [ 'name' ] = name log . info ( \"Getting Connection List.\" ) conns = dict () conn_list = list ( comp . connections ) num = len ( conn_list ) for ind , conn in enumerate ( conn_list ): log . info ( f \"Getting connection data ( { ind + 1 } / { num } )\" , comp = name , conn = conn , ) conns [ conn ] = dict () if cad_conn := comp . cad_connection ( conn ): log . info ( f \"Found connection data.\" , comp = name , conn = conn , cad_conn = cad_conn , ) conns [ conn ][ 'cad' ] = cad_conn rep [ 'conns' ] = conns if cad_part := comp . cad_part : log . info ( \"Getting cad part data.\" , comp = name , cad_part = cad_part , ) rep [ 'cad_part' ] = cad_part cad_props = cls . from_prop_list ( comp . cad_properties ) log . info ( \"Getting cad properties\" , comp = name , cad_props = cad_props , ) if len ( cad_props ) > 0 : rep [ 'cad_props' ] = cad_props cad_params = cls . from_prop_list ( comp . cad_params ) log . info ( \"Getting cad parameters\" , comp = name , cad_params = cad_params , ) if len ( cad_params ) > 0 : rep [ 'cad_params' ] = cad_params props = cls . from_prop_list ( comp . properties ) log . info ( \"Getting properties.\" , comp = name , props = props , ) if len ( props ) > 0 : rep [ 'props' ] = props params = cls . from_prop_list ( comp . params ) log . info ( \"Getting parameters\" , comp = name , params = params , ) if len ( params ) > 0 : rep [ 'params' ] = params return rep @classmethod def from_component ( cls , comp : ComponentReader ) -> 'StaticComponent' : return cls . from_rep ( cls . component_rep ( comp ))","title":"StaticComponent"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.rep","text":"Internal object representation of a component.","title":"rep"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.component_rep","text":"Extract the rep from a component. Source code in simple_uam/craidl/corpus/static.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 @classmethod def component_rep ( cls , comp : ComponentReader ) -> dict : \"\"\" Extract the rep from a component. \"\"\" rep = dict () log . info ( \"Getting component name.\" ) name = comp . name rep [ 'name' ] = name log . info ( \"Getting Connection List.\" ) conns = dict () conn_list = list ( comp . connections ) num = len ( conn_list ) for ind , conn in enumerate ( conn_list ): log . info ( f \"Getting connection data ( { ind + 1 } / { num } )\" , comp = name , conn = conn , ) conns [ conn ] = dict () if cad_conn := comp . cad_connection ( conn ): log . info ( f \"Found connection data.\" , comp = name , conn = conn , cad_conn = cad_conn , ) conns [ conn ][ 'cad' ] = cad_conn rep [ 'conns' ] = conns if cad_part := comp . cad_part : log . info ( \"Getting cad part data.\" , comp = name , cad_part = cad_part , ) rep [ 'cad_part' ] = cad_part cad_props = cls . from_prop_list ( comp . cad_properties ) log . info ( \"Getting cad properties\" , comp = name , cad_props = cad_props , ) if len ( cad_props ) > 0 : rep [ 'cad_props' ] = cad_props cad_params = cls . from_prop_list ( comp . cad_params ) log . info ( \"Getting cad parameters\" , comp = name , cad_params = cad_params , ) if len ( cad_params ) > 0 : rep [ 'cad_params' ] = cad_params props = cls . from_prop_list ( comp . properties ) log . info ( \"Getting properties.\" , comp = name , props = props , ) if len ( props ) > 0 : rep [ 'props' ] = props params = cls . from_prop_list ( comp . params ) log . info ( \"Getting parameters\" , comp = name , params = params , ) if len ( params ) > 0 : rep [ 'params' ] = params return rep","title":"component_rep()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.dump_json","text":"Same arguments as json.dump less the first. Source code in simple_uam/craidl/corpus/static.py 94 95 96 def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs )","title":"dump_json()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.dump_json_str","text":"Same arguments as json.dumps less the first. Source code in simple_uam/craidl/corpus/static.py 98 99 100 def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs )","title":"dump_json_str()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.from_rep","text":"Load from serializable rep. Source code in simple_uam/craidl/corpus/static.py 75 76 77 78 @classmethod def from_rep ( cls , rep ) -> 'StaticComponent' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep )","title":"from_rep()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.load_json","text":"Same arguments as json.load. Source code in simple_uam/craidl/corpus/static.py 84 85 86 87 @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs ))","title":"load_json()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.load_json_str","text":"Same arguments as json.loads. Source code in simple_uam/craidl/corpus/static.py 89 90 91 92 @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs ))","title":"load_json_str()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticComponent.to_rep","text":"Return the rep. Source code in simple_uam/craidl/corpus/static.py 80 81 82 def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep","title":"to_rep()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus","text":"Bases: CorpusReader A corpus that is backed by a static representation. Source code in simple_uam/craidl/corpus/static.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 @frozen class StaticCorpus ( CorpusReader ): \"\"\" A corpus that is backed by a static representation. \"\"\" rep : dict = field ( ) def __getitem__ ( self , comp : str ) -> ComponentReader : return StaticComponent . from_rep ( self . rep [ comp ]) @property def components ( self ) -> Iterator [ ComponentReader ]: return self . rep . keys () @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep ) def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs )) @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs )) def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs ) def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs ) @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , ))","title":"StaticCorpus"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.corpus_rep","text":"Generates a representation of a corpus. Argument corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. Source code in simple_uam/craidl/corpus/static.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @staticmethod def corpus_rep ( corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> dict : \"\"\" Generates a representation of a corpus. Argument: corp: The corpus we are reading from. cache_dir: The directory we are reading cached items from. cluster_size: The size of each cluster of files we write to size. \"\"\" if cache_dir : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Getting component list.\" ) cmp_list = sorted ( list ( corp . components )) cmp_num = len ( cmp_list ) # split into clusters clusters = list () cluster_filename = lambda x : f \"corpus_cache_ { x } .json\" if cache_dir : clusters = [ cmp_list [ x : x + cluster_size ] for x in range ( 0 , cmp_num , cluster_size )] else : clusters = [ cmp_list ] cluster_num = len ( clusters ) rep = dict () for cluster_ind , cluster in enumerate ( clusters ): # check if cluster file exists cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" if cache_dir and cluster_file . exists (): log . info ( f \"Found cached cluster of components, skipping read. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) continue elif cache_dir : log . info ( f \"Reading cluster from source corpus. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) # get components in cluster for comp_num , comp in enumerate ( cluster ): ind = ( cluster_ind * cluster_size ) + comp_num log . info ( f \"Getting component data ( { ind + 1 } / { cmp_num } )\" , component = comp ) rep [ comp ] = StaticComponent . component_rep ( corp [ comp ]) # write cluster file if cache_dir : log . info ( f \"Writing cluster to file. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'w' ) as cf : json . dump ( rep , cf , indent = \" \" ) rep = dict () if cache_dir : rep = dict () for cluster_ind , cluster in enumerate ( clusters ): cluster_file = cache_dir / cluster_filename ( cluster_ind ) cluster_str = f \"( { cluster_ind + 1 } / { cluster_num } )\" log . info ( f \"Reading cluster from cache. { cluster_str } \" , cache_dir = str ( cache_dir ), cluster_file = str ( cluster_file ), ) with cluster_file . open ( 'r' ) as cf : rep . update ( json . load ( cf )) log . info ( \"Generated complete dump, deleting cache dir.\" , cache_dir = str ( cache_dir ) ) shutil . rmtree ( cache_dir , ignore_error = True ) return rep","title":"corpus_rep()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.dump_json","text":"Same arguments as json.dump less the first. Source code in simple_uam/craidl/corpus/static.py 219 220 221 def dump_json ( self , fp , indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dump less the first. \"\"\" return json . dump ( self . rep , fp , indent = indent , ** kwargs )","title":"dump_json()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.dump_json_str","text":"Same arguments as json.dumps less the first. Source code in simple_uam/craidl/corpus/static.py 223 224 225 def dump_json_str ( indent = \" \" , ** kwargs ): \"\"\" Same arguments as json.dumps less the first. \"\"\" return json . dumps ( self . rep , indent = indent , ** kwargs )","title":"dump_json_str()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.from_corpus","text":"Extract the rep from a component. see corpus_rep for args. Source code in simple_uam/craidl/corpus/static.py 314 315 316 317 318 319 320 321 322 323 324 325 @classmethod def from_corpus ( cls , corp : CorpusReader , cache_dir : Union [ Path , str , None ] = None , cluster_size : int = 100 ) -> 'StaticCorpus' : \"\"\" Extract the rep from a component. see corpus_rep for args. \"\"\" return cls . from_rep ( cls . corpus_rep ( corp , cache_dir = cache_dir , cluster_size = cluster_size , ))","title":"from_corpus()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.from_rep","text":"Load from serializable rep. Source code in simple_uam/craidl/corpus/static.py 200 201 202 203 @classmethod def from_rep ( cls , rep ) -> 'StaticCorpus' : \"\"\" Load from serializable rep. \"\"\" return cls ( rep )","title":"from_rep()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.load_json","text":"Same arguments as json.load. Source code in simple_uam/craidl/corpus/static.py 209 210 211 212 @classmethod def load_json ( cls , fp , ** kwargs ): \"\"\" Same arguments as json.load. \"\"\" return cls . from_rep ( json . load ( fp , ** kwargs ))","title":"load_json()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.load_json_str","text":"Same arguments as json.loads. Source code in simple_uam/craidl/corpus/static.py 214 215 216 217 @classmethod def load_json_str ( cls , s , ** kwargs ): \"\"\" Same arguments as json.loads. \"\"\" return cls . from_rep ( json . loads ( s , ** kwargs ))","title":"load_json_str()"},{"location":"reference/simple_uam/craidl/corpus/static/#simple_uam.craidl.corpus.static.StaticCorpus.to_rep","text":"Return the rep. Source code in simple_uam/craidl/corpus/static.py 205 206 207 def to_rep ( self ): \"\"\" Return the rep. \"\"\" return self . rep","title":"to_rep()"},{"location":"reference/simple_uam/direct2cad/","text":"SimpleUAM windows node setup scripts. gen_info_files ( design , metadata = None ) \u00b6 gen_info_files as an actor that will perform the task on a worker node and return metadata information. Source code in simple_uam/direct2cad/actors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @actor def gen_info_files ( design , metadata = None ): \"\"\" gen_info_files as an actor that will perform the task on a worker node and return metadata information. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"gen_info_files\" , metadata = metadata ) as session : session . write_design ( design ) session . gen_info_files ( design ) return session . metadata process_design ( design , metadata = None ) \u00b6 Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. Source code in simple_uam/direct2cad/actors.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @actor def process_design ( design , metadata = None ): \"\"\" Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"process_design\" , metadata = metadata ) as session : session . process_design ( design ) return session . metadata","title":"direct2cad"},{"location":"reference/simple_uam/direct2cad/#simple_uam.direct2cad.gen_info_files","text":"gen_info_files as an actor that will perform the task on a worker node and return metadata information. Source code in simple_uam/direct2cad/actors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @actor def gen_info_files ( design , metadata = None ): \"\"\" gen_info_files as an actor that will perform the task on a worker node and return metadata information. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"gen_info_files\" , metadata = metadata ) as session : session . write_design ( design ) session . gen_info_files ( design ) return session . metadata","title":"gen_info_files()"},{"location":"reference/simple_uam/direct2cad/#simple_uam.direct2cad.process_design","text":"Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. Source code in simple_uam/direct2cad/actors.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @actor def process_design ( design , metadata = None ): \"\"\" Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"process_design\" , metadata = metadata ) as session : session . process_design ( design ) return session . metadata","title":"process_design()"},{"location":"reference/simple_uam/direct2cad/actors/","text":"gen_info_files ( design , metadata = None ) \u00b6 gen_info_files as an actor that will perform the task on a worker node and return metadata information. Source code in simple_uam/direct2cad/actors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @actor def gen_info_files ( design , metadata = None ): \"\"\" gen_info_files as an actor that will perform the task on a worker node and return metadata information. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"gen_info_files\" , metadata = metadata ) as session : session . write_design ( design ) session . gen_info_files ( design ) return session . metadata process_design ( design , metadata = None ) \u00b6 Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. Source code in simple_uam/direct2cad/actors.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @actor def process_design ( design , metadata = None ): \"\"\" Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"process_design\" , metadata = metadata ) as session : session . process_design ( design ) return session . metadata","title":"actors"},{"location":"reference/simple_uam/direct2cad/actors/#simple_uam.direct2cad.actors.gen_info_files","text":"gen_info_files as an actor that will perform the task on a worker node and return metadata information. Source code in simple_uam/direct2cad/actors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @actor def gen_info_files ( design , metadata = None ): \"\"\" gen_info_files as an actor that will perform the task on a worker node and return metadata information. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"gen_info_files\" , metadata = metadata ) as session : session . write_design ( design ) session . gen_info_files ( design ) return session . metadata","title":"gen_info_files()"},{"location":"reference/simple_uam/direct2cad/actors/#simple_uam.direct2cad.actors.process_design","text":"Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. Source code in simple_uam/direct2cad/actors.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @actor def process_design ( design , metadata = None ): \"\"\" Processes a design on a worker node and saves the result into a result archive on the worker. Returns metadata on the worker used and archive created. \"\"\" if not metadata : metadata = dict () metadata [ 'message_info' ] = message_metadata () with D2CWorkspace ( name = \"process_design\" , metadata = metadata ) as session : session . process_design ( design ) return session . metadata","title":"process_design()"},{"location":"reference/simple_uam/direct2cad/manager/","text":"D2CManager \u00b6 Bases: WorkspaceManager A workspace manager specialized to the Direct2Cad workflow. Source code in simple_uam/direct2cad/manager.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @frozen class D2CManager ( WorkspaceManager ): \"\"\" A workspace manager specialized to the Direct2Cad workflow. \"\"\" config : D2CWorkspaceConfig = field ( default = Config [ D2CWorkspaceConfig ], init = False , ) creoson_server_subdir : str = field ( default = \"creoson-server\" , init = False , ) creoson_unpack_folder : str = field ( default = \"CreosonServerWithSetup-2.8.0-win64\" , init = False , ) def init_ref_dir ( self , reference_dir : Path , assets_dir : Path , direct2cad_repo : Path , creoson_server_zip : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" rsync_args = dict ( src = str ( direct2cad_repo ), dst = str ( reference_dir ), exclude = [ '.git' ], exclude_from = [], delete = True , update = False , progress = True , ) log . info ( \"Copying direct2cad repo into reference directory.\" , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) creoson_ref_dir = reference_dir / self . creoson_server_subdir log . info ( \"Modifying parametric.bat to reference correct Creo version\" ) para_bat = reference_dir / \"parametric.bat\" configure_file ( para_bat , para_bat , replacements = { \"6.0.4.0\" : \"5.0.6.0\" }, backup = True , exist_ok = True , ) log . info ( \"Unpacking creoson server zip into reference dir.\" , creoson_zip = str ( creoson_server_zip ), creoson_dir = str ( creoson_ref_dir ), ) with tempfile . TemporaryDirectory () as temp_dir : unpack_file ( creoson_server_zip , temp_dir ) creoson_unpack_dir = Path ( temp_dir ) / self . creoson_unpack_folder shutil . move ( creoson_unpack_dir , creoson_ref_dir ) init_ref_dir ( reference_dir , assets_dir , direct2cad_repo , creoson_server_zip ) \u00b6 This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by Source code in simple_uam/direct2cad/manager.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def init_ref_dir ( self , reference_dir : Path , assets_dir : Path , direct2cad_repo : Path , creoson_server_zip : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" rsync_args = dict ( src = str ( direct2cad_repo ), dst = str ( reference_dir ), exclude = [ '.git' ], exclude_from = [], delete = True , update = False , progress = True , ) log . info ( \"Copying direct2cad repo into reference directory.\" , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) creoson_ref_dir = reference_dir / self . creoson_server_subdir log . info ( \"Modifying parametric.bat to reference correct Creo version\" ) para_bat = reference_dir / \"parametric.bat\" configure_file ( para_bat , para_bat , replacements = { \"6.0.4.0\" : \"5.0.6.0\" }, backup = True , exist_ok = True , ) log . info ( \"Unpacking creoson server zip into reference dir.\" , creoson_zip = str ( creoson_server_zip ), creoson_dir = str ( creoson_ref_dir ), ) with tempfile . TemporaryDirectory () as temp_dir : unpack_file ( creoson_server_zip , temp_dir ) creoson_unpack_dir = Path ( temp_dir ) / self . creoson_unpack_folder shutil . move ( creoson_unpack_dir , creoson_ref_dir )","title":"manager"},{"location":"reference/simple_uam/direct2cad/manager/#simple_uam.direct2cad.manager.D2CManager","text":"Bases: WorkspaceManager A workspace manager specialized to the Direct2Cad workflow. Source code in simple_uam/direct2cad/manager.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @frozen class D2CManager ( WorkspaceManager ): \"\"\" A workspace manager specialized to the Direct2Cad workflow. \"\"\" config : D2CWorkspaceConfig = field ( default = Config [ D2CWorkspaceConfig ], init = False , ) creoson_server_subdir : str = field ( default = \"creoson-server\" , init = False , ) creoson_unpack_folder : str = field ( default = \"CreosonServerWithSetup-2.8.0-win64\" , init = False , ) def init_ref_dir ( self , reference_dir : Path , assets_dir : Path , direct2cad_repo : Path , creoson_server_zip : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" rsync_args = dict ( src = str ( direct2cad_repo ), dst = str ( reference_dir ), exclude = [ '.git' ], exclude_from = [], delete = True , update = False , progress = True , ) log . info ( \"Copying direct2cad repo into reference directory.\" , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) creoson_ref_dir = reference_dir / self . creoson_server_subdir log . info ( \"Modifying parametric.bat to reference correct Creo version\" ) para_bat = reference_dir / \"parametric.bat\" configure_file ( para_bat , para_bat , replacements = { \"6.0.4.0\" : \"5.0.6.0\" }, backup = True , exist_ok = True , ) log . info ( \"Unpacking creoson server zip into reference dir.\" , creoson_zip = str ( creoson_server_zip ), creoson_dir = str ( creoson_ref_dir ), ) with tempfile . TemporaryDirectory () as temp_dir : unpack_file ( creoson_server_zip , temp_dir ) creoson_unpack_dir = Path ( temp_dir ) / self . creoson_unpack_folder shutil . move ( creoson_unpack_dir , creoson_ref_dir )","title":"D2CManager"},{"location":"reference/simple_uam/direct2cad/manager/#simple_uam.direct2cad.manager.D2CManager.init_ref_dir","text":"This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by Source code in simple_uam/direct2cad/manager.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def init_ref_dir ( self , reference_dir : Path , assets_dir : Path , direct2cad_repo : Path , creoson_server_zip : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" rsync_args = dict ( src = str ( direct2cad_repo ), dst = str ( reference_dir ), exclude = [ '.git' ], exclude_from = [], delete = True , update = False , progress = True , ) log . info ( \"Copying direct2cad repo into reference directory.\" , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) creoson_ref_dir = reference_dir / self . creoson_server_subdir log . info ( \"Modifying parametric.bat to reference correct Creo version\" ) para_bat = reference_dir / \"parametric.bat\" configure_file ( para_bat , para_bat , replacements = { \"6.0.4.0\" : \"5.0.6.0\" }, backup = True , exist_ok = True , ) log . info ( \"Unpacking creoson server zip into reference dir.\" , creoson_zip = str ( creoson_server_zip ), creoson_dir = str ( creoson_ref_dir ), ) with tempfile . TemporaryDirectory () as temp_dir : unpack_file ( creoson_server_zip , temp_dir ) creoson_unpack_dir = Path ( temp_dir ) / self . creoson_unpack_folder shutil . move ( creoson_unpack_dir , creoson_ref_dir )","title":"init_ref_dir()"},{"location":"reference/simple_uam/direct2cad/session/","text":"D2CSession \u00b6 Bases: Session A workspace session specialized to the direct2cad workflow. Source code in simple_uam/direct2cad/session.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @define class D2CSession ( Session ): \"\"\" A workspace session specialized to the direct2cad workflow. \"\"\" @session_op def start_creo ( self ): \"\"\" Runs startCreo.bat in order to ensure that creoson and an instance of creo is running. \"\"\" log . info ( \"Starting Creo.\" , workspace = self . number , ) self . run ( [ \"startCreo.bat\" ], input = \"y \\n \" , text = True , ) @session_op def write_design ( self , design , out_file = \"design_swri.json\" ): \"\"\" Writes the design data to a file in the work directory. Arguments: design: The design object itself. out_file: The file, within the workdir, to write to. Default: 'design_swri.json' \"\"\" out_file = Path ( out_file ) if out_file . is_absolute (): raise RuntimeError ( \"out_file for writing design must be relative.\" ) out_file = self . work_dir / out_file if out_file . exists (): raise RuntimeError ( \"out_file already exists.\" ) log . info ( \"Writing design to output file.\" , workspace = self . number , out_file = str ( out_file ), ) with out_file . open ( 'w' ) as fp : json . dump ( design , fp , indent = \" \" ) @session_op def gen_info_files ( self , design ): \"\"\" Creates info files in the target directory from the provided design data. Arguments: design: The design as returned by json.load or similar. \"\"\" log . info ( \"Initializing corpus.\" , workspace = self . number , ) corpus = get_corpus ( config = Config [ CraidlConfig ] ) log . info ( \"Generating info files.\" , workspace = self . number , ) info_files = DesignInfoFiles ( corpus = corpus , design = design ) log . info ( \"Writing info files to workspace.\" , workspace = self . number , ) info_files . write_files ( self . work_dir ) @session_op def build_cad ( self ): \"\"\" Runs buildcad.py on the currently loaded info files, leaving changes and parsed results in place for session cleanup to manage. \"\"\" self . start_creo () log . info ( \"Starting buildcad.py\" , workspace = self . number , ) self . run ( [ \"python\" , \"buildcad.py\" ] ) @session_op def process_design ( self , design ): \"\"\" Runs the chain of operations needed to process a single uam design and produce FDM, cad, and other output. \"\"\" self . write_design ( design ) self . gen_info_files ( design ) self . build_cad () build_cad () \u00b6 Runs buildcad.py on the currently loaded info files, leaving changes and parsed results in place for session cleanup to manage. Source code in simple_uam/direct2cad/session.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @session_op def build_cad ( self ): \"\"\" Runs buildcad.py on the currently loaded info files, leaving changes and parsed results in place for session cleanup to manage. \"\"\" self . start_creo () log . info ( \"Starting buildcad.py\" , workspace = self . number , ) self . run ( [ \"python\" , \"buildcad.py\" ] ) gen_info_files ( design ) \u00b6 Creates info files in the target directory from the provided design data. Parameters: Name Type Description Default design The design as returned by json.load or similar. required Source code in simple_uam/direct2cad/session.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 @session_op def gen_info_files ( self , design ): \"\"\" Creates info files in the target directory from the provided design data. Arguments: design: The design as returned by json.load or similar. \"\"\" log . info ( \"Initializing corpus.\" , workspace = self . number , ) corpus = get_corpus ( config = Config [ CraidlConfig ] ) log . info ( \"Generating info files.\" , workspace = self . number , ) info_files = DesignInfoFiles ( corpus = corpus , design = design ) log . info ( \"Writing info files to workspace.\" , workspace = self . number , ) info_files . write_files ( self . work_dir ) process_design ( design ) \u00b6 Runs the chain of operations needed to process a single uam design and produce FDM, cad, and other output. Source code in simple_uam/direct2cad/session.py 121 122 123 124 125 126 127 128 129 130 @session_op def process_design ( self , design ): \"\"\" Runs the chain of operations needed to process a single uam design and produce FDM, cad, and other output. \"\"\" self . write_design ( design ) self . gen_info_files ( design ) self . build_cad () start_creo () \u00b6 Runs startCreo.bat in order to ensure that creoson and an instance of creo is running. Source code in simple_uam/direct2cad/session.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @session_op def start_creo ( self ): \"\"\" Runs startCreo.bat in order to ensure that creoson and an instance of creo is running. \"\"\" log . info ( \"Starting Creo.\" , workspace = self . number , ) self . run ( [ \"startCreo.bat\" ], input = \"y \\n \" , text = True , ) write_design ( design , out_file = 'design_swri.json' ) \u00b6 Writes the design data to a file in the work directory. Parameters: Name Type Description Default design The design object itself. required out_file The file, within the workdir, to write to. Default: 'design_swri.json' 'design_swri.json' Source code in simple_uam/direct2cad/session.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @session_op def write_design ( self , design , out_file = \"design_swri.json\" ): \"\"\" Writes the design data to a file in the work directory. Arguments: design: The design object itself. out_file: The file, within the workdir, to write to. Default: 'design_swri.json' \"\"\" out_file = Path ( out_file ) if out_file . is_absolute (): raise RuntimeError ( \"out_file for writing design must be relative.\" ) out_file = self . work_dir / out_file if out_file . exists (): raise RuntimeError ( \"out_file already exists.\" ) log . info ( \"Writing design to output file.\" , workspace = self . number , out_file = str ( out_file ), ) with out_file . open ( 'w' ) as fp : json . dump ( design , fp , indent = \" \" )","title":"session"},{"location":"reference/simple_uam/direct2cad/session/#simple_uam.direct2cad.session.D2CSession","text":"Bases: Session A workspace session specialized to the direct2cad workflow. Source code in simple_uam/direct2cad/session.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @define class D2CSession ( Session ): \"\"\" A workspace session specialized to the direct2cad workflow. \"\"\" @session_op def start_creo ( self ): \"\"\" Runs startCreo.bat in order to ensure that creoson and an instance of creo is running. \"\"\" log . info ( \"Starting Creo.\" , workspace = self . number , ) self . run ( [ \"startCreo.bat\" ], input = \"y \\n \" , text = True , ) @session_op def write_design ( self , design , out_file = \"design_swri.json\" ): \"\"\" Writes the design data to a file in the work directory. Arguments: design: The design object itself. out_file: The file, within the workdir, to write to. Default: 'design_swri.json' \"\"\" out_file = Path ( out_file ) if out_file . is_absolute (): raise RuntimeError ( \"out_file for writing design must be relative.\" ) out_file = self . work_dir / out_file if out_file . exists (): raise RuntimeError ( \"out_file already exists.\" ) log . info ( \"Writing design to output file.\" , workspace = self . number , out_file = str ( out_file ), ) with out_file . open ( 'w' ) as fp : json . dump ( design , fp , indent = \" \" ) @session_op def gen_info_files ( self , design ): \"\"\" Creates info files in the target directory from the provided design data. Arguments: design: The design as returned by json.load or similar. \"\"\" log . info ( \"Initializing corpus.\" , workspace = self . number , ) corpus = get_corpus ( config = Config [ CraidlConfig ] ) log . info ( \"Generating info files.\" , workspace = self . number , ) info_files = DesignInfoFiles ( corpus = corpus , design = design ) log . info ( \"Writing info files to workspace.\" , workspace = self . number , ) info_files . write_files ( self . work_dir ) @session_op def build_cad ( self ): \"\"\" Runs buildcad.py on the currently loaded info files, leaving changes and parsed results in place for session cleanup to manage. \"\"\" self . start_creo () log . info ( \"Starting buildcad.py\" , workspace = self . number , ) self . run ( [ \"python\" , \"buildcad.py\" ] ) @session_op def process_design ( self , design ): \"\"\" Runs the chain of operations needed to process a single uam design and produce FDM, cad, and other output. \"\"\" self . write_design ( design ) self . gen_info_files ( design ) self . build_cad ()","title":"D2CSession"},{"location":"reference/simple_uam/direct2cad/session/#simple_uam.direct2cad.session.D2CSession.build_cad","text":"Runs buildcad.py on the currently loaded info files, leaving changes and parsed results in place for session cleanup to manage. Source code in simple_uam/direct2cad/session.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @session_op def build_cad ( self ): \"\"\" Runs buildcad.py on the currently loaded info files, leaving changes and parsed results in place for session cleanup to manage. \"\"\" self . start_creo () log . info ( \"Starting buildcad.py\" , workspace = self . number , ) self . run ( [ \"python\" , \"buildcad.py\" ] )","title":"build_cad()"},{"location":"reference/simple_uam/direct2cad/session/#simple_uam.direct2cad.session.D2CSession.gen_info_files","text":"Creates info files in the target directory from the provided design data. Parameters: Name Type Description Default design The design as returned by json.load or similar. required Source code in simple_uam/direct2cad/session.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 @session_op def gen_info_files ( self , design ): \"\"\" Creates info files in the target directory from the provided design data. Arguments: design: The design as returned by json.load or similar. \"\"\" log . info ( \"Initializing corpus.\" , workspace = self . number , ) corpus = get_corpus ( config = Config [ CraidlConfig ] ) log . info ( \"Generating info files.\" , workspace = self . number , ) info_files = DesignInfoFiles ( corpus = corpus , design = design ) log . info ( \"Writing info files to workspace.\" , workspace = self . number , ) info_files . write_files ( self . work_dir )","title":"gen_info_files()"},{"location":"reference/simple_uam/direct2cad/session/#simple_uam.direct2cad.session.D2CSession.process_design","text":"Runs the chain of operations needed to process a single uam design and produce FDM, cad, and other output. Source code in simple_uam/direct2cad/session.py 121 122 123 124 125 126 127 128 129 130 @session_op def process_design ( self , design ): \"\"\" Runs the chain of operations needed to process a single uam design and produce FDM, cad, and other output. \"\"\" self . write_design ( design ) self . gen_info_files ( design ) self . build_cad ()","title":"process_design()"},{"location":"reference/simple_uam/direct2cad/session/#simple_uam.direct2cad.session.D2CSession.start_creo","text":"Runs startCreo.bat in order to ensure that creoson and an instance of creo is running. Source code in simple_uam/direct2cad/session.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @session_op def start_creo ( self ): \"\"\" Runs startCreo.bat in order to ensure that creoson and an instance of creo is running. \"\"\" log . info ( \"Starting Creo.\" , workspace = self . number , ) self . run ( [ \"startCreo.bat\" ], input = \"y \\n \" , text = True , )","title":"start_creo()"},{"location":"reference/simple_uam/direct2cad/session/#simple_uam.direct2cad.session.D2CSession.write_design","text":"Writes the design data to a file in the work directory. Parameters: Name Type Description Default design The design object itself. required out_file The file, within the workdir, to write to. Default: 'design_swri.json' 'design_swri.json' Source code in simple_uam/direct2cad/session.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @session_op def write_design ( self , design , out_file = \"design_swri.json\" ): \"\"\" Writes the design data to a file in the work directory. Arguments: design: The design object itself. out_file: The file, within the workdir, to write to. Default: 'design_swri.json' \"\"\" out_file = Path ( out_file ) if out_file . is_absolute (): raise RuntimeError ( \"out_file for writing design must be relative.\" ) out_file = self . work_dir / out_file if out_file . exists (): raise RuntimeError ( \"out_file already exists.\" ) log . info ( \"Writing design to output file.\" , workspace = self . number , out_file = str ( out_file ), ) with out_file . open ( 'w' ) as fp : json . dump ( design , fp , indent = \" \" )","title":"write_design()"},{"location":"reference/simple_uam/direct2cad/workspace/","text":"D2CWorkspace \u00b6 Bases: Workspace A temporary wrapper that represents a direct2cad workspace w/ possibly an ongoing session transaction. Mostly the same as Workspace except that it hardcodes in the manager and session_class. Source code in simple_uam/direct2cad/workspace.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @define class D2CWorkspace ( Workspace ): \"\"\" A temporary wrapper that represents a direct2cad workspace w/ possibly an ongoing session transaction. Mostly the same as Workspace except that it hardcodes in the manager and session_class. \"\"\" manager : D2CManager = field ( factory = D2CManager , on_setattr = setters . frozen , init = False , ) config : D2CWorkspaceConfig = field ( init = False , on_setattr = setters . frozen , ) \"\"\" The workspace config object that collects all the relevant paths. \"\"\" @config . default def _config_def ( self ): return self . manager . config session_class : Type [ D2CSession ] = field ( default = D2CSession , init = False , on_setattr = setters . frozen , ) \"\"\" The class we're using to generate the active session context. \"\"\" active_session : Optional [ D2CSession ] = field ( default = None , init = False , ) \"\"\" The current session context. Should be non-None after session_started. \"\"\" active_session : Optional [ D2CSession ] = field ( default = None , init = False ) class-attribute \u00b6 The current session context. Should be non-None after session_started. config : D2CWorkspaceConfig = field ( init = False , on_setattr = setters . frozen ) class-attribute \u00b6 The workspace config object that collects all the relevant paths. session_class : Type [ D2CSession ] = field ( default = D2CSession , init = False , on_setattr = setters . frozen ) class-attribute \u00b6 The class we're using to generate the active session context.","title":"workspace"},{"location":"reference/simple_uam/direct2cad/workspace/#simple_uam.direct2cad.workspace.D2CWorkspace","text":"Bases: Workspace A temporary wrapper that represents a direct2cad workspace w/ possibly an ongoing session transaction. Mostly the same as Workspace except that it hardcodes in the manager and session_class. Source code in simple_uam/direct2cad/workspace.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @define class D2CWorkspace ( Workspace ): \"\"\" A temporary wrapper that represents a direct2cad workspace w/ possibly an ongoing session transaction. Mostly the same as Workspace except that it hardcodes in the manager and session_class. \"\"\" manager : D2CManager = field ( factory = D2CManager , on_setattr = setters . frozen , init = False , ) config : D2CWorkspaceConfig = field ( init = False , on_setattr = setters . frozen , ) \"\"\" The workspace config object that collects all the relevant paths. \"\"\" @config . default def _config_def ( self ): return self . manager . config session_class : Type [ D2CSession ] = field ( default = D2CSession , init = False , on_setattr = setters . frozen , ) \"\"\" The class we're using to generate the active session context. \"\"\" active_session : Optional [ D2CSession ] = field ( default = None , init = False , ) \"\"\" The current session context. Should be non-None after session_started. \"\"\"","title":"D2CWorkspace"},{"location":"reference/simple_uam/direct2cad/workspace/#simple_uam.direct2cad.workspace.D2CWorkspace.active_session","text":"The current session context. Should be non-None after session_started.","title":"active_session"},{"location":"reference/simple_uam/direct2cad/workspace/#simple_uam.direct2cad.workspace.D2CWorkspace.config","text":"The workspace config object that collects all the relevant paths.","title":"config"},{"location":"reference/simple_uam/direct2cad/workspace/#simple_uam.direct2cad.workspace.D2CWorkspace.session_class","text":"The class we're using to generate the active session context.","title":"session_class"},{"location":"reference/simple_uam/tools/","text":"SimpleUAM windows node setup scripts.","title":"tools"},{"location":"reference/simple_uam/tools/config_mgr/","text":"SimpleUAM development utilities.","title":"config_mgr"},{"location":"reference/simple_uam/tools/config_mgr/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/config_mgr/cli.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Setup the invoke program runner class program = InvokeProg ( namespace = Collection . from_module ( simple_uam . util . config . tasks ), version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/config_mgr/cli/#simple_uam.tools.config_mgr.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/config_mgr/cli.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Setup the invoke program runner class program = InvokeProg ( namespace = Collection . from_module ( simple_uam . util . config . tasks ), version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/craidl/","text":"SimpleUAM development utilities.","title":"craidl"},{"location":"reference/simple_uam/tools/craidl/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/craidl/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" sri_ns = Collection () sri_ns . add_task ( examples . download_examples , \"download\" ) sri_ns . add_task ( examples . install_examples , \"install\" ) examples_ns = Collection () examples_ns . add_task ( examples . list_examples , 'list' ) examples_ns . add_task ( examples . add_examples , 'add' ) examples_ns . add_task ( examples . clean_examples , 'clean' ) examples_ns . add_task ( examples . examples_dir , 'dir' ) examples_ns . add_collection ( sri_ns , 'sri' ) server_ns = Collection () server_ns . add_task ( stub_server . download_server , 'download' ) server_ns . add_task ( stub_server . unpack_server , 'unpack' ) server_ns . add_task ( stub_server . configure_server , 'configure' ) server_ns . add_task ( stub_server . run_server , 'run' ) corpus_ns = Collection () corpus_ns . add_task ( stub_server . download_corpus , 'download' ) corpus_ns . add_task ( stub_server . install_corpus , 'install' ) static_corpus_ns = Collection () static_corpus_ns . add_task ( tasks . copy_static_corpus , 'copy' ) static_corpus_ns . add_task ( tasks . gen_static_corpus , 'generate' ) namespace = Collection ( tasks . gen_info_files , ) namespace . add_collection ( examples_ns , 'examples' ) namespace . add_collection ( server_ns , 'stub_server' ) namespace . add_collection ( corpus_ns , 'corpus' ) namespace . add_collection ( static_corpus_ns , 'static_corpus' ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/craidl/cli/#simple_uam.tools.craidl.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/craidl/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" sri_ns = Collection () sri_ns . add_task ( examples . download_examples , \"download\" ) sri_ns . add_task ( examples . install_examples , \"install\" ) examples_ns = Collection () examples_ns . add_task ( examples . list_examples , 'list' ) examples_ns . add_task ( examples . add_examples , 'add' ) examples_ns . add_task ( examples . clean_examples , 'clean' ) examples_ns . add_task ( examples . examples_dir , 'dir' ) examples_ns . add_collection ( sri_ns , 'sri' ) server_ns = Collection () server_ns . add_task ( stub_server . download_server , 'download' ) server_ns . add_task ( stub_server . unpack_server , 'unpack' ) server_ns . add_task ( stub_server . configure_server , 'configure' ) server_ns . add_task ( stub_server . run_server , 'run' ) corpus_ns = Collection () corpus_ns . add_task ( stub_server . download_corpus , 'download' ) corpus_ns . add_task ( stub_server . install_corpus , 'install' ) static_corpus_ns = Collection () static_corpus_ns . add_task ( tasks . copy_static_corpus , 'copy' ) static_corpus_ns . add_task ( tasks . gen_static_corpus , 'generate' ) namespace = Collection ( tasks . gen_info_files , ) namespace . add_collection ( examples_ns , 'examples' ) namespace . add_collection ( server_ns , 'stub_server' ) namespace . add_collection ( corpus_ns , 'corpus' ) namespace . add_collection ( static_corpus_ns , 'static_corpus' ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/craidl/examples/","text":"Various setup and development tasks for SimpleUAM Utility Modules. add_examples ( ctx , input , name = None , skip = False ) \u00b6 Adds the given example(s) to the examples dir. Parameters: Name Type Description Default input If a directory this will copy over all the 'design_swri.json' files in subdirectories ('*/design_swri.json') with the parent dir used as an example name. See 'name' for when input is a file. required name The name of the example to use when copying over the single file example. None skip skips any examples that already exist with the same name, otherwise overwrites them. False Source code in simple_uam/tools/craidl/examples.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @task ( positional = [ \"input\" ]) def add_examples ( ctx , input , name = None , skip = False ): \"\"\" Adds the given example(s) to the examples dir. Arguments: input: If a directory this will copy over all the 'design_swri.json' files in subdirectories ('*/design_swri.json') with the parent dir used as an example name. See 'name' for when input is a file. name: The name of the example to use when copying over the single file example. skip: skips any examples that already exist with the same name, otherwise overwrites them. \"\"\" examples = dict () input_loc = Path ( input ) . resolve ( strict = True ) log . info ( \"Attempting to add examples from input.\" , input = input , ) if input_loc . is_file () and not name : err = RuntimeError ( \"Need name argument for single fine input.\" ) log . exception ( \"Provided input file, rather than dir, and no name.\" , input = input , name = name , ) raise err elif input_loc . is_file (): examples [ name ] = input_loc else : for example in input_loc . glob ( f '*/ { design_filename } ' ): name = example . parent . name examples [ name ] = example for name , design in examples . items (): target_dir = example_path / name target_file = target_dir / design_filename target_dir . mkdir ( parents = True , exist_ok = True ) if not target_file . exists (): log . info ( \"Example does not exist, installing.\" , name = name , source = str ( design ), destination = str ( target_file ), ) shutil . copy2 ( design , target_file ) elif not skip : log . warning ( \"Example already exists, deleting and reinstalling.\" , name = name , source = str ( design ), destination = str ( target_file ), ) target_file . unlink () shutil . copy2 ( design , target_file ) else : log . info ( \"Example already exists, skipping.\" , name = name , source = str ( design ), destination = str ( target_file ), ) all_examples () \u00b6 Returns a map of all examples from the example directory. Source code in simple_uam/tools/craidl/examples.py 24 25 26 27 28 29 30 31 32 33 34 35 def all_examples (): \"\"\" Returns a map of all examples from the example directory. \"\"\" examples = dict () for example in example_path . glob ( f '*/ { design_filename } ' ): name = example . parent . name examples [ name ] = example return examples clean_examples ( ctx ) \u00b6 Deletes all available examples in the default directory. Source code in simple_uam/tools/craidl/examples.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @task def clean_examples ( ctx ): \"\"\" Deletes all available examples in the default directory. \"\"\" example_path . mkdir ( parents = True , exist_ok = True ) for name , design_file in all_examples () . items (): design_file = design_file . resolve () log . info ( \"Deleting example design.\" , name = name , design_file = str ( design_file ), ) design_file . unlink () for example in example_path . iterdir (): if example . is_dir (): if sum ( 1 for _ in example . iterdir ()) == 0 : log . info ( \"Deleting example directory.\" , design_dir = str ( example ), ) example . rmdir () else : log . warning ( \"Example design directory still has items, skipping deletion.\" , design_dir = str ( example ), ) else : log . warning ( \"File found in example directory, skipping deletion.\" , file = str ( example ), ) download_examples ( ctx , prompt = True , quiet = False , verbose = False ) \u00b6 Clones the trinity-craidl repo into the appropriate folder. With a default config this makes the examples available for use with other options. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/craidl/examples.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 @task def download_examples ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones the trinity-craidl repo into the appropriate folder. With a default config this makes the examples available for use with other options. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = trinity_craidl_repo , deploy_dir = str ( trinity_craidl_dir ), branch = trinity_craidl_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for trinity-craidl design examples.\" , ** git_args ) Git . clone_or_pull ( ** git_args ) examples_dir ( ctx ) \u00b6 Print the current examples directory. Source code in simple_uam/tools/craidl/examples.py 235 236 237 238 239 240 @task def examples_dir ( ctx ): \"\"\" Print the current examples directory. \"\"\" print ( str ( example_path )) install_examples ( ctx ) \u00b6 Installs the trinity-craidl examples after, possibly, downloading them. Source code in simple_uam/tools/craidl/examples.py 217 218 219 220 221 222 223 224 @task ( pre = [ download_examples , call ( add_examples , input = trinity_craidl_examples )]) def install_examples ( ctx ): \"\"\" Installs the trinity-craidl examples after, possibly, downloading them. \"\"\" log . info ( \"Installed examples from trinity-craidl repo.\" ) list_examples ( ctx ) \u00b6 Lists all currently loaded examples. Source code in simple_uam/tools/craidl/examples.py 226 227 228 229 230 231 232 233 @task def list_examples ( ctx ): \"\"\" Lists all currently loaded examples. \"\"\" for name , path in all_examples () . items (): print ( f \" { name } : { str ( path ) } \" ) match_example ( ident , random = False ) \u00b6 Returns the first example from the example dir which has 'ident' as a substring. Parameters: Name Type Description Default ident the identifier substring required random return a random ex False Source code in simple_uam/tools/craidl/examples.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def match_example ( ident , random = False ) -> Tuple [ str , Path ]: \"\"\" Returns the first example from the example dir which has 'ident' as a substring. Arguments: ident: the identifier substring random: return a random ex \"\"\" for name , file in all_examples (): if ident in name : return tuple ( name , file ) if random : return random_example () else : err = RuntimeError ( \"Could not find design example with matching ident.\" ) log . exception ( \"No matching example found in available identifiers.\" , err = err , ident = ident , availabe = list ( all_examples . keys ()) ) raise err random_example () \u00b6 Returns a random example from the example directory. Source code in simple_uam/tools/craidl/examples.py 37 38 39 40 41 42 43 44 45 46 47 def random_example () -> Tuple [ str , Path ]: \"\"\" Returns a random example from the example directory. \"\"\" examples = list ( all_examples () . items ()) if len ( examples ) == 0 : raise RuntimeError ( \"No design examples installed to choose from.\" ) return random . choice ( examples )","title":"examples"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.add_examples","text":"Adds the given example(s) to the examples dir. Parameters: Name Type Description Default input If a directory this will copy over all the 'design_swri.json' files in subdirectories ('*/design_swri.json') with the parent dir used as an example name. See 'name' for when input is a file. required name The name of the example to use when copying over the single file example. None skip skips any examples that already exist with the same name, otherwise overwrites them. False Source code in simple_uam/tools/craidl/examples.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @task ( positional = [ \"input\" ]) def add_examples ( ctx , input , name = None , skip = False ): \"\"\" Adds the given example(s) to the examples dir. Arguments: input: If a directory this will copy over all the 'design_swri.json' files in subdirectories ('*/design_swri.json') with the parent dir used as an example name. See 'name' for when input is a file. name: The name of the example to use when copying over the single file example. skip: skips any examples that already exist with the same name, otherwise overwrites them. \"\"\" examples = dict () input_loc = Path ( input ) . resolve ( strict = True ) log . info ( \"Attempting to add examples from input.\" , input = input , ) if input_loc . is_file () and not name : err = RuntimeError ( \"Need name argument for single fine input.\" ) log . exception ( \"Provided input file, rather than dir, and no name.\" , input = input , name = name , ) raise err elif input_loc . is_file (): examples [ name ] = input_loc else : for example in input_loc . glob ( f '*/ { design_filename } ' ): name = example . parent . name examples [ name ] = example for name , design in examples . items (): target_dir = example_path / name target_file = target_dir / design_filename target_dir . mkdir ( parents = True , exist_ok = True ) if not target_file . exists (): log . info ( \"Example does not exist, installing.\" , name = name , source = str ( design ), destination = str ( target_file ), ) shutil . copy2 ( design , target_file ) elif not skip : log . warning ( \"Example already exists, deleting and reinstalling.\" , name = name , source = str ( design ), destination = str ( target_file ), ) target_file . unlink () shutil . copy2 ( design , target_file ) else : log . info ( \"Example already exists, skipping.\" , name = name , source = str ( design ), destination = str ( target_file ), )","title":"add_examples()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.all_examples","text":"Returns a map of all examples from the example directory. Source code in simple_uam/tools/craidl/examples.py 24 25 26 27 28 29 30 31 32 33 34 35 def all_examples (): \"\"\" Returns a map of all examples from the example directory. \"\"\" examples = dict () for example in example_path . glob ( f '*/ { design_filename } ' ): name = example . parent . name examples [ name ] = example return examples","title":"all_examples()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.clean_examples","text":"Deletes all available examples in the default directory. Source code in simple_uam/tools/craidl/examples.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @task def clean_examples ( ctx ): \"\"\" Deletes all available examples in the default directory. \"\"\" example_path . mkdir ( parents = True , exist_ok = True ) for name , design_file in all_examples () . items (): design_file = design_file . resolve () log . info ( \"Deleting example design.\" , name = name , design_file = str ( design_file ), ) design_file . unlink () for example in example_path . iterdir (): if example . is_dir (): if sum ( 1 for _ in example . iterdir ()) == 0 : log . info ( \"Deleting example directory.\" , design_dir = str ( example ), ) example . rmdir () else : log . warning ( \"Example design directory still has items, skipping deletion.\" , design_dir = str ( example ), ) else : log . warning ( \"File found in example directory, skipping deletion.\" , file = str ( example ), )","title":"clean_examples()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.download_examples","text":"Clones the trinity-craidl repo into the appropriate folder. With a default config this makes the examples available for use with other options. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/craidl/examples.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 @task def download_examples ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones the trinity-craidl repo into the appropriate folder. With a default config this makes the examples available for use with other options. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = trinity_craidl_repo , deploy_dir = str ( trinity_craidl_dir ), branch = trinity_craidl_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for trinity-craidl design examples.\" , ** git_args ) Git . clone_or_pull ( ** git_args )","title":"download_examples()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.examples_dir","text":"Print the current examples directory. Source code in simple_uam/tools/craidl/examples.py 235 236 237 238 239 240 @task def examples_dir ( ctx ): \"\"\" Print the current examples directory. \"\"\" print ( str ( example_path ))","title":"examples_dir()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.install_examples","text":"Installs the trinity-craidl examples after, possibly, downloading them. Source code in simple_uam/tools/craidl/examples.py 217 218 219 220 221 222 223 224 @task ( pre = [ download_examples , call ( add_examples , input = trinity_craidl_examples )]) def install_examples ( ctx ): \"\"\" Installs the trinity-craidl examples after, possibly, downloading them. \"\"\" log . info ( \"Installed examples from trinity-craidl repo.\" )","title":"install_examples()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.list_examples","text":"Lists all currently loaded examples. Source code in simple_uam/tools/craidl/examples.py 226 227 228 229 230 231 232 233 @task def list_examples ( ctx ): \"\"\" Lists all currently loaded examples. \"\"\" for name , path in all_examples () . items (): print ( f \" { name } : { str ( path ) } \" )","title":"list_examples()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.match_example","text":"Returns the first example from the example dir which has 'ident' as a substring. Parameters: Name Type Description Default ident the identifier substring required random return a random ex False Source code in simple_uam/tools/craidl/examples.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def match_example ( ident , random = False ) -> Tuple [ str , Path ]: \"\"\" Returns the first example from the example dir which has 'ident' as a substring. Arguments: ident: the identifier substring random: return a random ex \"\"\" for name , file in all_examples (): if ident in name : return tuple ( name , file ) if random : return random_example () else : err = RuntimeError ( \"Could not find design example with matching ident.\" ) log . exception ( \"No matching example found in available identifiers.\" , err = err , ident = ident , availabe = list ( all_examples . keys ()) ) raise err","title":"match_example()"},{"location":"reference/simple_uam/tools/craidl/examples/#simple_uam.tools.craidl.examples.random_example","text":"Returns a random example from the example directory. Source code in simple_uam/tools/craidl/examples.py 37 38 39 40 41 42 43 44 45 46 47 def random_example () -> Tuple [ str , Path ]: \"\"\" Returns a random example from the example directory. \"\"\" examples = list ( all_examples () . items ()) if len ( examples ) == 0 : raise RuntimeError ( \"No design examples installed to choose from.\" ) return random . choice ( examples )","title":"random_example()"},{"location":"reference/simple_uam/tools/craidl/stub_server/","text":"Various setup and development tasks for SimpleUAM Utility Modules. configure_server ( ctx , host = None , port = None , corpus = None ) \u00b6 Will patch the stub server's config files and copy over the corpus if needed. Parameters: Name Type Description Default host The host the server will serve on. None port The port the server will server on. None corpus The '.graphml' file to be loaded whenever the server starts. None All three arguments will default to values from 'CraidlConfig's 'stub_server' entry. Source code in simple_uam/tools/craidl/stub_server.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @task ( unpack_server ) def configure_server ( ctx , host = None , port = None , corpus = None ): \"\"\" Will patch the stub server's config files and copy over the corpus if needed. Arguments: host: The host the server will serve on. port: The port the server will server on. corpus: The '.graphml' file to be loaded whenever the server starts. All three arguments will default to values from 'CraidlConfig's 'stub_server' entry. \"\"\" # Defaults if not host : host = Config [ CraidlConfig ] . stub_server . host if not port : port = Config [ CraidlConfig ] . stub_server . port if not corpus : install_corpus ( ctx , skip = True ) corpus = Path ( Config [ CraidlConfig ] . stub_server . graphml_corpus ) else : corpus = Path ( corpus ) ## Server Conf configure_file ( server_conf_data , server_conf_path , replacements = { '<<HOSTNAME>>' : host , '<<PORT>>' : str ( port ), '<<LOAD_SCRIPT>>' : corpus_loader_target . as_posix (), }, exist_ok = True , ) ## Corpus Loader read_only = Config [ CraidlConfig ] . stub_server . read_only traversal_strats = 'ReadOnlyStrategy' if read_only else '' configure_file ( corpus_loader_data , corpus_loader_path , replacements = { '<<CORPUS_DATA>>' : corpus_data_target . as_posix (), '<<TRAVERSAL_STRATEGIES>>' : traversal_strats , }, exist_ok = True , ) ## Corpus schema if corpus_data_path . is_symlink (): log . info ( \"Unlinking corpus data symlink.\" , corpus_link = str ( corpus_data_path ), ) corpus_data_path . unlink () elif corpus_data_path . is_file (): log . warning ( \"Corpus file exists and isn't symlink, moving to backup.\" , corpus_file = str ( corpus_data_path ), ) backup_file ( corpus_data_path , delete = True , ) log . info ( \"Creating corpus symlink.\" , src = corpus , dest = str ( corpus_data_path ), ) corpus_data_path . symlink_to ( corpus ) download_corpus ( ctx , prompt = True , quiet = False , verbose = False ) \u00b6 Downloads the repo which has the default stub server corpus and saves it to an appropriate cache directory. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/craidl/stub_server.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @task def download_corpus ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Downloads the repo which has the default stub server corpus and saves it to an appropriate cache directory. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = uav_workflows_repo , deploy_dir = str ( uav_workflows_dir ), branch = uav_workflows_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for uav-workflows.\" , ** git_args ) Git . clone_or_pull ( ** git_args ) download_server ( ctx , force_download = False ) \u00b6 Downloads, and the gremlin server stub. Parameters: Name Type Description Default force_download Download even if the archive is already there. False Source code in simple_uam/tools/craidl/stub_server.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 @task def download_server ( ctx , force_download = False ): \"\"\" Downloads, and the gremlin server stub. Arguments: force_download: Download even if the archive is already there. \"\"\" gremlin_server_zip . parent . mkdir ( parents = True , exist_ok = True ) if gremlin_server_zip . exists (): if force_download : log . info ( \"Archive exists with force download flag set. Deleting.\" , zip_file = str ( gremlin_server_zip ), ) gremlin_server_zip . unlink () elif gremlin_server_md5 : log . info ( \"Found existing zip for the gremlin_server, validating hash.\" , zip_file = str ( gremlin_server_zip ), md5_hash = gremlin_server_md5 , ) if not verify_file ( gremlin_server_zip , gremlin_server_md5 ): log . warning ( \"Server zip is broken redownloading.\" ) gremlin_server_zip . unlink () if gremlin_server_zip . exists (): log . info ( \"Server archive is already downloaded.\" , archive = str ( gremlin_server_zip ), ) else : log . info ( \"No archive found, downloading now.\" , archive = str ( gremlin_server_zip ), download_uri = gremlin_server_uri , ) download_file ( gremlin_server_uri , gremlin_server_zip ) install_corpus ( ctx , corpus = None , skip = False , yes = False ) \u00b6 Install the stub server corpus into the configured location. Parameters: Name Type Description Default corpus The '.graphml' file to generate the corpus from, defaults to the file from the download step if found. None skip skip updating corpus if existing corpus is found. False yes skip confirmation prompt if old corpus is deleted. False Source code in simple_uam/tools/craidl/stub_server.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 @task def install_corpus ( ctx , corpus = None , skip = False , yes = False ): \"\"\" Install the stub server corpus into the configured location. Arguments: corpus: The '.graphml' file to generate the corpus from, defaults to the file from the download step if found. skip: skip updating corpus if existing corpus is found. yes: skip confirmation prompt if old corpus is deleted. \"\"\" # Default if not corpus : download_corpus ( ctx ) corpus = uav_workflows_corpus else : corpus = Path ( corpus ) # Input Checks if not corpus . is_file (): err = RuntimeError ( \"Corpus file does not exist.\" ) log . exception ( \"Could not find corpus file in the following location.\" , corpus_file = str ( corpus ), ) raise err if corpus . suffix != \".graphml\" : err = RuntimeError ( \"Corpus file must be '.graphml' format.\" ) log . exception ( \"Corpus file \" , corpus_file = str ( corpus ), ) raise err # Output Checks target = Path ( Config [ CraidlConfig ] . stub_server . graphml_corpus ) if target . exists () and skip : log . info ( \"Corpus already exists at target location, skipping update.\" , target = target , ) return elif target . exists () and not yes : log . warning ( \"Corpus already exists at target location, delete?\" , target = target , ) choice = input ( \"Confirm deletion (y/N):\" ) if choice != 'y' : raise RuntimeError ( \"Confirmation not given, aborting.\" ) target . unlink () target . parent . mkdir ( parents = True , exist_ok = True ) # Copying log . info ( \"Copying stub-server corpus to target location.\" , corpus = str ( corpus ), target = str ( target ), ) shutil . copy2 ( corpus , target ) run_server ( ctx ) \u00b6 Runs the stub server with the configured settings. This does not run the server as a service, it's just in the current session. Source code in simple_uam/tools/craidl/stub_server.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 @task def run_server ( ctx ): \"\"\" Runs the stub server with the configured settings. This does not run the server as a service, it's just in the current session. \"\"\" cmd = ( gremlin_server_dir / gremlin_server_cmd ) . resolve () log . info ( \"Starting gremlin stub server.\" , cmd = str ( cmd ), conf = str ( server_conf_target ), cwd = str ( gremlin_server_dir ), ) subprocess . run ( [ cmd , server_conf_target ], cwd = gremlin_server_dir , ) unpack_server ( ctx , force_unpack = False , force_download = False ) \u00b6 Unpacks the gremlin server archive into its final location. Parameters: Name Type Description Default force_unpack Will delete the current server dir before unpacking. False force_download Will force a redownload of the server archive, implies force_unpack. False Source code in simple_uam/tools/craidl/stub_server.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 @task def unpack_server ( ctx , force_unpack = False , force_download = False ): \"\"\" Unpacks the gremlin server archive into its final location. Arguments: force_unpack: Will delete the current server dir before unpacking. force_download: Will force a redownload of the server archive, implies force_unpack. \"\"\" force_unpack = force_unpack or force_download download_server ( ctx , force_download = force_download ) gremlin_server_dir . parent . mkdir ( parents = True , exist_ok = True ) if gremlin_server_dir . exists () and force_unpack : log . info ( \"Server dir already exists but force unpack is set. Deleting.\" , server_dir = str ( gremlin_server_dir ), ) shutil . rmtree ( gremlin_server_dir ) if gremlin_server_dir . exists (): log . info ( \"Server dir already exists skipping unpack.\" , server_dir = str ( gremlin_server_dir ), ) else : log . info ( \"Server dir not found, unpacking zip now.\" , server_zip = str ( gremlin_server_zip ), server_dir = str ( gremlin_server_dir ), ) with tempfile . TemporaryDirectory () as temp_dir : unpack_file ( gremlin_server_zip , temp_dir ) gremlin_unpack_dir = Path ( temp_dir ) / gremlin_unpack_folder shutil . move ( gremlin_unpack_dir , gremlin_server_dir )","title":"stub_server"},{"location":"reference/simple_uam/tools/craidl/stub_server/#simple_uam.tools.craidl.stub_server.configure_server","text":"Will patch the stub server's config files and copy over the corpus if needed. Parameters: Name Type Description Default host The host the server will serve on. None port The port the server will server on. None corpus The '.graphml' file to be loaded whenever the server starts. None All three arguments will default to values from 'CraidlConfig's 'stub_server' entry. Source code in simple_uam/tools/craidl/stub_server.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @task ( unpack_server ) def configure_server ( ctx , host = None , port = None , corpus = None ): \"\"\" Will patch the stub server's config files and copy over the corpus if needed. Arguments: host: The host the server will serve on. port: The port the server will server on. corpus: The '.graphml' file to be loaded whenever the server starts. All three arguments will default to values from 'CraidlConfig's 'stub_server' entry. \"\"\" # Defaults if not host : host = Config [ CraidlConfig ] . stub_server . host if not port : port = Config [ CraidlConfig ] . stub_server . port if not corpus : install_corpus ( ctx , skip = True ) corpus = Path ( Config [ CraidlConfig ] . stub_server . graphml_corpus ) else : corpus = Path ( corpus ) ## Server Conf configure_file ( server_conf_data , server_conf_path , replacements = { '<<HOSTNAME>>' : host , '<<PORT>>' : str ( port ), '<<LOAD_SCRIPT>>' : corpus_loader_target . as_posix (), }, exist_ok = True , ) ## Corpus Loader read_only = Config [ CraidlConfig ] . stub_server . read_only traversal_strats = 'ReadOnlyStrategy' if read_only else '' configure_file ( corpus_loader_data , corpus_loader_path , replacements = { '<<CORPUS_DATA>>' : corpus_data_target . as_posix (), '<<TRAVERSAL_STRATEGIES>>' : traversal_strats , }, exist_ok = True , ) ## Corpus schema if corpus_data_path . is_symlink (): log . info ( \"Unlinking corpus data symlink.\" , corpus_link = str ( corpus_data_path ), ) corpus_data_path . unlink () elif corpus_data_path . is_file (): log . warning ( \"Corpus file exists and isn't symlink, moving to backup.\" , corpus_file = str ( corpus_data_path ), ) backup_file ( corpus_data_path , delete = True , ) log . info ( \"Creating corpus symlink.\" , src = corpus , dest = str ( corpus_data_path ), ) corpus_data_path . symlink_to ( corpus )","title":"configure_server()"},{"location":"reference/simple_uam/tools/craidl/stub_server/#simple_uam.tools.craidl.stub_server.download_corpus","text":"Downloads the repo which has the default stub server corpus and saves it to an appropriate cache directory. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/craidl/stub_server.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @task def download_corpus ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Downloads the repo which has the default stub server corpus and saves it to an appropriate cache directory. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = uav_workflows_repo , deploy_dir = str ( uav_workflows_dir ), branch = uav_workflows_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for uav-workflows.\" , ** git_args ) Git . clone_or_pull ( ** git_args )","title":"download_corpus()"},{"location":"reference/simple_uam/tools/craidl/stub_server/#simple_uam.tools.craidl.stub_server.download_server","text":"Downloads, and the gremlin server stub. Parameters: Name Type Description Default force_download Download even if the archive is already there. False Source code in simple_uam/tools/craidl/stub_server.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 @task def download_server ( ctx , force_download = False ): \"\"\" Downloads, and the gremlin server stub. Arguments: force_download: Download even if the archive is already there. \"\"\" gremlin_server_zip . parent . mkdir ( parents = True , exist_ok = True ) if gremlin_server_zip . exists (): if force_download : log . info ( \"Archive exists with force download flag set. Deleting.\" , zip_file = str ( gremlin_server_zip ), ) gremlin_server_zip . unlink () elif gremlin_server_md5 : log . info ( \"Found existing zip for the gremlin_server, validating hash.\" , zip_file = str ( gremlin_server_zip ), md5_hash = gremlin_server_md5 , ) if not verify_file ( gremlin_server_zip , gremlin_server_md5 ): log . warning ( \"Server zip is broken redownloading.\" ) gremlin_server_zip . unlink () if gremlin_server_zip . exists (): log . info ( \"Server archive is already downloaded.\" , archive = str ( gremlin_server_zip ), ) else : log . info ( \"No archive found, downloading now.\" , archive = str ( gremlin_server_zip ), download_uri = gremlin_server_uri , ) download_file ( gremlin_server_uri , gremlin_server_zip )","title":"download_server()"},{"location":"reference/simple_uam/tools/craidl/stub_server/#simple_uam.tools.craidl.stub_server.install_corpus","text":"Install the stub server corpus into the configured location. Parameters: Name Type Description Default corpus The '.graphml' file to generate the corpus from, defaults to the file from the download step if found. None skip skip updating corpus if existing corpus is found. False yes skip confirmation prompt if old corpus is deleted. False Source code in simple_uam/tools/craidl/stub_server.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 @task def install_corpus ( ctx , corpus = None , skip = False , yes = False ): \"\"\" Install the stub server corpus into the configured location. Arguments: corpus: The '.graphml' file to generate the corpus from, defaults to the file from the download step if found. skip: skip updating corpus if existing corpus is found. yes: skip confirmation prompt if old corpus is deleted. \"\"\" # Default if not corpus : download_corpus ( ctx ) corpus = uav_workflows_corpus else : corpus = Path ( corpus ) # Input Checks if not corpus . is_file (): err = RuntimeError ( \"Corpus file does not exist.\" ) log . exception ( \"Could not find corpus file in the following location.\" , corpus_file = str ( corpus ), ) raise err if corpus . suffix != \".graphml\" : err = RuntimeError ( \"Corpus file must be '.graphml' format.\" ) log . exception ( \"Corpus file \" , corpus_file = str ( corpus ), ) raise err # Output Checks target = Path ( Config [ CraidlConfig ] . stub_server . graphml_corpus ) if target . exists () and skip : log . info ( \"Corpus already exists at target location, skipping update.\" , target = target , ) return elif target . exists () and not yes : log . warning ( \"Corpus already exists at target location, delete?\" , target = target , ) choice = input ( \"Confirm deletion (y/N):\" ) if choice != 'y' : raise RuntimeError ( \"Confirmation not given, aborting.\" ) target . unlink () target . parent . mkdir ( parents = True , exist_ok = True ) # Copying log . info ( \"Copying stub-server corpus to target location.\" , corpus = str ( corpus ), target = str ( target ), ) shutil . copy2 ( corpus , target )","title":"install_corpus()"},{"location":"reference/simple_uam/tools/craidl/stub_server/#simple_uam.tools.craidl.stub_server.run_server","text":"Runs the stub server with the configured settings. This does not run the server as a service, it's just in the current session. Source code in simple_uam/tools/craidl/stub_server.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 @task def run_server ( ctx ): \"\"\" Runs the stub server with the configured settings. This does not run the server as a service, it's just in the current session. \"\"\" cmd = ( gremlin_server_dir / gremlin_server_cmd ) . resolve () log . info ( \"Starting gremlin stub server.\" , cmd = str ( cmd ), conf = str ( server_conf_target ), cwd = str ( gremlin_server_dir ), ) subprocess . run ( [ cmd , server_conf_target ], cwd = gremlin_server_dir , )","title":"run_server()"},{"location":"reference/simple_uam/tools/craidl/stub_server/#simple_uam.tools.craidl.stub_server.unpack_server","text":"Unpacks the gremlin server archive into its final location. Parameters: Name Type Description Default force_unpack Will delete the current server dir before unpacking. False force_download Will force a redownload of the server archive, implies force_unpack. False Source code in simple_uam/tools/craidl/stub_server.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 @task def unpack_server ( ctx , force_unpack = False , force_download = False ): \"\"\" Unpacks the gremlin server archive into its final location. Arguments: force_unpack: Will delete the current server dir before unpacking. force_download: Will force a redownload of the server archive, implies force_unpack. \"\"\" force_unpack = force_unpack or force_download download_server ( ctx , force_download = force_download ) gremlin_server_dir . parent . mkdir ( parents = True , exist_ok = True ) if gremlin_server_dir . exists () and force_unpack : log . info ( \"Server dir already exists but force unpack is set. Deleting.\" , server_dir = str ( gremlin_server_dir ), ) shutil . rmtree ( gremlin_server_dir ) if gremlin_server_dir . exists (): log . info ( \"Server dir already exists skipping unpack.\" , server_dir = str ( gremlin_server_dir ), ) else : log . info ( \"Server dir not found, unpacking zip now.\" , server_zip = str ( gremlin_server_zip ), server_dir = str ( gremlin_server_dir ), ) with tempfile . TemporaryDirectory () as temp_dir : unpack_file ( gremlin_server_zip , temp_dir ) gremlin_unpack_dir = Path ( temp_dir ) / gremlin_unpack_folder shutil . move ( gremlin_unpack_dir , gremlin_server_dir )","title":"unpack_server()"},{"location":"reference/simple_uam/tools/craidl/tasks/","text":"Various setup and development tasks for SimpleUAM Utility Modules. copy_static_corpus ( ctx , input = None , force = False , backup = True ) \u00b6 Copies a provided static corpus to the configured location. Parameters: Name Type Description Default input The input copy of the static corpus, defaults to the copy provided with the repo. None force Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. False backup Should we create a backup if there's a preexisting output file? True Source code in simple_uam/tools/craidl/tasks.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @task def copy_static_corpus ( ctx , input = None , force = False , backup = True ): \"\"\" Copies a provided static corpus to the configured location. Arguments: input: The input copy of the static corpus, defaults to the copy provided with the repo. force: Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. backup: Should we create a backup if there's a preexisting output file? \"\"\" if input == None : input = repo_data_corpus input = Path ( input ) output = Config [ CraidlConfig ] . static_corpus output = Path ( output ) if output . exists () and not force : log . info ( \"Found existing corpus dump, skipping further operations.\" , output = str ( output ), ) return elif output . exists () and backup : log . info ( \"Found existing corpus dump, creating backup and removing.\" , output = str ( output ), ) backup_file ( output , delete = True ) elif output . exists (): log . warning ( \"Found existing corpus dump, deleting.\" , output = str ( output ), ) output . unlink () log . info ( \"Copying corpus data into location.\" , input_corpus = str ( input ), output_corpus = str ( output ), ) shutil . copy2 ( input , output ) gen_info_files ( ctx , design = 'design_swri.json' , output = None , copy_design = False , static = None , host = None , port = None ) \u00b6 Generates the info files for a given design. Parameters: Name Type Description Default design '.json' with design information. Default: 'design_swri.json' 'design_swri.json' output The output directory in which to place the files. Default: cwd None copy_design Do we copy the input design to the output directory? Default: False False static The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured None host The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured None port The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured None Source code in simple_uam/tools/craidl/tasks.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 @task def gen_info_files ( ctx , design = 'design_swri.json' , output = None , copy_design = False , static = None , host = None , port = None ): \"\"\" Generates the info files for a given design. Arguments: design: '.json' with design information. Default: 'design_swri.json' output: The output **directory** in which to place the files. Default: cwd copy_design: Do we copy the input design to the output directory? Default: False static: The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured host: The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured port: The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured \"\"\" design = Path ( design ) if not output : output = Path . cwd () output = Path ( output ) corpus = get_corpus ( config = Config [ CraidlConfig ], static = static , host = host , port = port ) log . info ( \"Loading designs from file.\" , design = str ( design ), ) design_rep = None with design . open () as dp : design_rep = json . load ( dp ) log . info ( \"Generating info file data.\" , ) info_files = DesignInfoFiles ( corpus = corpus , design = design_rep ) log . info ( \"Writing info files to disk.\" , output = str ( output ), copy_design = copy_design , ) output . mkdir ( parents = True , exist_ok = True ) info_files . write_files ( output ) if copy_design : shutil . copy2 ( design , output / 'design_swri.json' ) gen_static_corpus ( ctx , host = None , port = None , output = None , force = False , backup = True , cache = True , cache_dir = corpus_cache , cluster_size = 100 ) \u00b6 Generates a static corpus from a running corpus server. Parameters: Name Type Description Default host The hostname of the server we connect to. None port The port we're connecting to the server on. None output The output file to write the corpus dump to. None Secondary Arguments force: Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. backup: Should we create a backup if there's a preexisting output file? cache: Should we use a cache to generate this corpus? cache_dir: What directory should we use as a cache? cluster_size: If we're using a cache All three arguments will default to values from 'CraidlConfig' if not specified. Source code in simple_uam/tools/craidl/tasks.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 @task def gen_static_corpus ( ctx , host = None , port = None , output = None , force = False , backup = True , cache = True , cache_dir = corpus_cache , cluster_size = 100 ): \"\"\" Generates a static corpus from a running corpus server. Arguments: host: The hostname of the server we connect to. port: The port we're connecting to the server on. output: The output file to write the corpus dump to. Secondary Arguments: force: Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. backup: Should we create a backup if there's a preexisting output file? cache: Should we use a cache to generate this corpus? cache_dir: What directory should we use as a cache? cluster_size: If we're using a cache All three arguments will default to values from 'CraidlConfig' if not specified. \"\"\" ### Normalize Args if host == None : host = Config [ CraidlConfig ] . server_host if port == None : port = Config [ CraidlConfig ] . server_port if output == None : output = Config [ CraidlConfig ] . static_corpus else : force = True output = Path ( output ) cache_dir = Path ( cache_dir ) ### Clean Up Output Location if output . exists () and not force : log . info ( \"Found existing corpus dump, skipping further operations.\" , output = str ( output ), ) return elif output . exists () and backup : log . info ( \"Found existing corpus dump, creating backup and removing.\" , output = str ( output ), ) backup_file ( output , delete = True ) elif output . exists (): log . warning ( \"Found existing corpus dump, deleting.\" , output = str ( output ), ) output . unlink () ### Init cache corpus_opts = dict ( host = host , port = port , cache_dir = str ( cache_dir ), cluster_size = cluster_size , ) if cache : # Handle cases where cache was run with old options opts_file = cache_dir / corpus_cache_opts if opts_file . exists (): log . info ( \"Found existing cache options, checking for match.\" , opts_file = str ( opts_file ), ) delete_opts = False with opts_file . open ( 'r' ) as of : old_opts = json . load ( of ) if old_opts != corpus_opts : log . warning ( \"Previous cache options were different from current \" \\ \"options, deleting cache.\" , opts_file = str ( opts_file ), old_opts = old_opts , curr_opts = corpus_opts , ) delete_opts = True if delete_opts : shutil . rmtree ( cache_dir , ignore_errors = True ) # create new options file if needed cache_dir . mkdir ( parents = True , exist_ok = True ) if not opts_file . exists (): log . info ( \"Creating new cache options file.\" , opts_file = str ( opts_file ), curr_opts = corpus_opts , ) with opts_file . open ( 'w' ) as of : json . dump ( corpus_opts , of ) ### Convert Corpus log . info ( \"Starting gremlin client.\" , host = host , port = port , ) gremlin_corpus = GremlinCorpus ( host = host , port = port ) log . info ( \"Starting dump to static corpus.\" ) static_corpus = StaticCorpus . from_corpus ( gremlin_corpus , cache_dir = cache_dir if cache else None , cluster_size = cluster_size , ) ### Output Corpus log . info ( \"Writing static corpus to file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : static_corpus . dump_json ( fp )","title":"tasks"},{"location":"reference/simple_uam/tools/craidl/tasks/#simple_uam.tools.craidl.tasks.copy_static_corpus","text":"Copies a provided static corpus to the configured location. Parameters: Name Type Description Default input The input copy of the static corpus, defaults to the copy provided with the repo. None force Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. False backup Should we create a backup if there's a preexisting output file? True Source code in simple_uam/tools/craidl/tasks.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @task def copy_static_corpus ( ctx , input = None , force = False , backup = True ): \"\"\" Copies a provided static corpus to the configured location. Arguments: input: The input copy of the static corpus, defaults to the copy provided with the repo. force: Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. backup: Should we create a backup if there's a preexisting output file? \"\"\" if input == None : input = repo_data_corpus input = Path ( input ) output = Config [ CraidlConfig ] . static_corpus output = Path ( output ) if output . exists () and not force : log . info ( \"Found existing corpus dump, skipping further operations.\" , output = str ( output ), ) return elif output . exists () and backup : log . info ( \"Found existing corpus dump, creating backup and removing.\" , output = str ( output ), ) backup_file ( output , delete = True ) elif output . exists (): log . warning ( \"Found existing corpus dump, deleting.\" , output = str ( output ), ) output . unlink () log . info ( \"Copying corpus data into location.\" , input_corpus = str ( input ), output_corpus = str ( output ), ) shutil . copy2 ( input , output )","title":"copy_static_corpus()"},{"location":"reference/simple_uam/tools/craidl/tasks/#simple_uam.tools.craidl.tasks.gen_info_files","text":"Generates the info files for a given design. Parameters: Name Type Description Default design '.json' with design information. Default: 'design_swri.json' 'design_swri.json' output The output directory in which to place the files. Default: cwd None copy_design Do we copy the input design to the output directory? Default: False False static The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured None host The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured None port The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured None Source code in simple_uam/tools/craidl/tasks.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 @task def gen_info_files ( ctx , design = 'design_swri.json' , output = None , copy_design = False , static = None , host = None , port = None ): \"\"\" Generates the info files for a given design. Arguments: design: '.json' with design information. Default: 'design_swri.json' output: The output **directory** in which to place the files. Default: cwd copy_design: Do we copy the input design to the output directory? Default: False static: The static '.json' corpus to use when generating the files. Mutually exclusive with host and port. Default: As configured host: The hostname of the corpus server to use when generating the files. Mutually exclusive with static. Default: As configured port: The port of the corpus server to use when generating the files. Mutually exclusice with static. Default: As configured \"\"\" design = Path ( design ) if not output : output = Path . cwd () output = Path ( output ) corpus = get_corpus ( config = Config [ CraidlConfig ], static = static , host = host , port = port ) log . info ( \"Loading designs from file.\" , design = str ( design ), ) design_rep = None with design . open () as dp : design_rep = json . load ( dp ) log . info ( \"Generating info file data.\" , ) info_files = DesignInfoFiles ( corpus = corpus , design = design_rep ) log . info ( \"Writing info files to disk.\" , output = str ( output ), copy_design = copy_design , ) output . mkdir ( parents = True , exist_ok = True ) info_files . write_files ( output ) if copy_design : shutil . copy2 ( design , output / 'design_swri.json' )","title":"gen_info_files()"},{"location":"reference/simple_uam/tools/craidl/tasks/#simple_uam.tools.craidl.tasks.gen_static_corpus","text":"Generates a static corpus from a running corpus server. Parameters: Name Type Description Default host The hostname of the server we connect to. None port The port we're connecting to the server on. None output The output file to write the corpus dump to. None Secondary Arguments force: Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. backup: Should we create a backup if there's a preexisting output file? cache: Should we use a cache to generate this corpus? cache_dir: What directory should we use as a cache? cluster_size: If we're using a cache All three arguments will default to values from 'CraidlConfig' if not specified. Source code in simple_uam/tools/craidl/tasks.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 @task def gen_static_corpus ( ctx , host = None , port = None , output = None , force = False , backup = True , cache = True , cache_dir = corpus_cache , cluster_size = 100 ): \"\"\" Generates a static corpus from a running corpus server. Arguments: host: The hostname of the server we connect to. port: The port we're connecting to the server on. output: The output file to write the corpus dump to. Secondary Arguments: force: Do we overwrite the corpus if it's already there? Defaults to true if output is specified otherwise defaults to false. backup: Should we create a backup if there's a preexisting output file? cache: Should we use a cache to generate this corpus? cache_dir: What directory should we use as a cache? cluster_size: If we're using a cache All three arguments will default to values from 'CraidlConfig' if not specified. \"\"\" ### Normalize Args if host == None : host = Config [ CraidlConfig ] . server_host if port == None : port = Config [ CraidlConfig ] . server_port if output == None : output = Config [ CraidlConfig ] . static_corpus else : force = True output = Path ( output ) cache_dir = Path ( cache_dir ) ### Clean Up Output Location if output . exists () and not force : log . info ( \"Found existing corpus dump, skipping further operations.\" , output = str ( output ), ) return elif output . exists () and backup : log . info ( \"Found existing corpus dump, creating backup and removing.\" , output = str ( output ), ) backup_file ( output , delete = True ) elif output . exists (): log . warning ( \"Found existing corpus dump, deleting.\" , output = str ( output ), ) output . unlink () ### Init cache corpus_opts = dict ( host = host , port = port , cache_dir = str ( cache_dir ), cluster_size = cluster_size , ) if cache : # Handle cases where cache was run with old options opts_file = cache_dir / corpus_cache_opts if opts_file . exists (): log . info ( \"Found existing cache options, checking for match.\" , opts_file = str ( opts_file ), ) delete_opts = False with opts_file . open ( 'r' ) as of : old_opts = json . load ( of ) if old_opts != corpus_opts : log . warning ( \"Previous cache options were different from current \" \\ \"options, deleting cache.\" , opts_file = str ( opts_file ), old_opts = old_opts , curr_opts = corpus_opts , ) delete_opts = True if delete_opts : shutil . rmtree ( cache_dir , ignore_errors = True ) # create new options file if needed cache_dir . mkdir ( parents = True , exist_ok = True ) if not opts_file . exists (): log . info ( \"Creating new cache options file.\" , opts_file = str ( opts_file ), curr_opts = corpus_opts , ) with opts_file . open ( 'w' ) as of : json . dump ( corpus_opts , of ) ### Convert Corpus log . info ( \"Starting gremlin client.\" , host = host , port = port , ) gremlin_corpus = GremlinCorpus ( host = host , port = port ) log . info ( \"Starting dump to static corpus.\" ) static_corpus = StaticCorpus . from_corpus ( gremlin_corpus , cache_dir = cache_dir if cache else None , cluster_size = cluster_size , ) ### Output Corpus log . info ( \"Writing static corpus to file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : static_corpus . dump_json ( fp )","title":"gen_static_corpus()"},{"location":"reference/simple_uam/tools/d2c_client/","text":"SimpleUAM development utilities.","title":"d2c_client"},{"location":"reference/simple_uam/tools/d2c_client/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/d2c_client/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" namespace = Collection ( tasks . process_design , tasks . gen_info_files , ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/d2c_client/cli/#simple_uam.tools.d2c_client.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/d2c_client/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" namespace = Collection ( tasks . process_design , tasks . gen_info_files , ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/d2c_client/tasks/","text":"Various setup and development tasks for SimpleUAM Utility Modules. gen_info_files ( ctx , input = 'design_swri.json' , metadata = None , output = None ) \u00b6 Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Parameters: Name Type Description Default input The design file to read in. 'design_swri.json' metadata The json-format metadata file to include with the query. Should be a dictionary. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_client/tasks.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @task def gen_info_files ( ctx , input = 'design_swri.json' , metadata = None , output = None ): \"\"\" Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Arguments: input: The design file to read in. metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) design = Path ( input ) . resolve () metadata_json = Path ( metadata ) if metadata else None if output : output = Path ( output ) . resolve () log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) metadata = dict ( design_file = str ( design )) if metadata_json : log . info ( \"Loading metadata.\" , metadata = str ( metadata_json ), ) with metadata_json . open ( 'r' ) as fp : metadata . update ( json . load ( fp )) log . info ( \"Sending task gen_info_files to message broker.\" , design = str ( design ), ) msg = direct2cad . gen_info_files . send ( design_data , metadata = metadata ) log . info ( \"Finished sending task gen_info_files to message broker.\" , design = str ( design ), queue_name = msg . queue_name , actor_name = msg . actor_name , message_id = msg . message_id , ** msg . options , ) if not Config [ D2CWorkerConfig ] . backend . enabled : log . warning ( \"No result backend provided. Please examine the result archive \" \\ \"directory for the generated results.\" ) return None log . info ( \"Waiting for gen_info_files result.\" , ) result = msg . get_result ( block = True ) log . info ( \"Received gen_info_files result.\" , result = result , ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( result , fp ) else : print ( json . dumps ( result , indent = \" \" )) process_design ( ctx , input = 'design_swri.json' , metadata = None , output = None ) \u00b6 Runs the direct2cad pipeline on the input design files, producing output metadata and a result archive with all the generated files. Parameters: Name Type Description Default input The design file to read in. 'design_swri.json' metadata The json-format metadata file to include with the query. Should be a dictionary. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_client/tasks.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @task def process_design ( ctx , input = 'design_swri.json' , metadata = None , output = None ): \"\"\" Runs the direct2cad pipeline on the input design files, producing output metadata and a result archive with all the generated files. Arguments: input: The design file to read in. metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) design = Path ( input ) . resolve () metadata_json = Path ( metadata ) if metadata else None if output : output = Path ( output ) . resolve () log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) metadata = dict ( design_file = str ( design )) if metadata_json : log . info ( \"Loading metadata.\" , metadata = str ( metadata_json ), ) with metadata_json . open ( 'r' ) as fp : metadata . update ( json . load ( fp )) log . info ( \"Sending task process_design to message broker.\" , design = str ( design ), ) msg = direct2cad . process_design . send ( design_data , metadata = metadata ) log . info ( \"Finished sending task process_design to message broker.\" , design = str ( design ), queue_name = msg . queue_name , actor_name = msg . actor_name , message_id = msg . message_id , ** msg . options , ) if not Config [ D2CWorkerConfig ] . backend . enabled : log . warning ( \"No result backend provided. Please examine the result archive \" \\ \"directory for the generated results.\" ) return None log . info ( \"Waiting for process_design result.\" , ) result = msg . get_result ( block = True ) log . info ( \"Received process_design result.\" , result = result , ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( result , fp ) else : print ( json . dumps ( result , indent = \" \" ))","title":"tasks"},{"location":"reference/simple_uam/tools/d2c_client/tasks/#simple_uam.tools.d2c_client.tasks.gen_info_files","text":"Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Parameters: Name Type Description Default input The design file to read in. 'design_swri.json' metadata The json-format metadata file to include with the query. Should be a dictionary. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_client/tasks.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @task def gen_info_files ( ctx , input = 'design_swri.json' , metadata = None , output = None ): \"\"\" Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Arguments: input: The design file to read in. metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) design = Path ( input ) . resolve () metadata_json = Path ( metadata ) if metadata else None if output : output = Path ( output ) . resolve () log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) metadata = dict ( design_file = str ( design )) if metadata_json : log . info ( \"Loading metadata.\" , metadata = str ( metadata_json ), ) with metadata_json . open ( 'r' ) as fp : metadata . update ( json . load ( fp )) log . info ( \"Sending task gen_info_files to message broker.\" , design = str ( design ), ) msg = direct2cad . gen_info_files . send ( design_data , metadata = metadata ) log . info ( \"Finished sending task gen_info_files to message broker.\" , design = str ( design ), queue_name = msg . queue_name , actor_name = msg . actor_name , message_id = msg . message_id , ** msg . options , ) if not Config [ D2CWorkerConfig ] . backend . enabled : log . warning ( \"No result backend provided. Please examine the result archive \" \\ \"directory for the generated results.\" ) return None log . info ( \"Waiting for gen_info_files result.\" , ) result = msg . get_result ( block = True ) log . info ( \"Received gen_info_files result.\" , result = result , ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( result , fp ) else : print ( json . dumps ( result , indent = \" \" ))","title":"gen_info_files()"},{"location":"reference/simple_uam/tools/d2c_client/tasks/#simple_uam.tools.d2c_client.tasks.process_design","text":"Runs the direct2cad pipeline on the input design files, producing output metadata and a result archive with all the generated files. Parameters: Name Type Description Default input The design file to read in. 'design_swri.json' metadata The json-format metadata file to include with the query. Should be a dictionary. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_client/tasks.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @task def process_design ( ctx , input = 'design_swri.json' , metadata = None , output = None ): \"\"\" Runs the direct2cad pipeline on the input design files, producing output metadata and a result archive with all the generated files. Arguments: input: The design file to read in. metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) design = Path ( input ) . resolve () metadata_json = Path ( metadata ) if metadata else None if output : output = Path ( output ) . resolve () log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) metadata = dict ( design_file = str ( design )) if metadata_json : log . info ( \"Loading metadata.\" , metadata = str ( metadata_json ), ) with metadata_json . open ( 'r' ) as fp : metadata . update ( json . load ( fp )) log . info ( \"Sending task process_design to message broker.\" , design = str ( design ), ) msg = direct2cad . process_design . send ( design_data , metadata = metadata ) log . info ( \"Finished sending task process_design to message broker.\" , design = str ( design ), queue_name = msg . queue_name , actor_name = msg . actor_name , message_id = msg . message_id , ** msg . options , ) if not Config [ D2CWorkerConfig ] . backend . enabled : log . warning ( \"No result backend provided. Please examine the result archive \" \\ \"directory for the generated results.\" ) return None log . info ( \"Waiting for process_design result.\" , ) result = msg . get_result ( block = True ) log . info ( \"Received process_design result.\" , result = result , ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( result , fp ) else : print ( json . dumps ( result , indent = \" \" ))","title":"process_design()"},{"location":"reference/simple_uam/tools/d2c_worker/","text":"SimpleUAM development utilities.","title":"d2c_worker"},{"location":"reference/simple_uam/tools/d2c_worker/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/d2c_worker/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" namespace = Collection () namespace . add_task ( worker . run , 'run' ) namespace . add_collection ( service , 'service' ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/d2c_worker/cli/#simple_uam.tools.d2c_worker.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/d2c_worker/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" namespace = Collection () namespace . add_task ( worker . run , 'run' ) namespace . add_collection ( service , 'service' ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/d2c_worker/service/","text":"Various setup and development tasks for SimpleUAM Utility Modules. configure ( ctx ) \u00b6 (Re)configures the SimpleUAM Worker service with current settings. Source code in simple_uam/tools/d2c_worker/service.py 46 47 48 49 50 51 @task def configure ( ctx ): \"\"\" (Re)configures the SimpleUAM Worker service with current settings. \"\"\" worker_service . configure () gui_edit ( ctx ) \u00b6 Opens the NSSM GUI service editor. Source code in simple_uam/tools/d2c_worker/service.py 81 82 83 84 85 86 @task def gui_edit ( ctx ): \"\"\" Opens the NSSM GUI service editor. \"\"\" return worker_service . gui_edit () install ( ctx ) \u00b6 Installs the SimpleUAM Worker service with NSSM. Source code in simple_uam/tools/d2c_worker/service.py 29 30 31 32 33 34 @task def install ( ctx ): \"\"\" Installs the SimpleUAM Worker service with NSSM. \"\"\" worker_service . install () restart ( ctx ) \u00b6 Restarts the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 67 68 69 70 71 72 @task def restart ( ctx ): \"\"\" Restarts the SimpleUAM Worker service. \"\"\" worker_service . restart () start ( ctx ) \u00b6 Starts the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 53 54 55 56 57 58 @task def start ( ctx ): \"\"\" Starts the SimpleUAM Worker service. \"\"\" worker_service . start () status ( ctx ) \u00b6 Prints the status of the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 74 75 76 77 78 79 @task def status ( ctx ): \"\"\" Prints the status of the SimpleUAM Worker service. \"\"\" return worker_service . restart () stop ( ctx ) \u00b6 Stops the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 60 61 62 63 64 65 @task def stop ( ctx ): \"\"\" Stops the SimpleUAM Worker service. \"\"\" worker_service . stop () uninstall ( ctx , confirm = True ) \u00b6 Uninstalls the SimpleUAM Worker service with NSSM. Parameters: Name Type Description Default confirm Require a GUI confirmation box before uninstalling. True Source code in simple_uam/tools/d2c_worker/service.py 36 37 38 39 40 41 42 43 44 @task def uninstall ( ctx , confirm = True ): \"\"\" Uninstalls the SimpleUAM Worker service with NSSM. Arguments: confirm: Require a GUI confirmation box before uninstalling. \"\"\" worker_service . uninstall ()","title":"service"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.configure","text":"(Re)configures the SimpleUAM Worker service with current settings. Source code in simple_uam/tools/d2c_worker/service.py 46 47 48 49 50 51 @task def configure ( ctx ): \"\"\" (Re)configures the SimpleUAM Worker service with current settings. \"\"\" worker_service . configure ()","title":"configure()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.gui_edit","text":"Opens the NSSM GUI service editor. Source code in simple_uam/tools/d2c_worker/service.py 81 82 83 84 85 86 @task def gui_edit ( ctx ): \"\"\" Opens the NSSM GUI service editor. \"\"\" return worker_service . gui_edit ()","title":"gui_edit()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.install","text":"Installs the SimpleUAM Worker service with NSSM. Source code in simple_uam/tools/d2c_worker/service.py 29 30 31 32 33 34 @task def install ( ctx ): \"\"\" Installs the SimpleUAM Worker service with NSSM. \"\"\" worker_service . install ()","title":"install()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.restart","text":"Restarts the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 67 68 69 70 71 72 @task def restart ( ctx ): \"\"\" Restarts the SimpleUAM Worker service. \"\"\" worker_service . restart ()","title":"restart()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.start","text":"Starts the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 53 54 55 56 57 58 @task def start ( ctx ): \"\"\" Starts the SimpleUAM Worker service. \"\"\" worker_service . start ()","title":"start()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.status","text":"Prints the status of the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 74 75 76 77 78 79 @task def status ( ctx ): \"\"\" Prints the status of the SimpleUAM Worker service. \"\"\" return worker_service . restart ()","title":"status()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.stop","text":"Stops the SimpleUAM Worker service. Source code in simple_uam/tools/d2c_worker/service.py 60 61 62 63 64 65 @task def stop ( ctx ): \"\"\" Stops the SimpleUAM Worker service. \"\"\" worker_service . stop ()","title":"stop()"},{"location":"reference/simple_uam/tools/d2c_worker/service/#simple_uam.tools.d2c_worker.service.uninstall","text":"Uninstalls the SimpleUAM Worker service with NSSM. Parameters: Name Type Description Default confirm Require a GUI confirmation box before uninstalling. True Source code in simple_uam/tools/d2c_worker/service.py 36 37 38 39 40 41 42 43 44 @task def uninstall ( ctx , confirm = True ): \"\"\" Uninstalls the SimpleUAM Worker service with NSSM. Arguments: confirm: Require a GUI confirmation box before uninstalling. \"\"\" worker_service . uninstall ()","title":"uninstall()"},{"location":"reference/simple_uam/tools/d2c_worker/worker/","text":"Various setup and development tasks for SimpleUAM Utility Modules. run ( ctx , processes = 0 , threads = 0 , verbose = 0 ) \u00b6 Runs the worker node compute process. This will pull tasks from the broker and perform them. Parameters: Name Type Description Default processes Number of simultaneous worker processes. 0 threads Number of threads per worker process. 0 verbose Verbosity of output. 0 Source code in simple_uam/tools/d2c_worker/worker.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @task ( incrementable = [ 'verbose' ]) def run ( ctx , processes = 0 , threads = 0 , verbose = 0 ): \"\"\" Runs the worker node compute process. This will pull tasks from the broker and perform them. Arguments: processes: Number of simultaneous worker processes. threads: Number of threads per worker process. verbose: Verbosity of output. \"\"\" if processes <= 0 : processes = Config [ D2CWorkerConfig ] . max_processes if threads <= 0 : threads = Config [ D2CWorkerConfig ] . max_threads return run_worker_node ( modules = [ __name__ ], processes = processes , threads = threads , shutdown_timeout = Config [ D2CWorkerConfig ] . shutdown_timeout , skip_logging = Config [ D2CWorkerConfig ] . skip_logging , )","title":"worker"},{"location":"reference/simple_uam/tools/d2c_worker/worker/#simple_uam.tools.d2c_worker.worker.run","text":"Runs the worker node compute process. This will pull tasks from the broker and perform them. Parameters: Name Type Description Default processes Number of simultaneous worker processes. 0 threads Number of threads per worker process. 0 verbose Verbosity of output. 0 Source code in simple_uam/tools/d2c_worker/worker.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @task ( incrementable = [ 'verbose' ]) def run ( ctx , processes = 0 , threads = 0 , verbose = 0 ): \"\"\" Runs the worker node compute process. This will pull tasks from the broker and perform them. Arguments: processes: Number of simultaneous worker processes. threads: Number of threads per worker process. verbose: Verbosity of output. \"\"\" if processes <= 0 : processes = Config [ D2CWorkerConfig ] . max_processes if threads <= 0 : threads = Config [ D2CWorkerConfig ] . max_threads return run_worker_node ( modules = [ __name__ ], processes = processes , threads = threads , shutdown_timeout = Config [ D2CWorkerConfig ] . shutdown_timeout , skip_logging = Config [ D2CWorkerConfig ] . skip_logging , )","title":"run()"},{"location":"reference/simple_uam/tools/d2c_workspace/","text":"SimpleUAM development utilities.","title":"d2c_workspace"},{"location":"reference/simple_uam/tools/d2c_workspace/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/d2c_workspace/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" setup_ns = Collection () setup_ns . add_task ( env . mkdirs , \"dirs\" ) setup_ns . add_task ( env . creoson_server , \"creoson_server\" ) setup_ns . add_task ( env . direct2cad , \"uam_direct2cad\" ) setup_ns . add_task ( env . setup_reference , \"reference_workspace\" ) manage_ns = Collection () manage_ns . add_task ( manage . delete_locks , \"delete_locks\" ) manage_ns . add_task ( manage . prune_results , \"prune_results\" ) manage_ns . add_task ( manage . workspaces_dir , \"workspaces_dir\" ) manage_ns . add_task ( manage . cache_dir , \"cache_dir\" ) manage_ns . add_task ( manage . results_dir , \"results_dir\" ) tasks_ns = Collection () tasks_ns . add_task ( tasks . start_creo , \"start_creo\" ) tasks_ns . add_task ( tasks . gen_info_files , \"gen_info_files\" ) tasks_ns . add_task ( tasks . process_design , \"process_design\" ) namespace = Collection ( ) namespace . add_collection ( setup_ns , 'setup' ) namespace . add_collection ( manage_ns , 'manage' ) namespace . add_collection ( tasks_ns , 'tasks' ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/d2c_workspace/cli/#simple_uam.tools.d2c_workspace.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/d2c_workspace/cli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" setup_ns = Collection () setup_ns . add_task ( env . mkdirs , \"dirs\" ) setup_ns . add_task ( env . creoson_server , \"creoson_server\" ) setup_ns . add_task ( env . direct2cad , \"uam_direct2cad\" ) setup_ns . add_task ( env . setup_reference , \"reference_workspace\" ) manage_ns = Collection () manage_ns . add_task ( manage . delete_locks , \"delete_locks\" ) manage_ns . add_task ( manage . prune_results , \"prune_results\" ) manage_ns . add_task ( manage . workspaces_dir , \"workspaces_dir\" ) manage_ns . add_task ( manage . cache_dir , \"cache_dir\" ) manage_ns . add_task ( manage . results_dir , \"results_dir\" ) tasks_ns = Collection () tasks_ns . add_task ( tasks . start_creo , \"start_creo\" ) tasks_ns . add_task ( tasks . gen_info_files , \"gen_info_files\" ) tasks_ns . add_task ( tasks . process_design , \"process_design\" ) namespace = Collection ( ) namespace . add_collection ( setup_ns , 'setup' ) namespace . add_collection ( manage_ns , 'manage' ) namespace . add_collection ( tasks_ns , 'tasks' ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/d2c_workspace/env/","text":"Various setup and development tasks for SimpleUAM Utility Modules. creoson_server ( ctx , prompt = True , quiet = False , verbose = False ) \u00b6 Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/d2c_workspace/env.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @task def creoson_server ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = creoson_server_repo , deploy_dir = str ( creoson_server_dir ), branch = creoson_server_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for creoson server zip.\" , ** git_args ) Git . clone_or_pull ( ** git_args ) direct2cad ( ctx , prompt = True , quiet = False , verbose = False ) \u00b6 Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/d2c_workspace/env.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @task def direct2cad ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = direct2cad_repo , deploy_dir = str ( direct2cad_dir ), branch = direct2cad_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for uam_direct2cad.\" , ** git_args ) Git . clone_or_pull ( ** git_args ) mkdirs ( ctx ) \u00b6 Creates the various directories needed for managing direct2cad workspaces. Source code in simple_uam/tools/d2c_workspace/env.py 21 22 23 24 25 26 27 28 29 30 @task def mkdirs ( ctx ): \"\"\" Creates the various directories needed for managing direct2cad workspaces. \"\"\" log . info ( \"Initializing workspace directory structure.\" ) manager . init_dirs () setup_reference ( ctx ) \u00b6 Will set up the reference directory if needed. Note: Will delete the contents of the reference directory! Source code in simple_uam/tools/d2c_workspace/env.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @task ( mkdirs , creoson_server , direct2cad ) def setup_reference ( ctx ): \"\"\" Will set up the reference directory if needed. Note: Will delete the contents of the reference directory! \"\"\" log . info ( \"Settting up d2c workspace reference directory.\" , direct2cad_repo = str ( direct2cad_dir ), creoson_server_zip = str ( creoson_server_zip ), ) manager . setup_reference_dir ( direct2cad_repo = direct2cad_dir , creoson_server_zip = creoson_server_zip , )","title":"env"},{"location":"reference/simple_uam/tools/d2c_workspace/env/#simple_uam.tools.d2c_workspace.env.creoson_server","text":"Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/d2c_workspace/env.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @task def creoson_server ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = creoson_server_repo , deploy_dir = str ( creoson_server_dir ), branch = creoson_server_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for creoson server zip.\" , ** git_args ) Git . clone_or_pull ( ** git_args )","title":"creoson_server()"},{"location":"reference/simple_uam/tools/d2c_workspace/env/#simple_uam.tools.d2c_workspace.env.direct2cad","text":"Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/d2c_workspace/env.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @task def direct2cad ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones the creoson server repo into the appropriate cache folder. Required to setup direct2cad reference dir. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" git_args = dict ( repo_uri = direct2cad_repo , deploy_dir = str ( direct2cad_dir ), branch = direct2cad_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for uam_direct2cad.\" , ** git_args ) Git . clone_or_pull ( ** git_args )","title":"direct2cad()"},{"location":"reference/simple_uam/tools/d2c_workspace/env/#simple_uam.tools.d2c_workspace.env.mkdirs","text":"Creates the various directories needed for managing direct2cad workspaces. Source code in simple_uam/tools/d2c_workspace/env.py 21 22 23 24 25 26 27 28 29 30 @task def mkdirs ( ctx ): \"\"\" Creates the various directories needed for managing direct2cad workspaces. \"\"\" log . info ( \"Initializing workspace directory structure.\" ) manager . init_dirs ()","title":"mkdirs()"},{"location":"reference/simple_uam/tools/d2c_workspace/env/#simple_uam.tools.d2c_workspace.env.setup_reference","text":"Will set up the reference directory if needed. Note: Will delete the contents of the reference directory! Source code in simple_uam/tools/d2c_workspace/env.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @task ( mkdirs , creoson_server , direct2cad ) def setup_reference ( ctx ): \"\"\" Will set up the reference directory if needed. Note: Will delete the contents of the reference directory! \"\"\" log . info ( \"Settting up d2c workspace reference directory.\" , direct2cad_repo = str ( direct2cad_dir ), creoson_server_zip = str ( creoson_server_zip ), ) manager . setup_reference_dir ( direct2cad_repo = direct2cad_dir , creoson_server_zip = creoson_server_zip , )","title":"setup_reference()"},{"location":"reference/simple_uam/tools/d2c_workspace/manage/","text":"Various setup and development tasks for SimpleUAM Utility Modules. cache_dir ( ctx ) \u00b6 Prints the cache directory for these workspaces. Source code in simple_uam/tools/d2c_workspace/manage.py 55 56 57 58 59 60 61 @task def cache_dir ( ctx ): \"\"\" Prints the cache directory for these workspaces. \"\"\" print ( str ( Path ( manager . config . cache_dir ))) delete_locks ( ctx , skip_reference = False , skip_results = False ) \u00b6 Forcefully deletes all the locks for direct2cad workspaces. Only use this when no workspaces are operational, otherwise sessions in a workspace can clobber each other. Parameters: Name Type Description Default skip_reference If true, skip deleting the reference lockfile. False skip_results If true, skip deleting the results lock. False Source code in simple_uam/tools/d2c_workspace/manage.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @task def delete_locks ( ctx , skip_reference = False , skip_results = False ): \"\"\" Forcefully deletes all the locks for direct2cad workspaces. Only use this when no workspaces are operational, otherwise sessions in a workspace can clobber each other. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" manager . delete_locks ( skip_reference = skip_reference , skip_results = skip_results , ) prune_results ( ctx ) \u00b6 Deletes the oldest files in the results dir if there are too many. Source code in simple_uam/tools/d2c_workspace/manage.py 40 41 42 43 44 45 @task def prune_results ( ctx ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" manager . prune_results () results_dir ( ctx ) \u00b6 Prints the directory where session results are kept. Source code in simple_uam/tools/d2c_workspace/manage.py 63 64 65 66 67 68 69 @task def results_dir ( ctx ): \"\"\" Prints the directory where session results are kept. \"\"\" print ( str ( manager . config . results_path )) workspaces_dir ( ctx ) \u00b6 Prints the root directory of the workspaces. Source code in simple_uam/tools/d2c_workspace/manage.py 47 48 49 50 51 52 53 @task def workspaces_dir ( ctx ): \"\"\" Prints the root directory of the workspaces. \"\"\" print ( str ( Path ( manager . config . workspaces_dir )))","title":"manage"},{"location":"reference/simple_uam/tools/d2c_workspace/manage/#simple_uam.tools.d2c_workspace.manage.cache_dir","text":"Prints the cache directory for these workspaces. Source code in simple_uam/tools/d2c_workspace/manage.py 55 56 57 58 59 60 61 @task def cache_dir ( ctx ): \"\"\" Prints the cache directory for these workspaces. \"\"\" print ( str ( Path ( manager . config . cache_dir )))","title":"cache_dir()"},{"location":"reference/simple_uam/tools/d2c_workspace/manage/#simple_uam.tools.d2c_workspace.manage.delete_locks","text":"Forcefully deletes all the locks for direct2cad workspaces. Only use this when no workspaces are operational, otherwise sessions in a workspace can clobber each other. Parameters: Name Type Description Default skip_reference If true, skip deleting the reference lockfile. False skip_results If true, skip deleting the results lock. False Source code in simple_uam/tools/d2c_workspace/manage.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @task def delete_locks ( ctx , skip_reference = False , skip_results = False ): \"\"\" Forcefully deletes all the locks for direct2cad workspaces. Only use this when no workspaces are operational, otherwise sessions in a workspace can clobber each other. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" manager . delete_locks ( skip_reference = skip_reference , skip_results = skip_results , )","title":"delete_locks()"},{"location":"reference/simple_uam/tools/d2c_workspace/manage/#simple_uam.tools.d2c_workspace.manage.prune_results","text":"Deletes the oldest files in the results dir if there are too many. Source code in simple_uam/tools/d2c_workspace/manage.py 40 41 42 43 44 45 @task def prune_results ( ctx ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" manager . prune_results ()","title":"prune_results()"},{"location":"reference/simple_uam/tools/d2c_workspace/manage/#simple_uam.tools.d2c_workspace.manage.results_dir","text":"Prints the directory where session results are kept. Source code in simple_uam/tools/d2c_workspace/manage.py 63 64 65 66 67 68 69 @task def results_dir ( ctx ): \"\"\" Prints the directory where session results are kept. \"\"\" print ( str ( manager . config . results_path ))","title":"results_dir()"},{"location":"reference/simple_uam/tools/d2c_workspace/manage/#simple_uam.tools.d2c_workspace.manage.workspaces_dir","text":"Prints the root directory of the workspaces. Source code in simple_uam/tools/d2c_workspace/manage.py 47 48 49 50 51 52 53 @task def workspaces_dir ( ctx ): \"\"\" Prints the root directory of the workspaces. \"\"\" print ( str ( Path ( manager . config . workspaces_dir )))","title":"workspaces_dir()"},{"location":"reference/simple_uam/tools/d2c_workspace/tasks/","text":"Various setup and development tasks for SimpleUAM Utility Modules. gen_info_files ( ctx , input = 'design_swri.json' , workspace = None , output = None ) \u00b6 Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Parameters: Name Type Description Default input The design file to read in. 'design_swri.json' workspace The workspace to run this operation in. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_workspace/tasks.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @task def gen_info_files ( ctx , input = 'design_swri.json' , workspace = None , output = None ): \"\"\" Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Arguments: input: The design file to read in. workspace: The workspace to run this operation in. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" design = Path ( input ) . resolve () if output : output = Path ( output ) . resolve () log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) with D2CWorkspace ( name = \"gen-info-files\" , number = workspace ) as session : session . write_design ( design_data ) session . gen_info_files ( design_data ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( session . metadata , fp ) else : print ( json . dumps ( session . metadata , indent = \" \" )) process_design ( ctx , input = 'design_swri.json' , workspace = None , output = None ) \u00b6 Runs the direct2cad pipeline on the input design files, producing output metadata and a results archive with all the generated files. Parameters: Name Type Description Default design The design file to read in. required workspace The workspace to run this operation in. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_workspace/tasks.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @task def process_design ( ctx , input = 'design_swri.json' , workspace = None , output = None ): \"\"\" Runs the direct2cad pipeline on the input design files, producing output metadata and a results archive with all the generated files. Arguments: design: The design file to read in. workspace: The workspace to run this operation in. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" design = Path ( input ) if output : output = Path ( output ) log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) with D2CWorkspace ( name = \"process-design\" , number = workspace ) as session : session . process_design ( design_data ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( session . metadata , fp ) else : print ( json . dumps ( session . metadata , indent = \" \" )) start_creo ( ctx , workspace = None , output = None ) \u00b6 Start creo within the specified workspace, whichever's available if none. Parameters: Name Type Description Default workspace The workspace to run this operation in. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_workspace/tasks.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @task def start_creo ( ctx , workspace = None , output = None ): \"\"\" Start creo within the specified workspace, whichever's available if none. Arguments: workspace: The workspace to run this operation in. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" if output : output = Path ( output ) with D2CWorkspace ( name = \"start-creo\" , number = workspace ) as session : session . start_creo () if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( session . metadata , fp ) else : print ( json . dumps ( session . metadata , indent = \" \" ))","title":"tasks"},{"location":"reference/simple_uam/tools/d2c_workspace/tasks/#simple_uam.tools.d2c_workspace.tasks.gen_info_files","text":"Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Parameters: Name Type Description Default input The design file to read in. 'design_swri.json' workspace The workspace to run this operation in. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_workspace/tasks.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @task def gen_info_files ( ctx , input = 'design_swri.json' , workspace = None , output = None ): \"\"\" Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Arguments: input: The design file to read in. workspace: The workspace to run this operation in. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" design = Path ( input ) . resolve () if output : output = Path ( output ) . resolve () log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) with D2CWorkspace ( name = \"gen-info-files\" , number = workspace ) as session : session . write_design ( design_data ) session . gen_info_files ( design_data ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( session . metadata , fp ) else : print ( json . dumps ( session . metadata , indent = \" \" ))","title":"gen_info_files()"},{"location":"reference/simple_uam/tools/d2c_workspace/tasks/#simple_uam.tools.d2c_workspace.tasks.process_design","text":"Runs the direct2cad pipeline on the input design files, producing output metadata and a results archive with all the generated files. Parameters: Name Type Description Default design The design file to read in. required workspace The workspace to run this operation in. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_workspace/tasks.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @task def process_design ( ctx , input = 'design_swri.json' , workspace = None , output = None ): \"\"\" Runs the direct2cad pipeline on the input design files, producing output metadata and a results archive with all the generated files. Arguments: design: The design file to read in. workspace: The workspace to run this operation in. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" design = Path ( input ) if output : output = Path ( output ) log . info ( \"Loading design data.\" , input = str ( design ), ) design_data = None with design . open ( 'r' ) as fp : design_data = json . load ( fp ) with D2CWorkspace ( name = \"process-design\" , number = workspace ) as session : session . process_design ( design_data ) if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( session . metadata , fp ) else : print ( json . dumps ( session . metadata , indent = \" \" ))","title":"process_design()"},{"location":"reference/simple_uam/tools/d2c_workspace/tasks/#simple_uam.tools.d2c_workspace.tasks.start_creo","text":"Start creo within the specified workspace, whichever's available if none. Parameters: Name Type Description Default workspace The workspace to run this operation in. None output File to write output session metadata to, prints to stdout if not specified. None Source code in simple_uam/tools/d2c_workspace/tasks.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @task def start_creo ( ctx , workspace = None , output = None ): \"\"\" Start creo within the specified workspace, whichever's available if none. Arguments: workspace: The workspace to run this operation in. output: File to write output session metadata to, prints to stdout if not specified. \"\"\" if output : output = Path ( output ) with D2CWorkspace ( name = \"start-creo\" , number = workspace ) as session : session . start_creo () if output : log . info ( \"Writing session metadata to output file.\" , output = str ( output ), ) with output . open ( 'w' ) as fp : json . dump ( session . metadata , fp ) else : print ( json . dumps ( session . metadata , indent = \" \" ))","title":"start_creo()"},{"location":"reference/simple_uam/tools/dev_util/","text":"SimpleUAM development utilities.","title":"dev_util"},{"location":"reference/simple_uam/tools/dev_util/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/dev_util/cli.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Setup the invoke program runner class program = InvokeProg ( namespace = Collection . from_module ( simple_uam . tools . dev_util . tasks ), version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/dev_util/cli/#simple_uam.tools.dev_util.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/dev_util/cli.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Setup the invoke program runner class program = InvokeProg ( namespace = Collection . from_module ( simple_uam . tools . dev_util . tasks ), version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/dev_util/tasks/","text":"Various setup and development tasks for SimpleUAM Utility Modules. check ( ctx ) \u00b6 Runs both the type and code quality checkers. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 96 97 98 99 100 101 102 103 104 @task ( check_types , check_quality ) def check ( ctx ): \"\"\" Runs both the type and code quality checkers. Arguments: ctx: The context instance (passed automatically). \"\"\" pass check_docs ( ctx ) \u00b6 Check if the documentation builds correctly. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 177 178 179 180 181 182 183 184 185 186 187 @task def check_docs ( ctx ): \"\"\" Check if the documentation builds correctly. Arguments: ctx: The context instance (passed automatically). \"\"\" Path ( \"htmlcov\" ) . mkdir ( parents = True , exist_ok = True ) Path ( \"htmlcov/index.html\" ) . touch ( exist_ok = True ) ctx . run ( \"pdm run mkdocs build -s\" ) check_quality ( ctx , files = PY_SRC ) \u00b6 Check that the code is well formatted (using flake8) Parameters: Name Type Description Default ctx The context instance (passed automatically). required files The list of files to check, defaults to all '.py` PY_SRC Source code in simple_uam/tools/dev_util/tasks.py 32 33 34 35 36 37 38 39 40 41 @task def check_quality ( ctx , files = PY_SRC ): \"\"\" Check that the code is well formatted (using flake8) Arguments: ctx: The context instance (passed automatically). files: The list of files to check, defaults to all '.py` \"\"\" ctx . run ( f \"pdm run flake8 --config= { CONF_DIR } /flake8.ini { files } \" ) check_types ( ctx ) \u00b6 Check that the code is correctly typed. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @task # noqa: WPS231 def check_types ( ctx ): # noqa: WPS231 \"\"\" Check that the code is correctly typed. Arguments: ctx: The context instance (passed automatically). \"\"\" # NOTE: the following code works around this issue: # https://github.com/python/mypy/issues/10633 # compute packages directory path py = f \" { sys . version_info . major } . { sys . version_info . minor } \" pkgs_dir = Path ( \"__pypackages__\" , py , \"lib\" ) . resolve () # build the list of available packages packages = {} for package in pkgs_dir . glob ( \"*\" ): if package . suffix not in { \".dist-info\" , \".pth\" } and package . name != \"__pycache__\" : packages [ package . name ] = package # handle .pth files for pth in pkgs_dir . glob ( \"*.pth\" ): with suppress ( OSError ): for package in Path ( pth . read_text () . splitlines ()[ 0 ]) . glob ( \"*\" ): # noqa: WPS440 if package . suffix != \".dist-info\" : packages [ package . name ] = package # create a temporary directory to assign to MYPYPATH with tempfile . TemporaryDirectory () as tmpdir : # symlink the stubs ignore = set () for stubs in ( path for name , path in packages . items () if name . endswith ( \"-stubs\" )): # noqa: WPS335 Path ( tmpdir , stubs . name ) . symlink_to ( stubs , target_is_directory = True ) # try to symlink the corresponding package # see https://www.python.org/dev/peps/pep-0561/#stub-only-packages pkg_name = stubs . name . replace ( \"-stubs\" , \"\" ) if pkg_name in packages : ignore . add ( pkg_name ) Path ( tmpdir , pkg_name ) . symlink_to ( packages [ pkg_name ], target_is_directory = True ) # create temporary mypy config to ignore stubbed packages newconfig = Path ( CONF_DIR , \"mypy.ini\" ) . read_text () newconfig += \" \\n \" + \" \\n\\n \" . join ( f \"[mypy- { pkg } .*] \\n ignore_errors=true\" for pkg in ignore ) tmpconfig = Path ( tmpdir , \"mypy.ini\" ) tmpconfig . write_text ( newconfig ) # set MYPYPATH and run mypy os . environ [ \"MYPYPATH\" ] = tmpdir ctx . run ( f \"pdm run mypy --config-file { tmpconfig } { PY_SRC } \" ) clean ( ctx ) \u00b6 Delete temporary files. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @task def clean ( ctx ): \"\"\" Delete temporary files. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"rm -rfv .coverage*\" ) ctx . run ( \"rm -rfv .mypy_cache\" ) ctx . run ( \"rm -rfv .pytest_cache\" ) ctx . run ( \"rm -rfv tests/.pytest_cache\" ) ctx . run ( \"rm -rfv build\" ) ctx . run ( \"rm -rfv dist\" ) ctx . run ( \"rm -rfv htmlcov\" ) ctx . run ( \"rm -rfv pip-wheel-metadata\" ) ctx . run ( \"rm -rfv site\" ) ctx . run ( \"find . -type d -name __pycache__ | xargs rm -rf\" ) ctx . run ( \"find . -name '*.rej' -delete\" ) coverage ( ctx ) \u00b6 Report coverage as text and HTML. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 165 166 167 168 169 170 171 172 173 174 175 @task def coverage ( ctx ): \"\"\" Report coverage as text and HTML. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"pdm run coverage combine\" ) ctx . run ( f \"pdm run coverage report --rcfile= { CONF_DIR } /coverage.ini\" ) ctx . run ( f \"pdm run coverage html --rcfile= { CONF_DIR } /coverage.ini\" ) docs ( ctx ) \u00b6 Build the documentation locally. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 189 190 191 192 193 194 195 196 197 @task def docs ( ctx ): \"\"\" Build the documentation locally. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"pdm run mkdocs build\" ) docs_deploy ( ctx ) \u00b6 Deploy the documentation on GitHub pages. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 212 213 214 215 216 217 218 219 220 @task def docs_deploy ( ctx ): \"\"\" Deploy the documentation on GitHub pages. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"pdm run mkdocs gh-deploy\" ) docs_serve ( ctx , host = '0.0.0.0' , port = 8000 ) \u00b6 Serve the documentation (localhost:8000). Parameters: Name Type Description Default ctx The context instance (passed automatically). required host The host to serve the docs from. '0.0.0.0' port The port to serve the docs on. 8000 Source code in simple_uam/tools/dev_util/tasks.py 199 200 201 202 203 204 205 206 207 208 209 @task def docs_serve ( ctx , host = \"0.0.0.0\" , port = 8000 ): \"\"\" Serve the documentation (localhost:8000). Arguments: ctx: The context instance (passed automatically). host: The host to serve the docs from. port: The port to serve the docs on. \"\"\" ctx . run ( f \"pdm run mkdocs serve -a { host } : { port } \" ) format ( ctx ) \u00b6 Run formatting tools on the code. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 136 137 138 139 140 141 142 143 144 145 146 147 148 @task def format ( ctx ): \"\"\" Run formatting tools on the code. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( f \"pdm run autoflake -ir --exclude tests/fixtures --remove-all-unused-imports { PY_SRC } \" , ) ctx . run ( f \"pdm run isort { PY_SRC } \" ) ctx . run ( f \"pdm run black { PY_SRC } \" ) reset_pdm ( ctx ) \u00b6 Delete all local pdm install files. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 126 127 128 129 130 131 132 133 134 @task ( clean ) def reset_pdm ( ctx ): \"\"\" Delete all local pdm install files. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"rm -rfv pdm.lock .pdm.toml __pypackages__\" ) test ( ctx , match = '' ) \u00b6 Run the test suite. Parameters: Name Type Description Default ctx The context instance (passed automatically). required match A pytest expression to filter selected tests. '' Source code in simple_uam/tools/dev_util/tasks.py 151 152 153 154 155 156 157 158 159 160 161 162 @task def test ( ctx , match = \"\" ): \"\"\" Run the test suite. Arguments: ctx: The context instance (passed automatically). match: A pytest expression to filter selected tests. \"\"\" py_version = f \" { sys . version_info . major }{ sys . version_info . minor } \" os . environ [ \"COVERAGE_FILE\" ] = f \".coverage. { py_version } \" ctx . run ( f \"pdm run pytest -c { CONF_DIR } /pytest.ini -n auto -k { repr ( match ) } tests\" )","title":"tasks"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.check","text":"Runs both the type and code quality checkers. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 96 97 98 99 100 101 102 103 104 @task ( check_types , check_quality ) def check ( ctx ): \"\"\" Runs both the type and code quality checkers. Arguments: ctx: The context instance (passed automatically). \"\"\" pass","title":"check()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.check_docs","text":"Check if the documentation builds correctly. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 177 178 179 180 181 182 183 184 185 186 187 @task def check_docs ( ctx ): \"\"\" Check if the documentation builds correctly. Arguments: ctx: The context instance (passed automatically). \"\"\" Path ( \"htmlcov\" ) . mkdir ( parents = True , exist_ok = True ) Path ( \"htmlcov/index.html\" ) . touch ( exist_ok = True ) ctx . run ( \"pdm run mkdocs build -s\" )","title":"check_docs()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.check_quality","text":"Check that the code is well formatted (using flake8) Parameters: Name Type Description Default ctx The context instance (passed automatically). required files The list of files to check, defaults to all '.py` PY_SRC Source code in simple_uam/tools/dev_util/tasks.py 32 33 34 35 36 37 38 39 40 41 @task def check_quality ( ctx , files = PY_SRC ): \"\"\" Check that the code is well formatted (using flake8) Arguments: ctx: The context instance (passed automatically). files: The list of files to check, defaults to all '.py` \"\"\" ctx . run ( f \"pdm run flake8 --config= { CONF_DIR } /flake8.ini { files } \" )","title":"check_quality()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.check_types","text":"Check that the code is correctly typed. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @task # noqa: WPS231 def check_types ( ctx ): # noqa: WPS231 \"\"\" Check that the code is correctly typed. Arguments: ctx: The context instance (passed automatically). \"\"\" # NOTE: the following code works around this issue: # https://github.com/python/mypy/issues/10633 # compute packages directory path py = f \" { sys . version_info . major } . { sys . version_info . minor } \" pkgs_dir = Path ( \"__pypackages__\" , py , \"lib\" ) . resolve () # build the list of available packages packages = {} for package in pkgs_dir . glob ( \"*\" ): if package . suffix not in { \".dist-info\" , \".pth\" } and package . name != \"__pycache__\" : packages [ package . name ] = package # handle .pth files for pth in pkgs_dir . glob ( \"*.pth\" ): with suppress ( OSError ): for package in Path ( pth . read_text () . splitlines ()[ 0 ]) . glob ( \"*\" ): # noqa: WPS440 if package . suffix != \".dist-info\" : packages [ package . name ] = package # create a temporary directory to assign to MYPYPATH with tempfile . TemporaryDirectory () as tmpdir : # symlink the stubs ignore = set () for stubs in ( path for name , path in packages . items () if name . endswith ( \"-stubs\" )): # noqa: WPS335 Path ( tmpdir , stubs . name ) . symlink_to ( stubs , target_is_directory = True ) # try to symlink the corresponding package # see https://www.python.org/dev/peps/pep-0561/#stub-only-packages pkg_name = stubs . name . replace ( \"-stubs\" , \"\" ) if pkg_name in packages : ignore . add ( pkg_name ) Path ( tmpdir , pkg_name ) . symlink_to ( packages [ pkg_name ], target_is_directory = True ) # create temporary mypy config to ignore stubbed packages newconfig = Path ( CONF_DIR , \"mypy.ini\" ) . read_text () newconfig += \" \\n \" + \" \\n\\n \" . join ( f \"[mypy- { pkg } .*] \\n ignore_errors=true\" for pkg in ignore ) tmpconfig = Path ( tmpdir , \"mypy.ini\" ) tmpconfig . write_text ( newconfig ) # set MYPYPATH and run mypy os . environ [ \"MYPYPATH\" ] = tmpdir ctx . run ( f \"pdm run mypy --config-file { tmpconfig } { PY_SRC } \" )","title":"check_types()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.clean","text":"Delete temporary files. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @task def clean ( ctx ): \"\"\" Delete temporary files. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"rm -rfv .coverage*\" ) ctx . run ( \"rm -rfv .mypy_cache\" ) ctx . run ( \"rm -rfv .pytest_cache\" ) ctx . run ( \"rm -rfv tests/.pytest_cache\" ) ctx . run ( \"rm -rfv build\" ) ctx . run ( \"rm -rfv dist\" ) ctx . run ( \"rm -rfv htmlcov\" ) ctx . run ( \"rm -rfv pip-wheel-metadata\" ) ctx . run ( \"rm -rfv site\" ) ctx . run ( \"find . -type d -name __pycache__ | xargs rm -rf\" ) ctx . run ( \"find . -name '*.rej' -delete\" )","title":"clean()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.coverage","text":"Report coverage as text and HTML. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 165 166 167 168 169 170 171 172 173 174 175 @task def coverage ( ctx ): \"\"\" Report coverage as text and HTML. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"pdm run coverage combine\" ) ctx . run ( f \"pdm run coverage report --rcfile= { CONF_DIR } /coverage.ini\" ) ctx . run ( f \"pdm run coverage html --rcfile= { CONF_DIR } /coverage.ini\" )","title":"coverage()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.docs","text":"Build the documentation locally. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 189 190 191 192 193 194 195 196 197 @task def docs ( ctx ): \"\"\" Build the documentation locally. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"pdm run mkdocs build\" )","title":"docs()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.docs_deploy","text":"Deploy the documentation on GitHub pages. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 212 213 214 215 216 217 218 219 220 @task def docs_deploy ( ctx ): \"\"\" Deploy the documentation on GitHub pages. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"pdm run mkdocs gh-deploy\" )","title":"docs_deploy()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.docs_serve","text":"Serve the documentation (localhost:8000). Parameters: Name Type Description Default ctx The context instance (passed automatically). required host The host to serve the docs from. '0.0.0.0' port The port to serve the docs on. 8000 Source code in simple_uam/tools/dev_util/tasks.py 199 200 201 202 203 204 205 206 207 208 209 @task def docs_serve ( ctx , host = \"0.0.0.0\" , port = 8000 ): \"\"\" Serve the documentation (localhost:8000). Arguments: ctx: The context instance (passed automatically). host: The host to serve the docs from. port: The port to serve the docs on. \"\"\" ctx . run ( f \"pdm run mkdocs serve -a { host } : { port } \" )","title":"docs_serve()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.format","text":"Run formatting tools on the code. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 136 137 138 139 140 141 142 143 144 145 146 147 148 @task def format ( ctx ): \"\"\" Run formatting tools on the code. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( f \"pdm run autoflake -ir --exclude tests/fixtures --remove-all-unused-imports { PY_SRC } \" , ) ctx . run ( f \"pdm run isort { PY_SRC } \" ) ctx . run ( f \"pdm run black { PY_SRC } \" )","title":"format()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.reset_pdm","text":"Delete all local pdm install files. Parameters: Name Type Description Default ctx The context instance (passed automatically). required Source code in simple_uam/tools/dev_util/tasks.py 126 127 128 129 130 131 132 133 134 @task ( clean ) def reset_pdm ( ctx ): \"\"\" Delete all local pdm install files. Arguments: ctx: The context instance (passed automatically). \"\"\" ctx . run ( \"rm -rfv pdm.lock .pdm.toml __pypackages__\" )","title":"reset_pdm()"},{"location":"reference/simple_uam/tools/dev_util/tasks/#simple_uam.tools.dev_util.tasks.test","text":"Run the test suite. Parameters: Name Type Description Default ctx The context instance (passed automatically). required match A pytest expression to filter selected tests. '' Source code in simple_uam/tools/dev_util/tasks.py 151 152 153 154 155 156 157 158 159 160 161 162 @task def test ( ctx , match = \"\" ): \"\"\" Run the test suite. Arguments: ctx: The context instance (passed automatically). match: A pytest expression to filter selected tests. \"\"\" py_version = f \" { sys . version_info . major }{ sys . version_info . minor } \" os . environ [ \"COVERAGE_FILE\" ] = f \".coverage. { py_version } \" ctx . run ( f \"pdm run pytest -c { CONF_DIR } /pytest.ini -n auto -k { repr ( match ) } tests\" )","title":"test()"},{"location":"reference/simple_uam/tools/setup_lin/","text":"SimpleUAM windows node setup scripts.","title":"setup_lin"},{"location":"reference/simple_uam/tools/setup_lin/broker/","text":"dep_pkgs ( ctx ) \u00b6 Install/Update Message Broker Dependencies (idempotent) Source code in simple_uam/tools/setup_lin/broker.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = broker_dep_pkg_list )]) def dep_pkgs ( ctx ): \"\"\" Install/Update Message Broker Dependencies (idempotent) \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"broker"},{"location":"reference/simple_uam/tools/setup_lin/broker/#simple_uam.tools.setup_lin.broker.dep_pkgs","text":"Install/Update Message Broker Dependencies (idempotent) Source code in simple_uam/tools/setup_lin/broker.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = broker_dep_pkg_list )]) def dep_pkgs ( ctx ): \"\"\" Install/Update Message Broker Dependencies (idempotent) \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"dep_pkgs()"},{"location":"reference/simple_uam/tools/setup_lin/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/setup_lin/cli.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Tasks initialized in root of namespace namespace = Collection ( shared . mac_address , shared . clear_cache , ) # Collect all choco install tasks install = Collection () install . add_task ( worker . dep_pkgs , name = 'worker-pkgs' ) install . add_task ( license_server . dep_pkgs , name = 'licence-pkgs' ) install . add_task ( broker . dep_pkgs , name = 'broker-pkgs' ) install . add_task ( shared . qol_pkgs , name = 'qol-pkgs' ) namespace . add_collection ( install , name = 'install' ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( worker ), 'worker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( broker ), 'broker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( license_server ), 'license_server' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( choco ), 'choco' , ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/setup_lin/cli/#simple_uam.tools.setup_lin.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/setup_lin/cli.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Tasks initialized in root of namespace namespace = Collection ( shared . mac_address , shared . clear_cache , ) # Collect all choco install tasks install = Collection () install . add_task ( worker . dep_pkgs , name = 'worker-pkgs' ) install . add_task ( license_server . dep_pkgs , name = 'licence-pkgs' ) install . add_task ( broker . dep_pkgs , name = 'broker-pkgs' ) install . add_task ( shared . qol_pkgs , name = 'qol-pkgs' ) namespace . add_collection ( install , name = 'install' ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( worker ), 'worker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( broker ), 'broker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( license_server ), 'license_server' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( choco ), 'choco' , ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/setup_lin/license_server/","text":"dep_pkgs ( ctx ) \u00b6 Install/Update License Server Dependencies (idempotent) Source code in simple_uam/tools/setup_lin/license_server.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = license_dep_pkg_list )]) def dep_pkgs ( ctx ): \"\"\" Install/Update License Server Dependencies (idempotent) \"\"\" log . info ( \"Finished Installing Dependency Packages\" ) disable_firewall ( ctx ) \u00b6 Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. Source code in simple_uam/tools/setup_lin/license_server.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @task def disable_firewall ( ctx ): \"\"\" Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. \"\"\" log . info ( \"Disabling Windows Firewall.\" , ) installed = subprocess . run ([ 'NetSh' , 'Advfirewall' , 'set' , 'allprofiles' , 'state' , 'off' ]) flexnet ( ctx ) \u00b6 Install Flexnet License Server Source code in simple_uam/tools/setup_lin/license_server.py 63 64 65 66 @task ( pre = flexnet_installer . invoke_deps ) def flexnet ( ctx ): \"\"\" Install Flexnet License Server \"\"\" flexnet_installer . run ()","title":"license_server"},{"location":"reference/simple_uam/tools/setup_lin/license_server/#simple_uam.tools.setup_lin.license_server.dep_pkgs","text":"Install/Update License Server Dependencies (idempotent) Source code in simple_uam/tools/setup_lin/license_server.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = license_dep_pkg_list )]) def dep_pkgs ( ctx ): \"\"\" Install/Update License Server Dependencies (idempotent) \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"dep_pkgs()"},{"location":"reference/simple_uam/tools/setup_lin/license_server/#simple_uam.tools.setup_lin.license_server.disable_firewall","text":"Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. Source code in simple_uam/tools/setup_lin/license_server.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @task def disable_firewall ( ctx ): \"\"\" Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. \"\"\" log . info ( \"Disabling Windows Firewall.\" , ) installed = subprocess . run ([ 'NetSh' , 'Advfirewall' , 'set' , 'allprofiles' , 'state' , 'off' ])","title":"disable_firewall()"},{"location":"reference/simple_uam/tools/setup_lin/license_server/#simple_uam.tools.setup_lin.license_server.flexnet","text":"Install Flexnet License Server Source code in simple_uam/tools/setup_lin/license_server.py 63 64 65 66 @task ( pre = flexnet_installer . invoke_deps ) def flexnet ( ctx ): \"\"\" Install Flexnet License Server \"\"\" flexnet_installer . run ()","title":"flexnet()"},{"location":"reference/simple_uam/tools/setup_lin/shared/","text":"clear_cache ( ctx ) \u00b6 Delete the installer cache. Source code in simple_uam/tools/setup_lin/shared.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @task def clear_cache ( ctx ): \"\"\" Delete the installer cache. \"\"\" if not installer_cache_path . exists (): return ret = input ( f 'Delete the entire cache at { str ( installer_cache_path ) } (y/N):' ) if ret == 'N' : print ( \"Skipping deletion\" ) elif ret != 'y' : print ( \"Please enter 'y' or 'N', skipping deletion.\" ) else : shutil . rmtree ( installer_cache_path ) global_dep_pkgs ( ctx ) \u00b6 Install/Update global Dependencies (idempotent) Source code in simple_uam/tools/setup_lin/shared.py 18 19 20 21 22 @task ( pre = [ call ( install , pkg = global_pkg_list )]) def global_dep_pkgs ( ctx ): \"\"\" Install/Update global Dependencies (idempotent) \"\"\" log . info ( \"Finished Installing Dependency Packages\" ) installer_cache ( ctx ) \u00b6 Create the cache directory where installers are downloaded. Source code in simple_uam/tools/setup_lin/shared.py 40 41 42 43 44 45 46 @task def installer_cache ( ctx ): \"\"\" Create the cache directory where installers are downloaded. \"\"\" if not installer_cache_path . exists (): log . info ( \"Creating Installer Cache Dir.\" , loc = str ( installer_cache_path )) installer_cache_path . mkdir ( parents = True ) mac_address ( ctx ) \u00b6 print the mac address of this machine. Source code in simple_uam/tools/setup_lin/shared.py 32 33 34 35 @task def mac_address ( ctx ): \"\"\" print the mac address of this machine. \"\"\" print ( get_mac_address ()) qol_pkgs ( ctx ) \u00b6 Install/Update Worker QoL Packages (idempotent) Source code in simple_uam/tools/setup_lin/shared.py 26 27 28 29 30 @task ( pre = [ call ( install , pkg = qol_pkg_list )]) def qol_pkgs ( ctx ): \"\"\" Install/Update Worker QoL Packages (idempotent) \"\"\" log . info ( \"Finished Installing Quality of Life Packages\" )","title":"shared"},{"location":"reference/simple_uam/tools/setup_lin/shared/#simple_uam.tools.setup_lin.shared.clear_cache","text":"Delete the installer cache. Source code in simple_uam/tools/setup_lin/shared.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @task def clear_cache ( ctx ): \"\"\" Delete the installer cache. \"\"\" if not installer_cache_path . exists (): return ret = input ( f 'Delete the entire cache at { str ( installer_cache_path ) } (y/N):' ) if ret == 'N' : print ( \"Skipping deletion\" ) elif ret != 'y' : print ( \"Please enter 'y' or 'N', skipping deletion.\" ) else : shutil . rmtree ( installer_cache_path )","title":"clear_cache()"},{"location":"reference/simple_uam/tools/setup_lin/shared/#simple_uam.tools.setup_lin.shared.global_dep_pkgs","text":"Install/Update global Dependencies (idempotent) Source code in simple_uam/tools/setup_lin/shared.py 18 19 20 21 22 @task ( pre = [ call ( install , pkg = global_pkg_list )]) def global_dep_pkgs ( ctx ): \"\"\" Install/Update global Dependencies (idempotent) \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"global_dep_pkgs()"},{"location":"reference/simple_uam/tools/setup_lin/shared/#simple_uam.tools.setup_lin.shared.installer_cache","text":"Create the cache directory where installers are downloaded. Source code in simple_uam/tools/setup_lin/shared.py 40 41 42 43 44 45 46 @task def installer_cache ( ctx ): \"\"\" Create the cache directory where installers are downloaded. \"\"\" if not installer_cache_path . exists (): log . info ( \"Creating Installer Cache Dir.\" , loc = str ( installer_cache_path )) installer_cache_path . mkdir ( parents = True )","title":"installer_cache()"},{"location":"reference/simple_uam/tools/setup_lin/shared/#simple_uam.tools.setup_lin.shared.mac_address","text":"print the mac address of this machine. Source code in simple_uam/tools/setup_lin/shared.py 32 33 34 35 @task def mac_address ( ctx ): \"\"\" print the mac address of this machine. \"\"\" print ( get_mac_address ())","title":"mac_address()"},{"location":"reference/simple_uam/tools/setup_lin/shared/#simple_uam.tools.setup_lin.shared.qol_pkgs","text":"Install/Update Worker QoL Packages (idempotent) Source code in simple_uam/tools/setup_lin/shared.py 26 27 28 29 30 @task ( pre = [ call ( install , pkg = qol_pkg_list )]) def qol_pkgs ( ctx ): \"\"\" Install/Update Worker QoL Packages (idempotent) \"\"\" log . info ( \"Finished Installing Quality of Life Packages\" )","title":"qol_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/","text":"SimpleUAM windows node setup scripts.","title":"setup_win"},{"location":"reference/simple_uam/tools/setup_win/broker/","text":"choco_pkgs ( ctx ) \u00b6 Install/Update Message Broker Dependencies Source code in simple_uam/tools/setup_win/broker.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = broker_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update Message Broker Dependencies \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"broker"},{"location":"reference/simple_uam/tools/setup_win/broker/#simple_uam.tools.setup_win.broker.choco_pkgs","text":"Install/Update Message Broker Dependencies Source code in simple_uam/tools/setup_win/broker.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = broker_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update Message Broker Dependencies \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"choco_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/choco/","text":"install ( ctx , pkg ) \u00b6 Install Choco packages Parameters: Name Type Description Default pkg Name of the chocolatey package to install, can be provided multiple times. required Source code in simple_uam/tools/setup_win/choco.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @task ( setup , iterable = [ 'pkg' ]) def install ( ctx , pkg ): \"\"\" Install Choco packages Args: pkg: Name of the chocolatey package to install, can be provided multiple times. \"\"\" pkgs = list () for p in pkg : if p in choco_exe_map and shutil . which ( choco_exe_map [ p ]): log . info ( f \"Executable for pkg already installed.\" , pkg = p , exe = choco_exe_map [ p ] ) else : pkgs . append ( p ) if len ( pkgs ) >= 1 : log . info ( f \"Installing Chocolatey packages.\" , pkgs = pkgs ) installed = subprocess . run ([ 'powershell' , '-executionpolicy' , 'bypass' , '-File' , choco_install_script , * pkgs ]) elif len ( pkg ) <= 0 : log . warning ( \"No Chocolatey packages specified for install, skipping.\" ) else : log . info ( \"No Chocolatey packages specified for install, skipping.\" ) setup ( ctx ) \u00b6 Install Chocolatey (idempotent) Source code in simple_uam/tools/setup_win/choco.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @task def setup ( ctx ): \"\"\" Install Chocolatey (idempotent) \"\"\" if shutil . which ( 'choco.exe' ): log . info ( \"Chocolatey executable found, skipping.\" , script = str ( choco_setup_script ), ) else : log . info ( \"Installing Chocolatey.\" , script = str ( choco_setup_script ), ) installed = subprocess . run ([ 'powershell' , '-executionpolicy' , 'bypass' , '-File' , choco_setup_script ])","title":"choco"},{"location":"reference/simple_uam/tools/setup_win/choco/#simple_uam.tools.setup_win.choco.install","text":"Install Choco packages Parameters: Name Type Description Default pkg Name of the chocolatey package to install, can be provided multiple times. required Source code in simple_uam/tools/setup_win/choco.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @task ( setup , iterable = [ 'pkg' ]) def install ( ctx , pkg ): \"\"\" Install Choco packages Args: pkg: Name of the chocolatey package to install, can be provided multiple times. \"\"\" pkgs = list () for p in pkg : if p in choco_exe_map and shutil . which ( choco_exe_map [ p ]): log . info ( f \"Executable for pkg already installed.\" , pkg = p , exe = choco_exe_map [ p ] ) else : pkgs . append ( p ) if len ( pkgs ) >= 1 : log . info ( f \"Installing Chocolatey packages.\" , pkgs = pkgs ) installed = subprocess . run ([ 'powershell' , '-executionpolicy' , 'bypass' , '-File' , choco_install_script , * pkgs ]) elif len ( pkg ) <= 0 : log . warning ( \"No Chocolatey packages specified for install, skipping.\" ) else : log . info ( \"No Chocolatey packages specified for install, skipping.\" )","title":"install()"},{"location":"reference/simple_uam/tools/setup_win/choco/#simple_uam.tools.setup_win.choco.setup","text":"Install Chocolatey (idempotent) Source code in simple_uam/tools/setup_win/choco.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @task def setup ( ctx ): \"\"\" Install Chocolatey (idempotent) \"\"\" if shutil . which ( 'choco.exe' ): log . info ( \"Chocolatey executable found, skipping.\" , script = str ( choco_setup_script ), ) else : log . info ( \"Installing Chocolatey.\" , script = str ( choco_setup_script ), ) installed = subprocess . run ([ 'powershell' , '-executionpolicy' , 'bypass' , '-File' , choco_setup_script ])","title":"setup()"},{"location":"reference/simple_uam/tools/setup_win/cli/","text":"Module that contains the command line application. main ( args = None ) \u00b6 Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/setup_win/cli.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Tasks initialized in root of namespace namespace = Collection ( shared . mac_address , shared . clear_cache , ) # Collect all choco install tasks install = Collection () install . add_task ( worker . dep_pkgs , name = 'worker-deps' ) install . add_task ( license_server . choco_pkgs , name = 'license-deps' ) install . add_task ( broker . choco_pkgs , name = 'broker-deps' ) install . add_task ( shared . qol_pkgs , name = 'qol-deps' ) install . add_task ( shared . global_pkgs , name = 'global-deps' ) install . add_task ( graph_server . choco_pkgs , name = 'graph-deps' ) namespace . add_collection ( install , name = 'install' ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( worker ), 'worker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( broker ), 'broker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( license_server ), 'license_server' , ) namespace . add_collection ( Collection . from_module ( graph_server ), 'graph_server' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( choco ), 'choco' , ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"cli"},{"location":"reference/simple_uam/tools/setup_win/cli/#simple_uam.tools.setup_win.cli.main","text":"Run the main program. This function is executed when you type simpleuam-utils or python -m util . Parameters: Name Type Description Default args Optional [ List [ str ]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in simple_uam/tools/setup_win/cli.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `simpleuam-utils` or `python -m util`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" # Tasks initialized in root of namespace namespace = Collection ( shared . mac_address , shared . clear_cache , ) # Collect all choco install tasks install = Collection () install . add_task ( worker . dep_pkgs , name = 'worker-deps' ) install . add_task ( license_server . choco_pkgs , name = 'license-deps' ) install . add_task ( broker . choco_pkgs , name = 'broker-deps' ) install . add_task ( shared . qol_pkgs , name = 'qol-deps' ) install . add_task ( shared . global_pkgs , name = 'global-deps' ) install . add_task ( graph_server . choco_pkgs , name = 'graph-deps' ) namespace . add_collection ( install , name = 'install' ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( worker ), 'worker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( broker ), 'broker' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( license_server ), 'license_server' , ) namespace . add_collection ( Collection . from_module ( graph_server ), 'graph_server' , ) # Import tasks from other files/modules namespace . add_collection ( Collection . from_module ( choco ), 'choco' , ) # Setup the invoke program runner class program = InvokeProg ( namespace = namespace , version = \"0.1.0\" , ) return program . run ( argv = args )","title":"main()"},{"location":"reference/simple_uam/tools/setup_win/graph_server/","text":"choco_pkgs ( ctx ) \u00b6 Install/Update Graph Stub Server Dependencies Source code in simple_uam/tools/setup_win/graph_server.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = graph_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update Graph Stub Server Dependencies \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"graph_server"},{"location":"reference/simple_uam/tools/setup_win/graph_server/#simple_uam.tools.setup_win.graph_server.choco_pkgs","text":"Install/Update Graph Stub Server Dependencies Source code in simple_uam/tools/setup_win/graph_server.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = graph_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update Graph Stub Server Dependencies \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"choco_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/helpers/","text":"GUIInstaller \u00b6 Wrapper for a GUI install of an application. Source code in simple_uam/tools/setup_win/helpers.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 @define class GUIInstaller (): \"\"\" Wrapper for a GUI install of an application. \"\"\" installed_path : Optional [ Path ] = field ( default = None , converter = conv . optional ( Path ), kw_only = True , ) \"\"\" Path to check for an existing install. Will always install if none \"\"\" path : Path = field ( converter = Path , kw_only = True , ) \"\"\" Installer location (as subdir of install cache) \"\"\" uri : Optional [ str ] = field ( default = None , kw_only = True , ) \"\"\" URI to download installer from. Will prompt user to copy installer into location if none. \"\"\" md5 : Optional [ str ] = field ( default = None , kw_only = True , ) \"\"\" MD5 Hash for verification of installer, will skip if none. \"\"\" unpack_dir : Optional [ Path ] = field ( default = None , converter = conv . optional ( Path ), kw_only = True , ) \"\"\" Dir within install_cache to unpack download into. Assume that installer is exe if none. \"\"\" exe : Path = field ( converter = Path , kw_only = True , ) \"\"\" The executable to install, assumed to be in installer_cache dir \"\"\" @exe . default def _exe_default ( self ): if not self . unpack_dir : return self . path else : raise RuntimeError ( \"Must specify exe if unpack_dir is given.\" ) instructions : str = field ( converter = textwrap . dedent , kw_only = True , ) \"\"\" Instructions to give the user before running the installer executable. \"\"\" @property def installer_path ( self ) -> Path : return ( installer_cache_path / self . path ) . resolve () @property def installer_unpack_dir ( self ) -> Optional [ Path ]: if self . unpack_dir : return ( installer_cache_path / self . unpack_dir ) . resolve () else : return None @property def installer_exe ( self ): unpack_exe = self . installer_unpack_dir / self . exe base_exe = installer_cache_path / self . exe if base_exe . is_relative_to ( self . installer_unpack_dir ): return base_exe else : return unpack_exe @property def invoke_deps ( self ): \"\"\" Return list of dependencies for install task. \"\"\" deps = list () if self . uri : deps . append ( 'wget' ) if self . md5 : deps . append ( 'checksum' ) if self . unpack_dir : deps . append ( '7zip' ) if deps : return [ installer_cache , call ( install , pkg = deps )] else : return [ installer_cache ] @property def already_installed ( self ) -> bool : if self . installed_path : return self . installed_path . exists () else : return False def get_installer ( self ): \"\"\" Makes sure that the installer is in the correct location or throws an error. \"\"\" # If file exists try to verify if self . installer_path . exists (): if self . md5 : if not verify_file ( self . installer_path , self . md5 ): log . info ( textwrap . dedent ( \"\"\" File exists, verifying checksum. \"\"\" ), path = str ( self . installer_path ), ) # file is invalid, delete it self . installer_path . unlink () elif not self . md5 : log . warning ( \"If installer is broken delete and re-download installer.\" , installer = str ( self . installer_path ), ) # If file does not exist, try to download. if not self . installer_path . exists (): if self . uri : log . info ( textwrap . dedent ( \"\"\" No installer found. Attempting to download file to the following. \"\"\" ), path = str ( self . installer_path ), ) download_file ( self . uri , self . installer_path ) else : log . exception ( textwrap . dedent ( \"\"\" No installer found and no download uri available. Please place installer in the following location: \"\"\" ), path = str ( self . installer_path ), ) raise RuntimeError ( \"No installer found\" ) def get_installer_exe ( self ) -> Path : self . get_installer () unpack_dir = self . installer_unpack_dir if unpack_dir and not unpack_dir . exists (): log . info ( \"Unpacking installer files\" , archive = self . installer_path , target = unpack_dir , ) unpack_file ( self . installer_path , unpack_dir ) return self . installer_exe def run ( self , force = False ): \"\"\" Runs the installer. Arguments: force: Run the installer even if the application is already installed. \"\"\" if not self . already_installed or force : exe = self . get_installer_exe () run_gui_exe ( exe , self . instructions ) else : log . info ( \"Software already installed, skipping.\" , install_path = self . installed_path , ) exe : Path = field ( converter = Path , kw_only = True ) class-attribute \u00b6 The executable to install, assumed to be in installer_cache dir installed_path : Optional [ Path ] = field ( default = None , converter = conv . optional ( Path ), kw_only = True ) class-attribute \u00b6 Path to check for an existing install. Will always install if none instructions : str = field ( converter = textwrap . dedent , kw_only = True ) class-attribute \u00b6 Instructions to give the user before running the installer executable. md5 : Optional [ str ] = field ( default = None , kw_only = True ) class-attribute \u00b6 MD5 Hash for verification of installer, will skip if none. path : Path = field ( converter = Path , kw_only = True ) class-attribute \u00b6 Installer location (as subdir of install cache) unpack_dir : Optional [ Path ] = field ( default = None , converter = conv . optional ( Path ), kw_only = True ) class-attribute \u00b6 Dir within install_cache to unpack download into. Assume that installer is exe if none. uri : Optional [ str ] = field ( default = None , kw_only = True ) class-attribute \u00b6 URI to download installer from. Will prompt user to copy installer into location if none. get_installer () \u00b6 Makes sure that the installer is in the correct location or throws an error. Source code in simple_uam/tools/setup_win/helpers.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def get_installer ( self ): \"\"\" Makes sure that the installer is in the correct location or throws an error. \"\"\" # If file exists try to verify if self . installer_path . exists (): if self . md5 : if not verify_file ( self . installer_path , self . md5 ): log . info ( textwrap . dedent ( \"\"\" File exists, verifying checksum. \"\"\" ), path = str ( self . installer_path ), ) # file is invalid, delete it self . installer_path . unlink () elif not self . md5 : log . warning ( \"If installer is broken delete and re-download installer.\" , installer = str ( self . installer_path ), ) # If file does not exist, try to download. if not self . installer_path . exists (): if self . uri : log . info ( textwrap . dedent ( \"\"\" No installer found. Attempting to download file to the following. \"\"\" ), path = str ( self . installer_path ), ) download_file ( self . uri , self . installer_path ) else : log . exception ( textwrap . dedent ( \"\"\" No installer found and no download uri available. Please place installer in the following location: \"\"\" ), path = str ( self . installer_path ), ) raise RuntimeError ( \"No installer found\" ) invoke_deps () property \u00b6 Return list of dependencies for install task. Source code in simple_uam/tools/setup_win/helpers.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @property def invoke_deps ( self ): \"\"\" Return list of dependencies for install task. \"\"\" deps = list () if self . uri : deps . append ( 'wget' ) if self . md5 : deps . append ( 'checksum' ) if self . unpack_dir : deps . append ( '7zip' ) if deps : return [ installer_cache , call ( install , pkg = deps )] else : return [ installer_cache ] run ( force = False ) \u00b6 Runs the installer. Parameters: Name Type Description Default force Run the installer even if the application is already installed. False Source code in simple_uam/tools/setup_win/helpers.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run ( self , force = False ): \"\"\" Runs the installer. Arguments: force: Run the installer even if the application is already installed. \"\"\" if not self . already_installed or force : exe = self . get_installer_exe () run_gui_exe ( exe , self . instructions ) else : log . info ( \"Software already installed, skipping.\" , install_path = self . installed_path , )","title":"helpers"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller","text":"Wrapper for a GUI install of an application. Source code in simple_uam/tools/setup_win/helpers.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 @define class GUIInstaller (): \"\"\" Wrapper for a GUI install of an application. \"\"\" installed_path : Optional [ Path ] = field ( default = None , converter = conv . optional ( Path ), kw_only = True , ) \"\"\" Path to check for an existing install. Will always install if none \"\"\" path : Path = field ( converter = Path , kw_only = True , ) \"\"\" Installer location (as subdir of install cache) \"\"\" uri : Optional [ str ] = field ( default = None , kw_only = True , ) \"\"\" URI to download installer from. Will prompt user to copy installer into location if none. \"\"\" md5 : Optional [ str ] = field ( default = None , kw_only = True , ) \"\"\" MD5 Hash for verification of installer, will skip if none. \"\"\" unpack_dir : Optional [ Path ] = field ( default = None , converter = conv . optional ( Path ), kw_only = True , ) \"\"\" Dir within install_cache to unpack download into. Assume that installer is exe if none. \"\"\" exe : Path = field ( converter = Path , kw_only = True , ) \"\"\" The executable to install, assumed to be in installer_cache dir \"\"\" @exe . default def _exe_default ( self ): if not self . unpack_dir : return self . path else : raise RuntimeError ( \"Must specify exe if unpack_dir is given.\" ) instructions : str = field ( converter = textwrap . dedent , kw_only = True , ) \"\"\" Instructions to give the user before running the installer executable. \"\"\" @property def installer_path ( self ) -> Path : return ( installer_cache_path / self . path ) . resolve () @property def installer_unpack_dir ( self ) -> Optional [ Path ]: if self . unpack_dir : return ( installer_cache_path / self . unpack_dir ) . resolve () else : return None @property def installer_exe ( self ): unpack_exe = self . installer_unpack_dir / self . exe base_exe = installer_cache_path / self . exe if base_exe . is_relative_to ( self . installer_unpack_dir ): return base_exe else : return unpack_exe @property def invoke_deps ( self ): \"\"\" Return list of dependencies for install task. \"\"\" deps = list () if self . uri : deps . append ( 'wget' ) if self . md5 : deps . append ( 'checksum' ) if self . unpack_dir : deps . append ( '7zip' ) if deps : return [ installer_cache , call ( install , pkg = deps )] else : return [ installer_cache ] @property def already_installed ( self ) -> bool : if self . installed_path : return self . installed_path . exists () else : return False def get_installer ( self ): \"\"\" Makes sure that the installer is in the correct location or throws an error. \"\"\" # If file exists try to verify if self . installer_path . exists (): if self . md5 : if not verify_file ( self . installer_path , self . md5 ): log . info ( textwrap . dedent ( \"\"\" File exists, verifying checksum. \"\"\" ), path = str ( self . installer_path ), ) # file is invalid, delete it self . installer_path . unlink () elif not self . md5 : log . warning ( \"If installer is broken delete and re-download installer.\" , installer = str ( self . installer_path ), ) # If file does not exist, try to download. if not self . installer_path . exists (): if self . uri : log . info ( textwrap . dedent ( \"\"\" No installer found. Attempting to download file to the following. \"\"\" ), path = str ( self . installer_path ), ) download_file ( self . uri , self . installer_path ) else : log . exception ( textwrap . dedent ( \"\"\" No installer found and no download uri available. Please place installer in the following location: \"\"\" ), path = str ( self . installer_path ), ) raise RuntimeError ( \"No installer found\" ) def get_installer_exe ( self ) -> Path : self . get_installer () unpack_dir = self . installer_unpack_dir if unpack_dir and not unpack_dir . exists (): log . info ( \"Unpacking installer files\" , archive = self . installer_path , target = unpack_dir , ) unpack_file ( self . installer_path , unpack_dir ) return self . installer_exe def run ( self , force = False ): \"\"\" Runs the installer. Arguments: force: Run the installer even if the application is already installed. \"\"\" if not self . already_installed or force : exe = self . get_installer_exe () run_gui_exe ( exe , self . instructions ) else : log . info ( \"Software already installed, skipping.\" , install_path = self . installed_path , )","title":"GUIInstaller"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.exe","text":"The executable to install, assumed to be in installer_cache dir","title":"exe"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.installed_path","text":"Path to check for an existing install. Will always install if none","title":"installed_path"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.instructions","text":"Instructions to give the user before running the installer executable.","title":"instructions"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.md5","text":"MD5 Hash for verification of installer, will skip if none.","title":"md5"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.path","text":"Installer location (as subdir of install cache)","title":"path"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.unpack_dir","text":"Dir within install_cache to unpack download into. Assume that installer is exe if none.","title":"unpack_dir"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.uri","text":"URI to download installer from. Will prompt user to copy installer into location if none.","title":"uri"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.get_installer","text":"Makes sure that the installer is in the correct location or throws an error. Source code in simple_uam/tools/setup_win/helpers.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def get_installer ( self ): \"\"\" Makes sure that the installer is in the correct location or throws an error. \"\"\" # If file exists try to verify if self . installer_path . exists (): if self . md5 : if not verify_file ( self . installer_path , self . md5 ): log . info ( textwrap . dedent ( \"\"\" File exists, verifying checksum. \"\"\" ), path = str ( self . installer_path ), ) # file is invalid, delete it self . installer_path . unlink () elif not self . md5 : log . warning ( \"If installer is broken delete and re-download installer.\" , installer = str ( self . installer_path ), ) # If file does not exist, try to download. if not self . installer_path . exists (): if self . uri : log . info ( textwrap . dedent ( \"\"\" No installer found. Attempting to download file to the following. \"\"\" ), path = str ( self . installer_path ), ) download_file ( self . uri , self . installer_path ) else : log . exception ( textwrap . dedent ( \"\"\" No installer found and no download uri available. Please place installer in the following location: \"\"\" ), path = str ( self . installer_path ), ) raise RuntimeError ( \"No installer found\" )","title":"get_installer()"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.invoke_deps","text":"Return list of dependencies for install task. Source code in simple_uam/tools/setup_win/helpers.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @property def invoke_deps ( self ): \"\"\" Return list of dependencies for install task. \"\"\" deps = list () if self . uri : deps . append ( 'wget' ) if self . md5 : deps . append ( 'checksum' ) if self . unpack_dir : deps . append ( '7zip' ) if deps : return [ installer_cache , call ( install , pkg = deps )] else : return [ installer_cache ]","title":"invoke_deps()"},{"location":"reference/simple_uam/tools/setup_win/helpers/#simple_uam.tools.setup_win.helpers.GUIInstaller.run","text":"Runs the installer. Parameters: Name Type Description Default force Run the installer even if the application is already installed. False Source code in simple_uam/tools/setup_win/helpers.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run ( self , force = False ): \"\"\" Runs the installer. Arguments: force: Run the installer even if the application is already installed. \"\"\" if not self . already_installed or force : exe = self . get_installer_exe () run_gui_exe ( exe , self . instructions ) else : log . info ( \"Software already installed, skipping.\" , install_path = self . installed_path , )","title":"run()"},{"location":"reference/simple_uam/tools/setup_win/license_server/","text":"choco_pkgs ( ctx ) \u00b6 Install/Update License Server Dependencies. Source code in simple_uam/tools/setup_win/license_server.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = license_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update License Server Dependencies. \"\"\" log . info ( \"Finished Installing Dependency Packages\" ) disable_firewall ( ctx ) \u00b6 Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. Source code in simple_uam/tools/setup_win/license_server.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @task def disable_firewall ( ctx ): \"\"\" Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. \"\"\" log . info ( \"Disabling Windows Firewall.\" , ) installed = subprocess . run ([ 'NetSh' , 'Advfirewall' , 'set' , 'allprofiles' , 'state' , 'off' ]) flexnet ( ctx ) \u00b6 Install Flexnet License Server Source code in simple_uam/tools/setup_win/license_server.py 63 64 65 66 @task ( pre = flexnet_installer . invoke_deps ) def flexnet ( ctx ): \"\"\" Install Flexnet License Server \"\"\" flexnet_installer . run ()","title":"license_server"},{"location":"reference/simple_uam/tools/setup_win/license_server/#simple_uam.tools.setup_win.license_server.choco_pkgs","text":"Install/Update License Server Dependencies. Source code in simple_uam/tools/setup_win/license_server.py 20 21 22 23 24 @task ( pre = [ call ( choco . install , pkg = license_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update License Server Dependencies. \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"choco_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/license_server/#simple_uam.tools.setup_win.license_server.disable_firewall","text":"Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. Source code in simple_uam/tools/setup_win/license_server.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @task def disable_firewall ( ctx ): \"\"\" Disable the Windows Server firewall. (ONLY USE ON PRIVATE NETWORK) This disables the firewall for all port and connections. Do not use this if the license server can be accessed by any untrusted devices. \"\"\" log . info ( \"Disabling Windows Firewall.\" , ) installed = subprocess . run ([ 'NetSh' , 'Advfirewall' , 'set' , 'allprofiles' , 'state' , 'off' ])","title":"disable_firewall()"},{"location":"reference/simple_uam/tools/setup_win/license_server/#simple_uam.tools.setup_win.license_server.flexnet","text":"Install Flexnet License Server Source code in simple_uam/tools/setup_win/license_server.py 63 64 65 66 @task ( pre = flexnet_installer . invoke_deps ) def flexnet ( ctx ): \"\"\" Install Flexnet License Server \"\"\" flexnet_installer . run ()","title":"flexnet()"},{"location":"reference/simple_uam/tools/setup_win/shared/","text":"clear_cache ( ctx ) \u00b6 Delete the installer cache. Source code in simple_uam/tools/setup_win/shared.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @task def clear_cache ( ctx ): \"\"\" Delete the installer cache. \"\"\" if not installer_cache_path . exists (): return ret = input ( f 'Delete the entire cache at { str ( installer_cache_path ) } (y/N):' ) if ret == 'N' : print ( \"Skipping deletion\" ) elif ret != 'y' : print ( \"Please enter 'y' or 'N', skipping deletion.\" ) else : shutil . rmtree ( installer_cache_path ) global_pkgs ( ctx ) \u00b6 Install/Update Global Dependencies Source code in simple_uam/tools/setup_win/shared.py 18 19 20 21 22 @task ( pre = [ call ( install , pkg = global_pkg_list )]) def global_pkgs ( ctx ): \"\"\" Install/Update Global Dependencies \"\"\" log . info ( \"Finished Installing Dependency Packages\" ) installer_cache ( ctx ) \u00b6 Create the cache directory where installers are downloaded. Source code in simple_uam/tools/setup_win/shared.py 40 41 42 43 44 45 46 @task def installer_cache ( ctx ): \"\"\" Create the cache directory where installers are downloaded. \"\"\" if not installer_cache_path . exists (): log . info ( \"Creating Installer Cache Dir.\" , loc = str ( installer_cache_path )) installer_cache_path . mkdir ( parents = True ) mac_address ( ctx ) \u00b6 print the mac address of this machine. Source code in simple_uam/tools/setup_win/shared.py 32 33 34 35 @task def mac_address ( ctx ): \"\"\" print the mac address of this machine. \"\"\" print ( get_mac_address ()) qol_pkgs ( ctx ) \u00b6 Install/Update Worker QoL Packages Source code in simple_uam/tools/setup_win/shared.py 26 27 28 29 30 @task ( pre = [ call ( install , pkg = qol_pkg_list )]) def qol_pkgs ( ctx ): \"\"\" Install/Update Worker QoL Packages \"\"\" log . info ( \"Finished Installing Quality of Life Packages\" )","title":"shared"},{"location":"reference/simple_uam/tools/setup_win/shared/#simple_uam.tools.setup_win.shared.clear_cache","text":"Delete the installer cache. Source code in simple_uam/tools/setup_win/shared.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @task def clear_cache ( ctx ): \"\"\" Delete the installer cache. \"\"\" if not installer_cache_path . exists (): return ret = input ( f 'Delete the entire cache at { str ( installer_cache_path ) } (y/N):' ) if ret == 'N' : print ( \"Skipping deletion\" ) elif ret != 'y' : print ( \"Please enter 'y' or 'N', skipping deletion.\" ) else : shutil . rmtree ( installer_cache_path )","title":"clear_cache()"},{"location":"reference/simple_uam/tools/setup_win/shared/#simple_uam.tools.setup_win.shared.global_pkgs","text":"Install/Update Global Dependencies Source code in simple_uam/tools/setup_win/shared.py 18 19 20 21 22 @task ( pre = [ call ( install , pkg = global_pkg_list )]) def global_pkgs ( ctx ): \"\"\" Install/Update Global Dependencies \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"global_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/shared/#simple_uam.tools.setup_win.shared.installer_cache","text":"Create the cache directory where installers are downloaded. Source code in simple_uam/tools/setup_win/shared.py 40 41 42 43 44 45 46 @task def installer_cache ( ctx ): \"\"\" Create the cache directory where installers are downloaded. \"\"\" if not installer_cache_path . exists (): log . info ( \"Creating Installer Cache Dir.\" , loc = str ( installer_cache_path )) installer_cache_path . mkdir ( parents = True )","title":"installer_cache()"},{"location":"reference/simple_uam/tools/setup_win/shared/#simple_uam.tools.setup_win.shared.mac_address","text":"print the mac address of this machine. Source code in simple_uam/tools/setup_win/shared.py 32 33 34 35 @task def mac_address ( ctx ): \"\"\" print the mac address of this machine. \"\"\" print ( get_mac_address ())","title":"mac_address()"},{"location":"reference/simple_uam/tools/setup_win/shared/#simple_uam.tools.setup_win.shared.qol_pkgs","text":"Install/Update Worker QoL Packages Source code in simple_uam/tools/setup_win/shared.py 26 27 28 29 30 @task ( pre = [ call ( install , pkg = qol_pkg_list )]) def qol_pkgs ( ctx ): \"\"\" Install/Update Worker QoL Packages \"\"\" log . info ( \"Finished Installing Quality of Life Packages\" )","title":"qol_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/worker/","text":"creopyson_dir = Config [ PathConfig ] . work_dir / 'creopyson' module-attribute \u00b6 Directory with creopyson repo. choco_pkgs ( ctx ) \u00b6 Install/Update Worker Node Dependencies. Source code in simple_uam/tools/setup_win/worker.py 28 29 30 31 32 @task ( pre = [ call ( choco . install , pkg = worker_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update Worker Node Dependencies. \"\"\" log . info ( \"Finished Installing Dependency Packages\" ) creo ( ctx , force = False ) \u00b6 Install Creo 5.6 Parameters: Name Type Description Default force Run the installer even if creo is already installed. False Source code in simple_uam/tools/setup_win/worker.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @task ( pre = creo_installer . invoke_deps ) def creo ( ctx , force = False ): \"\"\" Install Creo 5.6 Arguments: force: Run the installer even if creo is already installed. \"\"\" creo_installer . run ( force = force ) log . info ( \"Making changes to creo config (if needed).\" , file = creo_config , lines = creo_config_edits , ) append_to_file ( creo_config , creo_config_edits ) creopyson ( ctx , prompt = True , quiet = False , verbose = False ) \u00b6 Clones/updates the creopyson repository and installs the python library globally with pip. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/setup_win/worker.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 @task ( pre = [ call ( choco . install , pkg = [ 'python3' ])]) def creopyson ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones/updates the creopyson repository and installs the python library globally with pip. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" #### Git #### git_args = dict ( repo_uri = creopyson_repo , deploy_dir = creopyson_dir , branch = creopyson_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for creopyson.\" , ** git_args ) Git . clone_or_pull ( ** git_args ) #### Pip #### pip_args = dict ( editable = True , upgrade = False , quiet = quiet , verbose = verbose , cwd = creopyson_dir , ) if not quiet : log . info ( \"Running pip install w/ local package.\" , package = creopyson_dir , ** pip_args , ) Pip . install ( creopyson_dir , ** pip_args ) dep_pkgs ( ctx ) \u00b6 Install all the hands-free worker node dependencies. Source code in simple_uam/tools/setup_win/worker.py 40 41 42 43 @task ( choco_pkgs , pip_pkgs ) def dep_pkgs ( ctx ): \"\"\" Install all the hands-free worker node dependencies. \"\"\" pass disable_ieesc ( ctx ) \u00b6 Disable Internet Explorer Enhanced Security (Server 2019 only). These are the bloody annoying Look at the following links for other methods to disable IEESC on other versions of windows or via the GUI. - https://www.wintips.org/how-to-disable-internet-explorer-enhanced-security-configuration-in-server-2016/ - https://www.casbay.com/guide/kb/disable-enhanced-security-configuration-for-internet-explorer-in-windows-server-2019-2016 Source code in simple_uam/tools/setup_win/worker.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @task def disable_ieesc ( ctx ): \"\"\" Disable Internet Explorer Enhanced Security (Server 2019 only). These are the bloody annoying Look at the following links for other methods to disable IEESC on other versions of windows or via the GUI. - https://www.wintips.org/how-to-disable-internet-explorer-enhanced-security-configuration-in-server-2016/ - https://www.casbay.com/guide/kb/disable-enhanced-security-configuration-for-internet-explorer-in-windows-server-2019-2016 \"\"\" log . info ( \"Disabling IE Enhanced Security .\" , script = str ( disable_ieesc_script ), ) installed = subprocess . run ([ 'powershell' , '-executionpolicy' , 'bypass' , '-File' , disable_ieesc_script ]) matlab ( ctx , force = False ) \u00b6 Install Matlab Runtime 2020b Parameters: Name Type Description Default force Run the installer even if matlab is already installed. False Source code in simple_uam/tools/setup_win/worker.py 151 152 153 154 155 156 157 158 159 @task ( pre = matlab_installer . invoke_deps ) def matlab ( ctx , force = False ): \"\"\" Install Matlab Runtime 2020b Arguments: force: Run the installer even if matlab is already installed. \"\"\" matlab_installer . run ( force = force ) openmeta ( ctx , force = False ) \u00b6 Install OpenMETA v0.22.0 Parameters: Name Type Description Default force Run the installer even if openmeta is already installed. False Source code in simple_uam/tools/setup_win/worker.py 121 122 123 124 125 126 127 128 129 @task ( pre = openmeta_installer . invoke_deps ) def openmeta ( ctx , force = False ): \"\"\" Install OpenMETA v0.22.0 Arguments: force: Run the installer even if openmeta is already installed. \"\"\" openmeta_installer . run ( force = force ) pip_pkgs ( ctx ) \u00b6 Install Worker Node public Pip packages. Source code in simple_uam/tools/setup_win/worker.py 34 35 36 37 38 @task ( call ( choco . install , pkg = [ 'python3' ])) def pip_pkgs ( ctx ): \"\"\" Install Worker Node public Pip packages. \"\"\" Pip . install ( * Config [ WinSetupConfig ] . worker_pip_packages )","title":"worker"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.creopyson_dir","text":"Directory with creopyson repo.","title":"creopyson_dir"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.choco_pkgs","text":"Install/Update Worker Node Dependencies. Source code in simple_uam/tools/setup_win/worker.py 28 29 30 31 32 @task ( pre = [ call ( choco . install , pkg = worker_dep_pkg_list )]) def choco_pkgs ( ctx ): \"\"\" Install/Update Worker Node Dependencies. \"\"\" log . info ( \"Finished Installing Dependency Packages\" )","title":"choco_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.creo","text":"Install Creo 5.6 Parameters: Name Type Description Default force Run the installer even if creo is already installed. False Source code in simple_uam/tools/setup_win/worker.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @task ( pre = creo_installer . invoke_deps ) def creo ( ctx , force = False ): \"\"\" Install Creo 5.6 Arguments: force: Run the installer even if creo is already installed. \"\"\" creo_installer . run ( force = force ) log . info ( \"Making changes to creo config (if needed).\" , file = creo_config , lines = creo_config_edits , ) append_to_file ( creo_config , creo_config_edits )","title":"creo()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.creopyson","text":"Clones/updates the creopyson repository and installs the python library globally with pip. Parameters: Name Type Description Default prompt Prompt for a password on initial git clone. True quiet Run in quiet mode. False verbose Run in verbose mode. False Source code in simple_uam/tools/setup_win/worker.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 @task ( pre = [ call ( choco . install , pkg = [ 'python3' ])]) def creopyson ( ctx , prompt = True , quiet = False , verbose = False ): \"\"\" Clones/updates the creopyson repository and installs the python library globally with pip. Arguments: prompt: Prompt for a password on initial git clone. quiet: Run in quiet mode. verbose: Run in verbose mode. \"\"\" #### Git #### git_args = dict ( repo_uri = creopyson_repo , deploy_dir = creopyson_dir , branch = creopyson_branch , password_prompt = prompt , quiet = quiet , verbose = verbose , mkdir = True ) if not quiet : log . info ( \"Running git clone/pull for creopyson.\" , ** git_args ) Git . clone_or_pull ( ** git_args ) #### Pip #### pip_args = dict ( editable = True , upgrade = False , quiet = quiet , verbose = verbose , cwd = creopyson_dir , ) if not quiet : log . info ( \"Running pip install w/ local package.\" , package = creopyson_dir , ** pip_args , ) Pip . install ( creopyson_dir , ** pip_args )","title":"creopyson()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.dep_pkgs","text":"Install all the hands-free worker node dependencies. Source code in simple_uam/tools/setup_win/worker.py 40 41 42 43 @task ( choco_pkgs , pip_pkgs ) def dep_pkgs ( ctx ): \"\"\" Install all the hands-free worker node dependencies. \"\"\" pass","title":"dep_pkgs()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.disable_ieesc","text":"Disable Internet Explorer Enhanced Security (Server 2019 only). These are the bloody annoying Look at the following links for other methods to disable IEESC on other versions of windows or via the GUI. - https://www.wintips.org/how-to-disable-internet-explorer-enhanced-security-configuration-in-server-2016/ - https://www.casbay.com/guide/kb/disable-enhanced-security-configuration-for-internet-explorer-in-windows-server-2019-2016 Source code in simple_uam/tools/setup_win/worker.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @task def disable_ieesc ( ctx ): \"\"\" Disable Internet Explorer Enhanced Security (Server 2019 only). These are the bloody annoying Look at the following links for other methods to disable IEESC on other versions of windows or via the GUI. - https://www.wintips.org/how-to-disable-internet-explorer-enhanced-security-configuration-in-server-2016/ - https://www.casbay.com/guide/kb/disable-enhanced-security-configuration-for-internet-explorer-in-windows-server-2019-2016 \"\"\" log . info ( \"Disabling IE Enhanced Security .\" , script = str ( disable_ieesc_script ), ) installed = subprocess . run ([ 'powershell' , '-executionpolicy' , 'bypass' , '-File' , disable_ieesc_script ])","title":"disable_ieesc()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.matlab","text":"Install Matlab Runtime 2020b Parameters: Name Type Description Default force Run the installer even if matlab is already installed. False Source code in simple_uam/tools/setup_win/worker.py 151 152 153 154 155 156 157 158 159 @task ( pre = matlab_installer . invoke_deps ) def matlab ( ctx , force = False ): \"\"\" Install Matlab Runtime 2020b Arguments: force: Run the installer even if matlab is already installed. \"\"\" matlab_installer . run ( force = force )","title":"matlab()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.openmeta","text":"Install OpenMETA v0.22.0 Parameters: Name Type Description Default force Run the installer even if openmeta is already installed. False Source code in simple_uam/tools/setup_win/worker.py 121 122 123 124 125 126 127 128 129 @task ( pre = openmeta_installer . invoke_deps ) def openmeta ( ctx , force = False ): \"\"\" Install OpenMETA v0.22.0 Arguments: force: Run the installer even if openmeta is already installed. \"\"\" openmeta_installer . run ( force = force )","title":"openmeta()"},{"location":"reference/simple_uam/tools/setup_win/worker/#simple_uam.tools.setup_win.worker.pip_pkgs","text":"Install Worker Node public Pip packages. Source code in simple_uam/tools/setup_win/worker.py 34 35 36 37 38 @task ( call ( choco . install , pkg = [ 'python3' ])) def pip_pkgs ( ctx ): \"\"\" Install Worker Node public Pip packages. \"\"\" Pip . install ( * Config [ WinSetupConfig ] . worker_pip_packages )","title":"pip_pkgs()"},{"location":"reference/simple_uam/util/","text":"SimpleUAM windows node setup scripts.","title":"util"},{"location":"reference/simple_uam/util/config/","text":"Config \u00b6 Bases: object Singleton, project-wide manager of configuration files. Source code in simple_uam/util/config/manager.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 @define class Config ( object ): \"\"\" Singleton, project-wide manager of configuration files. \"\"\" config_types : Dict [ Type , ConfigData ] = field ( factory = dict , init = False , ) \"\"\" Dict of all the configuration types we manage \"\"\" key_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict of interpolation keys we manage.\"\"\" name_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from type name to key map.\"\"\" file_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from file name to key map.\"\"\" config_dirs : List [ Path ] = field ( factory = list , init = False , ) \"\"\" The search path of configuration directories \"\"\" mode_flags : List [ str ] = field ( factory = list , init = False ) \"\"\" The lode flags from lowest to highest priority \"\"\" def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance def __init__ ( self ): pass def __post_init__ ( self ) -> None : # Read args (non-destructively) parser = self . _get_argparser () ( args , remainder ) = parser . parse_known_args () # Update sys args w/ unused options # sys.argv = remainder # Set path from args self . mode_flags = args . run_mode self . config_dirs = [ * self . _get_conf_dir_chain (), * args . config_dir ] # Load PathConfig path_conf_data = ConfigData ( # type: ignore[call-arg] data_cls = PathConfig , interpolation_key = PATHCONFIG_INTERPOLATION_KEY , conf_file = PATHCONFIG_FILE_NAME , conf_deps = [], default_conf = OmegaConf . structured ( PathConfig ()), load_path = self . _search_path ( PATHCONFIG_FILE_NAME ), overrides = None , ) # Add PathConfig to config_types self . _add_conf_data ( path_conf_data ) def _add_conf_data ( self , conf_data : ConfigData ) -> None : data_cls = conf_data . data_cls # Register resolvers for deps OmegaConf . register_new_resolver ( conf_data . interpolation_key , conf_data . resolver , ) # print(f\"registering: {data_cls}\") self . config_types [ data_cls ] = conf_data self . key_map [ conf_data . interpolation_key ] = data_cls self . file_map [ str ( conf_data . conf_file )] = data_cls self . name_map [ data_cls . __name__ ] = data_cls # print(f\"registering: {self.name_map.keys()}\") def _get_conf_dir_chain ( self ) -> List [ Path ]: \"\"\" Returns the chain of config_dirs to load. \"\"\" current_dir = Path ( PathConfig () . config_dir ) conf_dirs = [ current_dir ] path_conf = current_dir / PATHCONFIG_FILE_NAME while current_dir and path_conf . is_file (): conf = OmegaConf . load ( path_conf ) current_dir = OmegaConf . select ( conf , \"config_directory\" ) if current_dir : current_dir = Path ( current_dir ) conf_dirs . append ( current_dir ) path_conf = current_dir / PATHCONFIG_FILE_NAME return conf_dirs def _search_path ( self , config_file : Union [ str , Path ]) -> List [ Path ]: \"\"\" Gives the search path to use for a particular file, with later paths over-riding earlier ones. \"\"\" path_list = list () for mode_flag in [ None , * self . mode_flags ]: for conf_dir in self . config_dirs : if not mode_flag : path_list . append ( conf_dir / config_file ) else : path_list . append ( conf_dir / mode_flag / config_file ) return path_list @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ()) @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ()) @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ()) @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ()) @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls ) def _load_path ( self , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return self . config_types [ self . _get_config_class ( data_cls )] . load_path @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , ) def _write_configs ( self , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" conf_types = None if configs == None : conf_types = self . config_types . values () else : conf_types = [ self . config_types [ self . _get_config_class ( config )] for config in configs ] path = None if out_dir : path = Path ( out_dir ) path . mkdir ( parents = True , exist_ok = True ) for conf_type in conf_types : conf_type . write_config ( path = path , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , # out_dir=out_dir, ) @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , ) def _register ( self , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" See register for details \"\"\" # If already registered error if data_cls in self . config_types : raise RuntimeError ( f \"Class { data_cls . __name__ } already registered with Config.\" ) # Check dependent configs are installed for par_cls in conf_deps : if par_cls not in self . config_types : raise RuntimeError ( f \"Dependent Config Class { par_cls . __name__ } not registered.\" ) # Create object class_data = ConfigData ( # type: ignore[call-arg] data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , default_conf = None , load_path = self . _search_path ( conf_file ), overrides = None , ) # Add to map self . _add_conf_data ( class_data ) @classmethod def _get_config_class ( cls , key : Union [ str , Type ]) -> Type : \"\"\" Unpacks the key based on its type and value to find the appropriate config class. \"\"\" inst = cls () data_cls = None if isinstance ( key , str ): if key in inst . name_map : data_cls = inst . name_map [ key ] elif key in inst . file_map : data_cls = inst . file_map [ key ] elif key in inst . key_map : data_cls = inst . key_map [ key ] else : RuntimeError ( f \"Config with key { key } not found.\" ) else : data_cls = key return data_cls @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key ) def _get ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . obj def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key ) def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key ) @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key ) def _get_yaml ( self , key : Union [ str , Type [ T ]]) -> str : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . yaml @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key ) def _get_omegaconf ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get_omegaconf for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . config @staticmethod def _get_argparser () -> argparse . ArgumentParser : \"\"\" Get the arg parser for the conf_dir and run_mode options. \"\"\" parser = argparse . ArgumentParser ( add_help = False ) # Config dir argument parser . add_argument ( \"--config-dir\" , action = \"append\" , default = [], type = Path , required = False , help = \"The directory containing all the config files used for this run. Can be given multiple times.\" , ) # Run Mode Argument parser . add_argument ( \"--run-mode\" , action = \"append\" , default = [], type = Path , required = False , help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'. Can be given multiple times.\" , ) return parser @staticmethod def _invoke_args () -> List [ Argument ]: \"\"\" Arguments to be used with an invoke Program class \"\"\" return [ Argument ( name = \"config-dir\" , kind = str , default = [], help = \"The directory containing all the config files used for this run.\" , ), Argument ( name = \"run-mode\" , kind = str , default = [], help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'.\" , ), ] config_dirs : List [ Path ] = field ( factory = list , init = False ) class-attribute \u00b6 The search path of configuration directories config_types : Dict [ Type , ConfigData ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict of all the configuration types we manage file_map : Dict [ str , Type ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict from file name to key map. key_map : Dict [ str , Type ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict of interpolation keys we manage. mode_flags : List [ str ] = field ( factory = list , init = False ) class-attribute \u00b6 The lode flags from lowest to highest priority name_map : Dict [ str , Type ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict from type name to key map. __class_getitem__ ( key ) \u00b6 See get for details Source code in simple_uam/util/config/manager.py 619 620 621 622 623 def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key ) __getitem__ ( key ) \u00b6 See get for details Source code in simple_uam/util/config/manager.py 613 614 615 616 617 def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key ) __new__ () \u00b6 Creates a singleton object, if it is not created, or else returns the previous singleton object Source code in simple_uam/util/config/manager.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance config_files () classmethod \u00b6 Returns a list of config dataclass file locs. (relative to config dir root) Source code in simple_uam/util/config/manager.py 390 391 392 393 394 395 396 @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ()) config_keys () classmethod \u00b6 Returns a list of config dataclass interpolation keys Source code in simple_uam/util/config/manager.py 398 399 400 401 402 403 @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ()) config_names () classmethod \u00b6 Returns a list of config dataclass type names Source code in simple_uam/util/config/manager.py 383 384 385 386 387 388 @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ()) configs () classmethod \u00b6 Returns a list of config dataclass types Source code in simple_uam/util/config/manager.py 376 377 378 379 380 381 @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ()) get ( key ) classmethod \u00b6 Retrieves an OmegaConf object for the associated dataclass type. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 593 594 595 596 597 598 599 600 601 602 603 604 @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key ) get_omegaconf ( key ) classmethod \u00b6 Returns the object for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 645 646 647 648 649 650 651 652 653 654 655 656 @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key ) get_yaml ( key ) classmethod \u00b6 Returns the yaml string for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 625 626 627 628 629 630 631 632 633 634 635 636 @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key ) load_path ( data_cls ) classmethod \u00b6 returns the list of files that are loaded for a given config dataclass. Source code in simple_uam/util/config/manager.py 405 406 407 408 409 410 @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls ) register ( data_cls , interpolation_key , conf_file , conf_deps = []) classmethod \u00b6 Registers a configuration dataclass with the manager. Raises an error if the operation fails. Parameters: Name Type Description Default data_cls Type The dataclass or attrs class that represents the contents of the file. required interpolation_key str The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. required conf_file Union [ str , Path ] The config file associated with this object, specified relative to the config_dir. required conf_deps List [ Type ] The config classes used for interpolations in this one. [] Source code in simple_uam/util/config/manager.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , ) write_configs ( configs = None , mkdir = True , write_all = False , overwrite = False , comment = True , fields = None , data = None , out_dir = None ) classmethod \u00b6 Writes a sample config out to the filesystem. Parameters: Name Type Description Default configs Optional [ List [ Union [ str , Type ]]] The list of config files to write (default: All) None mkdir default =True Create directories for config files if needed. If false will fail with warning when directory is missing. True write_all default=False Do we create all config files on path or just the terminal one? (ignored if path is provided) False overwrite bool Do we overwrite files if they already exist? False comment bool Should our sample be written as a comment or as a literal yaml document? True fields default=None what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. None data default=None Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. None out_dir default=None directory to write output to if provided. None Source code in simple_uam/util/config/manager.py 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , ) CraidlConfig \u00b6 Config properties for working with craidl and the gremlin stub server. Source code in simple_uam/util/config/craidl_config.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @define class CraidlConfig (): \"\"\" Config properties for working with craidl and the gremlin stub server. \"\"\" example_dir : Optional [ str ] = field ( default = SI ( \"${path:data_directory}/craidl_examples\" ), ) \"\"\" Dir containing craidl example designs (each in their own subfolder.) \"\"\" stub_server : StubServerConfig = StubServerConfig () \"\"\" Stub server settings. \"\"\" server_host : str = field ( default = SI ( \"localhost\" ) ) \"\"\" The host to connect to when using a gremlin corpus server. \"\"\" server_port : int = field ( default = SI ( \"$ {stub_server.port} \" ) ) \"\"\" The port to connect to when using a gremlin corpus server. \"\"\" static_corpus : str = field ( default = SI ( \"${path:data_directory}/corpus_static_dump.json\" ), ) \"\"\" The parsed static corpus to be generated/used when creating info files. \"\"\" static_corpus_cache : str = field ( default = SI ( \"${path:cache_directory}/static_corpus_cache\" ), ) \"\"\" The cache directory to use when generating a static corpus. \"\"\" use_static_corpus : bool = True \"\"\" Use the static corpus for generating design info files if true, otherwise use the server. \"\"\" example_dir : Optional [ str ] = field ( default = SI ( '${path:data_directory}/craidl_examples' )) class-attribute \u00b6 Dir containing craidl example designs (each in their own subfolder.) server_host : str = field ( default = SI ( 'localhost' )) class-attribute \u00b6 The host to connect to when using a gremlin corpus server. server_port : int = field ( default = SI ( '$ {stub_server.port} ' )) class-attribute \u00b6 The port to connect to when using a gremlin corpus server. static_corpus : str = field ( default = SI ( '${path:data_directory}/corpus_static_dump.json' )) class-attribute \u00b6 The parsed static corpus to be generated/used when creating info files. static_corpus_cache : str = field ( default = SI ( '${path:cache_directory}/static_corpus_cache' )) class-attribute \u00b6 The cache directory to use when generating a static corpus. stub_server : StubServerConfig = StubServerConfig () class-attribute \u00b6 Stub server settings. use_static_corpus : bool = True class-attribute \u00b6 Use the static corpus for generating design info files if true, otherwise use the server. D2CWorkerConfig \u00b6 A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/d2c_worker_config.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @define class D2CWorkerConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" broker : BrokerConfig = BrokerConfig () \"\"\" Configuration options for *connecting to* a broker. This applies to both workers and clients. \"\"\" backend : BackendConfig = BackendConfig () \"\"\" Configuration options for a result backend. Needed if you want to get return values for remote calls directly. \"\"\" max_processes : int = SI ( \"${d2c_workspace:max_workspaces}\" ) \"\"\" Max number of forked processes on a worker node. Default is the number of workspaces on the machine. \"\"\" max_threads : int = 1 \"\"\" Max number of threads per process. Default is 1. \"\"\" shutdown_timeout : int = 600000 \"\"\" Timeout for worker shutdown in milliseconds. \"\"\" skip_logging : bool = False \"\"\" Do we keep dramatiq specific logs? This doesn't affect structlog logs. \"\"\" service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( \"${path:log_directory}/d2c_worker/stdout.log\" ), stderr_file = SI ( \"${path:log_directory}/d2c_worker/stderr.log\" ), ) ) \"\"\" Settings for running the worker node service. \"\"\" backend : BackendConfig = BackendConfig () class-attribute \u00b6 Configuration options for a result backend. Needed if you want to get return values for remote calls directly. broker : BrokerConfig = BrokerConfig () class-attribute \u00b6 Configuration options for connecting to a broker. This applies to both workers and clients. max_processes : int = SI ( '${d2c_workspace:max_workspaces}' ) class-attribute \u00b6 Max number of forked processes on a worker node. Default is the number of workspaces on the machine. max_threads : int = 1 class-attribute \u00b6 Max number of threads per process. Default is 1. service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( '${path:log_directory}/d2c_worker/stdout.log' ), stderr_file = SI ( '${path:log_directory}/d2c_worker/stderr.log' ))) class-attribute \u00b6 Settings for running the worker node service. shutdown_timeout : int = 600000 class-attribute \u00b6 Timeout for worker shutdown in milliseconds. skip_logging : bool = False class-attribute \u00b6 Do we keep dramatiq specific logs? This doesn't affect structlog logs. D2CWorkspaceConfig \u00b6 Bases: WorkspaceConfig A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for results to your new class. Source code in simple_uam/util/config/d2c_workspace_config.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @define class D2CWorkspaceConfig ( WorkspaceConfig ): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for `results` to your new class. \"\"\" workspaces_dir : str = field ( default = SI ( \"${path:work_directory}/d2c_workspaces\" ) ) \"\"\" Dir where workspaces are stored. \"\"\" cache_dir : str = field ( default = SI ( \"${path:cache_directory}/d2c_workspaces\" ) ) \"\"\" Dir for cached data. \"\"\" max_workspaces : int = 1 \"\"\" The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine. \"\"\" exclude : List [ str ] = [ '.git' ] exclude_from : List [ str ] = [] result_exclude : List [ str ] = [ '.git' ] result_exclude_from : List [ str ] = [] cache_dir : str = field ( default = SI ( '${path:cache_directory}/d2c_workspaces' )) class-attribute \u00b6 Dir for cached data. max_workspaces : int = 1 class-attribute \u00b6 The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine. workspaces_dir : str = field ( default = SI ( '${path:work_directory}/d2c_workspaces' )) class-attribute \u00b6 Dir where workspaces are stored. PathConfig \u00b6 Source code in simple_uam/util/config/path_config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @define class PathConfig : config_directory : str = field ( default = str ( dirs . site_config_path / 'config' ), converter = str , ) cache_directory : str = field ( default = str ( dirs . site_data_path / 'cache' ), converter = str , ) log_directory : str = field ( default = dirs . user_log_dir , converter = str , ) work_directory : str = field ( default = str ( dirs . site_data_path ), converter = str , ) data_directory : str = field ( default = str ( dirs . site_data_path / 'data' ), converter = str , ) @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory ) @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory ) @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory ) @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory ) @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory ) @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data' cache_dir () property \u00b6 Cache Directory. Source code in simple_uam/util/config/path_config.py 50 51 52 53 @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory ) config_dir () property \u00b6 Config file root dir. Source code in simple_uam/util/config/path_config.py 45 46 47 48 @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory ) data_dir () property \u00b6 Ysytem user static data storage. Source code in simple_uam/util/config/path_config.py 65 66 67 68 @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory ) log_dir () property \u00b6 Log Storage Directory. Source code in simple_uam/util/config/path_config.py 55 56 57 58 @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory ) repo_data_dir () property \u00b6 Repository static data storage. Source code in simple_uam/util/config/path_config.py 70 71 72 73 @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data' repo_dir () property \u00b6 Repository root dir. Source code in simple_uam/util/config/path_config.py 40 41 42 43 @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent work_dir () property \u00b6 Working directory for assorted operations. Source code in simple_uam/util/config/path_config.py 60 61 62 63 @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory ) WinSetupConfig \u00b6 A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/win_setup_config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @define class WinSetupConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' , ] \"\"\" Packages that are needed for all windows nodes, please don't remove any from this default set. \"\"\" broker_dep_packages : List [ str ] = [ 'rabbitmq' , ] \"\"\" Chocolatey packages needed for a windows broker node. \"\"\" worker_dep_packages : List [ str ] = [ 'openjdk11' , # 'openmodelica', 'rsync' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows worker node. \"\"\" worker_pip_packages : List [ str ] = [ # Direct2Cad 'psutil' , # 'creopyson', # Need to use the creopyson from the swri repo 'numpy' , # Craidl # 'gremlinpython', # Handling this internally now ] \"\"\" Pip packages needed for a windows worker node. \"\"\" license_dep_packages : List [ str ] = [ ] \"\"\" Chocolatey packages needed for a windows license server node. \"\"\" graph_dep_packages : List [ str ] = [ 'openjdk11' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows graph server node. \"\"\" qol_packages : List [ str ] = [ 'firefox' , 'notepadplusplus' , 'foxitreader' , 'tess' , 'freecad' , ] \"\"\" Quality of life packages that make actually using a windows node bearable. \"\"\" broker_dep_packages : List [ str ] = [ 'rabbitmq' ] class-attribute \u00b6 Chocolatey packages needed for a windows broker node. global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' ] class-attribute \u00b6 Packages that are needed for all windows nodes, please don't remove any from this default set. graph_dep_packages : List [ str ] = [ 'openjdk11' , 'nssm' ] class-attribute \u00b6 Chocolatey packages needed for a windows graph server node. license_dep_packages : List [ str ] = [] class-attribute \u00b6 Chocolatey packages needed for a windows license server node. qol_packages : List [ str ] = [ 'firefox' , 'notepadplusplus' , 'foxitreader' , 'tess' , 'freecad' ] class-attribute \u00b6 Quality of life packages that make actually using a windows node bearable. worker_dep_packages : List [ str ] = [ 'openjdk11' , 'rsync' , 'nssm' ] class-attribute \u00b6 Chocolatey packages needed for a windows worker node. worker_pip_packages : List [ str ] = [ 'psutil' , 'numpy' ] class-attribute \u00b6 Pip packages needed for a windows worker node.","title":"config"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.Config","text":"Bases: object Singleton, project-wide manager of configuration files. Source code in simple_uam/util/config/manager.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 @define class Config ( object ): \"\"\" Singleton, project-wide manager of configuration files. \"\"\" config_types : Dict [ Type , ConfigData ] = field ( factory = dict , init = False , ) \"\"\" Dict of all the configuration types we manage \"\"\" key_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict of interpolation keys we manage.\"\"\" name_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from type name to key map.\"\"\" file_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from file name to key map.\"\"\" config_dirs : List [ Path ] = field ( factory = list , init = False , ) \"\"\" The search path of configuration directories \"\"\" mode_flags : List [ str ] = field ( factory = list , init = False ) \"\"\" The lode flags from lowest to highest priority \"\"\" def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance def __init__ ( self ): pass def __post_init__ ( self ) -> None : # Read args (non-destructively) parser = self . _get_argparser () ( args , remainder ) = parser . parse_known_args () # Update sys args w/ unused options # sys.argv = remainder # Set path from args self . mode_flags = args . run_mode self . config_dirs = [ * self . _get_conf_dir_chain (), * args . config_dir ] # Load PathConfig path_conf_data = ConfigData ( # type: ignore[call-arg] data_cls = PathConfig , interpolation_key = PATHCONFIG_INTERPOLATION_KEY , conf_file = PATHCONFIG_FILE_NAME , conf_deps = [], default_conf = OmegaConf . structured ( PathConfig ()), load_path = self . _search_path ( PATHCONFIG_FILE_NAME ), overrides = None , ) # Add PathConfig to config_types self . _add_conf_data ( path_conf_data ) def _add_conf_data ( self , conf_data : ConfigData ) -> None : data_cls = conf_data . data_cls # Register resolvers for deps OmegaConf . register_new_resolver ( conf_data . interpolation_key , conf_data . resolver , ) # print(f\"registering: {data_cls}\") self . config_types [ data_cls ] = conf_data self . key_map [ conf_data . interpolation_key ] = data_cls self . file_map [ str ( conf_data . conf_file )] = data_cls self . name_map [ data_cls . __name__ ] = data_cls # print(f\"registering: {self.name_map.keys()}\") def _get_conf_dir_chain ( self ) -> List [ Path ]: \"\"\" Returns the chain of config_dirs to load. \"\"\" current_dir = Path ( PathConfig () . config_dir ) conf_dirs = [ current_dir ] path_conf = current_dir / PATHCONFIG_FILE_NAME while current_dir and path_conf . is_file (): conf = OmegaConf . load ( path_conf ) current_dir = OmegaConf . select ( conf , \"config_directory\" ) if current_dir : current_dir = Path ( current_dir ) conf_dirs . append ( current_dir ) path_conf = current_dir / PATHCONFIG_FILE_NAME return conf_dirs def _search_path ( self , config_file : Union [ str , Path ]) -> List [ Path ]: \"\"\" Gives the search path to use for a particular file, with later paths over-riding earlier ones. \"\"\" path_list = list () for mode_flag in [ None , * self . mode_flags ]: for conf_dir in self . config_dirs : if not mode_flag : path_list . append ( conf_dir / config_file ) else : path_list . append ( conf_dir / mode_flag / config_file ) return path_list @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ()) @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ()) @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ()) @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ()) @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls ) def _load_path ( self , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return self . config_types [ self . _get_config_class ( data_cls )] . load_path @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , ) def _write_configs ( self , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" conf_types = None if configs == None : conf_types = self . config_types . values () else : conf_types = [ self . config_types [ self . _get_config_class ( config )] for config in configs ] path = None if out_dir : path = Path ( out_dir ) path . mkdir ( parents = True , exist_ok = True ) for conf_type in conf_types : conf_type . write_config ( path = path , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , # out_dir=out_dir, ) @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , ) def _register ( self , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" See register for details \"\"\" # If already registered error if data_cls in self . config_types : raise RuntimeError ( f \"Class { data_cls . __name__ } already registered with Config.\" ) # Check dependent configs are installed for par_cls in conf_deps : if par_cls not in self . config_types : raise RuntimeError ( f \"Dependent Config Class { par_cls . __name__ } not registered.\" ) # Create object class_data = ConfigData ( # type: ignore[call-arg] data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , default_conf = None , load_path = self . _search_path ( conf_file ), overrides = None , ) # Add to map self . _add_conf_data ( class_data ) @classmethod def _get_config_class ( cls , key : Union [ str , Type ]) -> Type : \"\"\" Unpacks the key based on its type and value to find the appropriate config class. \"\"\" inst = cls () data_cls = None if isinstance ( key , str ): if key in inst . name_map : data_cls = inst . name_map [ key ] elif key in inst . file_map : data_cls = inst . file_map [ key ] elif key in inst . key_map : data_cls = inst . key_map [ key ] else : RuntimeError ( f \"Config with key { key } not found.\" ) else : data_cls = key return data_cls @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key ) def _get ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . obj def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key ) def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key ) @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key ) def _get_yaml ( self , key : Union [ str , Type [ T ]]) -> str : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . yaml @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key ) def _get_omegaconf ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get_omegaconf for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . config @staticmethod def _get_argparser () -> argparse . ArgumentParser : \"\"\" Get the arg parser for the conf_dir and run_mode options. \"\"\" parser = argparse . ArgumentParser ( add_help = False ) # Config dir argument parser . add_argument ( \"--config-dir\" , action = \"append\" , default = [], type = Path , required = False , help = \"The directory containing all the config files used for this run. Can be given multiple times.\" , ) # Run Mode Argument parser . add_argument ( \"--run-mode\" , action = \"append\" , default = [], type = Path , required = False , help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'. Can be given multiple times.\" , ) return parser @staticmethod def _invoke_args () -> List [ Argument ]: \"\"\" Arguments to be used with an invoke Program class \"\"\" return [ Argument ( name = \"config-dir\" , kind = str , default = [], help = \"The directory containing all the config files used for this run.\" , ), Argument ( name = \"run-mode\" , kind = str , default = [], help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'.\" , ), ]","title":"Config"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.config_dirs","text":"The search path of configuration directories","title":"config_dirs"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.config_types","text":"Dict of all the configuration types we manage","title":"config_types"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.file_map","text":"Dict from file name to key map.","title":"file_map"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.key_map","text":"Dict of interpolation keys we manage.","title":"key_map"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.mode_flags","text":"The lode flags from lowest to highest priority","title":"mode_flags"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.name_map","text":"Dict from type name to key map.","title":"name_map"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.__class_getitem__","text":"See get for details Source code in simple_uam/util/config/manager.py 619 620 621 622 623 def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key )","title":"__class_getitem__()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.__getitem__","text":"See get for details Source code in simple_uam/util/config/manager.py 613 614 615 616 617 def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key )","title":"__getitem__()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.__new__","text":"Creates a singleton object, if it is not created, or else returns the previous singleton object Source code in simple_uam/util/config/manager.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance","title":"__new__()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.config_files","text":"Returns a list of config dataclass file locs. (relative to config dir root) Source code in simple_uam/util/config/manager.py 390 391 392 393 394 395 396 @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ())","title":"config_files()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.config_keys","text":"Returns a list of config dataclass interpolation keys Source code in simple_uam/util/config/manager.py 398 399 400 401 402 403 @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ())","title":"config_keys()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.config_names","text":"Returns a list of config dataclass type names Source code in simple_uam/util/config/manager.py 383 384 385 386 387 388 @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ())","title":"config_names()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.configs","text":"Returns a list of config dataclass types Source code in simple_uam/util/config/manager.py 376 377 378 379 380 381 @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ())","title":"configs()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.get","text":"Retrieves an OmegaConf object for the associated dataclass type. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 593 594 595 596 597 598 599 600 601 602 603 604 @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key )","title":"get()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.get_omegaconf","text":"Returns the object for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 645 646 647 648 649 650 651 652 653 654 655 656 @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key )","title":"get_omegaconf()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.get_yaml","text":"Returns the yaml string for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 625 626 627 628 629 630 631 632 633 634 635 636 @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key )","title":"get_yaml()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.load_path","text":"returns the list of files that are loaded for a given config dataclass. Source code in simple_uam/util/config/manager.py 405 406 407 408 409 410 @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls )","title":"load_path()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.register","text":"Registers a configuration dataclass with the manager. Raises an error if the operation fails. Parameters: Name Type Description Default data_cls Type The dataclass or attrs class that represents the contents of the file. required interpolation_key str The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. required conf_file Union [ str , Path ] The config file associated with this object, specified relative to the config_dir. required conf_deps List [ Type ] The config classes used for interpolations in this one. [] Source code in simple_uam/util/config/manager.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , )","title":"register()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.manager.Config.write_configs","text":"Writes a sample config out to the filesystem. Parameters: Name Type Description Default configs Optional [ List [ Union [ str , Type ]]] The list of config files to write (default: All) None mkdir default =True Create directories for config files if needed. If false will fail with warning when directory is missing. True write_all default=False Do we create all config files on path or just the terminal one? (ignored if path is provided) False overwrite bool Do we overwrite files if they already exist? False comment bool Should our sample be written as a comment or as a literal yaml document? True fields default=None what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. None data default=None Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. None out_dir default=None directory to write output to if provided. None Source code in simple_uam/util/config/manager.py 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , )","title":"write_configs()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.CraidlConfig","text":"Config properties for working with craidl and the gremlin stub server. Source code in simple_uam/util/config/craidl_config.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @define class CraidlConfig (): \"\"\" Config properties for working with craidl and the gremlin stub server. \"\"\" example_dir : Optional [ str ] = field ( default = SI ( \"${path:data_directory}/craidl_examples\" ), ) \"\"\" Dir containing craidl example designs (each in their own subfolder.) \"\"\" stub_server : StubServerConfig = StubServerConfig () \"\"\" Stub server settings. \"\"\" server_host : str = field ( default = SI ( \"localhost\" ) ) \"\"\" The host to connect to when using a gremlin corpus server. \"\"\" server_port : int = field ( default = SI ( \"$ {stub_server.port} \" ) ) \"\"\" The port to connect to when using a gremlin corpus server. \"\"\" static_corpus : str = field ( default = SI ( \"${path:data_directory}/corpus_static_dump.json\" ), ) \"\"\" The parsed static corpus to be generated/used when creating info files. \"\"\" static_corpus_cache : str = field ( default = SI ( \"${path:cache_directory}/static_corpus_cache\" ), ) \"\"\" The cache directory to use when generating a static corpus. \"\"\" use_static_corpus : bool = True \"\"\" Use the static corpus for generating design info files if true, otherwise use the server. \"\"\"","title":"CraidlConfig"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.example_dir","text":"Dir containing craidl example designs (each in their own subfolder.)","title":"example_dir"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.server_host","text":"The host to connect to when using a gremlin corpus server.","title":"server_host"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.server_port","text":"The port to connect to when using a gremlin corpus server.","title":"server_port"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.static_corpus","text":"The parsed static corpus to be generated/used when creating info files.","title":"static_corpus"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.static_corpus_cache","text":"The cache directory to use when generating a static corpus.","title":"static_corpus_cache"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.stub_server","text":"Stub server settings.","title":"stub_server"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.craidl_config.CraidlConfig.use_static_corpus","text":"Use the static corpus for generating design info files if true, otherwise use the server.","title":"use_static_corpus"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.D2CWorkerConfig","text":"A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/d2c_worker_config.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @define class D2CWorkerConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" broker : BrokerConfig = BrokerConfig () \"\"\" Configuration options for *connecting to* a broker. This applies to both workers and clients. \"\"\" backend : BackendConfig = BackendConfig () \"\"\" Configuration options for a result backend. Needed if you want to get return values for remote calls directly. \"\"\" max_processes : int = SI ( \"${d2c_workspace:max_workspaces}\" ) \"\"\" Max number of forked processes on a worker node. Default is the number of workspaces on the machine. \"\"\" max_threads : int = 1 \"\"\" Max number of threads per process. Default is 1. \"\"\" shutdown_timeout : int = 600000 \"\"\" Timeout for worker shutdown in milliseconds. \"\"\" skip_logging : bool = False \"\"\" Do we keep dramatiq specific logs? This doesn't affect structlog logs. \"\"\" service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( \"${path:log_directory}/d2c_worker/stdout.log\" ), stderr_file = SI ( \"${path:log_directory}/d2c_worker/stderr.log\" ), ) ) \"\"\" Settings for running the worker node service. \"\"\"","title":"D2CWorkerConfig"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.backend","text":"Configuration options for a result backend. Needed if you want to get return values for remote calls directly.","title":"backend"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.broker","text":"Configuration options for connecting to a broker. This applies to both workers and clients.","title":"broker"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.max_processes","text":"Max number of forked processes on a worker node. Default is the number of workspaces on the machine.","title":"max_processes"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.max_threads","text":"Max number of threads per process. Default is 1.","title":"max_threads"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.service","text":"Settings for running the worker node service.","title":"service"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.shutdown_timeout","text":"Timeout for worker shutdown in milliseconds.","title":"shutdown_timeout"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.skip_logging","text":"Do we keep dramatiq specific logs? This doesn't affect structlog logs.","title":"skip_logging"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.D2CWorkspaceConfig","text":"Bases: WorkspaceConfig A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for results to your new class. Source code in simple_uam/util/config/d2c_workspace_config.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @define class D2CWorkspaceConfig ( WorkspaceConfig ): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for `results` to your new class. \"\"\" workspaces_dir : str = field ( default = SI ( \"${path:work_directory}/d2c_workspaces\" ) ) \"\"\" Dir where workspaces are stored. \"\"\" cache_dir : str = field ( default = SI ( \"${path:cache_directory}/d2c_workspaces\" ) ) \"\"\" Dir for cached data. \"\"\" max_workspaces : int = 1 \"\"\" The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine. \"\"\" exclude : List [ str ] = [ '.git' ] exclude_from : List [ str ] = [] result_exclude : List [ str ] = [ '.git' ] result_exclude_from : List [ str ] = []","title":"D2CWorkspaceConfig"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig.cache_dir","text":"Dir for cached data.","title":"cache_dir"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig.max_workspaces","text":"The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine.","title":"max_workspaces"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig.workspaces_dir","text":"Dir where workspaces are stored.","title":"workspaces_dir"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.PathConfig","text":"Source code in simple_uam/util/config/path_config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @define class PathConfig : config_directory : str = field ( default = str ( dirs . site_config_path / 'config' ), converter = str , ) cache_directory : str = field ( default = str ( dirs . site_data_path / 'cache' ), converter = str , ) log_directory : str = field ( default = dirs . user_log_dir , converter = str , ) work_directory : str = field ( default = str ( dirs . site_data_path ), converter = str , ) data_directory : str = field ( default = str ( dirs . site_data_path / 'data' ), converter = str , ) @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory ) @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory ) @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory ) @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory ) @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory ) @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data'","title":"PathConfig"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.cache_dir","text":"Cache Directory. Source code in simple_uam/util/config/path_config.py 50 51 52 53 @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory )","title":"cache_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.config_dir","text":"Config file root dir. Source code in simple_uam/util/config/path_config.py 45 46 47 48 @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory )","title":"config_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.data_dir","text":"Ysytem user static data storage. Source code in simple_uam/util/config/path_config.py 65 66 67 68 @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory )","title":"data_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.log_dir","text":"Log Storage Directory. Source code in simple_uam/util/config/path_config.py 55 56 57 58 @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory )","title":"log_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.repo_data_dir","text":"Repository static data storage. Source code in simple_uam/util/config/path_config.py 70 71 72 73 @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data'","title":"repo_data_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.repo_dir","text":"Repository root dir. Source code in simple_uam/util/config/path_config.py 40 41 42 43 @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent","title":"repo_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.path_config.PathConfig.work_dir","text":"Working directory for assorted operations. Source code in simple_uam/util/config/path_config.py 60 61 62 63 @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory )","title":"work_dir()"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.WinSetupConfig","text":"A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/win_setup_config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @define class WinSetupConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' , ] \"\"\" Packages that are needed for all windows nodes, please don't remove any from this default set. \"\"\" broker_dep_packages : List [ str ] = [ 'rabbitmq' , ] \"\"\" Chocolatey packages needed for a windows broker node. \"\"\" worker_dep_packages : List [ str ] = [ 'openjdk11' , # 'openmodelica', 'rsync' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows worker node. \"\"\" worker_pip_packages : List [ str ] = [ # Direct2Cad 'psutil' , # 'creopyson', # Need to use the creopyson from the swri repo 'numpy' , # Craidl # 'gremlinpython', # Handling this internally now ] \"\"\" Pip packages needed for a windows worker node. \"\"\" license_dep_packages : List [ str ] = [ ] \"\"\" Chocolatey packages needed for a windows license server node. \"\"\" graph_dep_packages : List [ str ] = [ 'openjdk11' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows graph server node. \"\"\" qol_packages : List [ str ] = [ 'firefox' , 'notepadplusplus' , 'foxitreader' , 'tess' , 'freecad' , ] \"\"\" Quality of life packages that make actually using a windows node bearable. \"\"\"","title":"WinSetupConfig"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.broker_dep_packages","text":"Chocolatey packages needed for a windows broker node.","title":"broker_dep_packages"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.global_dep_packages","text":"Packages that are needed for all windows nodes, please don't remove any from this default set.","title":"global_dep_packages"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.graph_dep_packages","text":"Chocolatey packages needed for a windows graph server node.","title":"graph_dep_packages"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.license_dep_packages","text":"Chocolatey packages needed for a windows license server node.","title":"license_dep_packages"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.qol_packages","text":"Quality of life packages that make actually using a windows node bearable.","title":"qol_packages"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.worker_dep_packages","text":"Chocolatey packages needed for a windows worker node.","title":"worker_dep_packages"},{"location":"reference/simple_uam/util/config/#simple_uam.util.config.win_setup_config.WinSetupConfig.worker_pip_packages","text":"Pip packages needed for a windows worker node.","title":"worker_pip_packages"},{"location":"reference/simple_uam/util/config/craidl_config/","text":"CraidlConfig \u00b6 Config properties for working with craidl and the gremlin stub server. Source code in simple_uam/util/config/craidl_config.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @define class CraidlConfig (): \"\"\" Config properties for working with craidl and the gremlin stub server. \"\"\" example_dir : Optional [ str ] = field ( default = SI ( \"${path:data_directory}/craidl_examples\" ), ) \"\"\" Dir containing craidl example designs (each in their own subfolder.) \"\"\" stub_server : StubServerConfig = StubServerConfig () \"\"\" Stub server settings. \"\"\" server_host : str = field ( default = SI ( \"localhost\" ) ) \"\"\" The host to connect to when using a gremlin corpus server. \"\"\" server_port : int = field ( default = SI ( \"$ {stub_server.port} \" ) ) \"\"\" The port to connect to when using a gremlin corpus server. \"\"\" static_corpus : str = field ( default = SI ( \"${path:data_directory}/corpus_static_dump.json\" ), ) \"\"\" The parsed static corpus to be generated/used when creating info files. \"\"\" static_corpus_cache : str = field ( default = SI ( \"${path:cache_directory}/static_corpus_cache\" ), ) \"\"\" The cache directory to use when generating a static corpus. \"\"\" use_static_corpus : bool = True \"\"\" Use the static corpus for generating design info files if true, otherwise use the server. \"\"\" example_dir : Optional [ str ] = field ( default = SI ( '${path:data_directory}/craidl_examples' )) class-attribute \u00b6 Dir containing craidl example designs (each in their own subfolder.) server_host : str = field ( default = SI ( 'localhost' )) class-attribute \u00b6 The host to connect to when using a gremlin corpus server. server_port : int = field ( default = SI ( '$ {stub_server.port} ' )) class-attribute \u00b6 The port to connect to when using a gremlin corpus server. static_corpus : str = field ( default = SI ( '${path:data_directory}/corpus_static_dump.json' )) class-attribute \u00b6 The parsed static corpus to be generated/used when creating info files. static_corpus_cache : str = field ( default = SI ( '${path:cache_directory}/static_corpus_cache' )) class-attribute \u00b6 The cache directory to use when generating a static corpus. stub_server : StubServerConfig = StubServerConfig () class-attribute \u00b6 Stub server settings. use_static_corpus : bool = True class-attribute \u00b6 Use the static corpus for generating design info files if true, otherwise use the server. StubServerConfig \u00b6 Settings for the corpus stub server. Source code in simple_uam/util/config/craidl_config.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @define class StubServerConfig (): \"\"\" Settings for the corpus stub server. \"\"\" cache_dir : str = field ( default = SI ( \"${path:cache_directory}/corpus_stub_cache\" ), ) \"\"\" Cache for downloaded installers and static corpus dumps. \"\"\" server_dir : str = field ( default = SI ( \"${path:data_directory}/corpus_stub_server\" ) ) \"\"\" Dir where the gremlin stub server will be unpacked. \"\"\" graphml_corpus : str = field ( default = SI ( \"${path:data_directory}/corpus_stub.graphml\" ) ) \"\"\" The corpus to use with the stub server. Relative paths are assumed to be with respect to repo_root. \"\"\" host : str = field ( default = 'localhost' , ) \"\"\" The host the stub server will server from. \"\"\" port : int = field ( default = 8182 , ) \"\"\" The port the stub server will serve from. \"\"\" read_only : bool = field ( default = False , ) \"\"\" Should the stub server be run as a read only database? \"\"\" service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( \"${path:log_directory}/craidl_stub_db/stdout.log\" ), stderr_file = SI ( \"${path:log_directory}/craidl_stub_db/stderr.log\" ), ) ) \"\"\" Settings for running the stub server as a service. \"\"\" cache_dir : str = field ( default = SI ( '${path:cache_directory}/corpus_stub_cache' )) class-attribute \u00b6 Cache for downloaded installers and static corpus dumps. graphml_corpus : str = field ( default = SI ( '${path:data_directory}/corpus_stub.graphml' )) class-attribute \u00b6 The corpus to use with the stub server. Relative paths are assumed to be with respect to repo_root. host : str = field ( default = 'localhost' ) class-attribute \u00b6 The host the stub server will server from. port : int = field ( default = 8182 ) class-attribute \u00b6 The port the stub server will serve from. read_only : bool = field ( default = False ) class-attribute \u00b6 Should the stub server be run as a read only database? server_dir : str = field ( default = SI ( '${path:data_directory}/corpus_stub_server' )) class-attribute \u00b6 Dir where the gremlin stub server will be unpacked. service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( '${path:log_directory}/craidl_stub_db/stdout.log' ), stderr_file = SI ( '${path:log_directory}/craidl_stub_db/stderr.log' ))) class-attribute \u00b6 Settings for running the stub server as a service.","title":"craidl_config"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig","text":"Config properties for working with craidl and the gremlin stub server. Source code in simple_uam/util/config/craidl_config.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @define class CraidlConfig (): \"\"\" Config properties for working with craidl and the gremlin stub server. \"\"\" example_dir : Optional [ str ] = field ( default = SI ( \"${path:data_directory}/craidl_examples\" ), ) \"\"\" Dir containing craidl example designs (each in their own subfolder.) \"\"\" stub_server : StubServerConfig = StubServerConfig () \"\"\" Stub server settings. \"\"\" server_host : str = field ( default = SI ( \"localhost\" ) ) \"\"\" The host to connect to when using a gremlin corpus server. \"\"\" server_port : int = field ( default = SI ( \"$ {stub_server.port} \" ) ) \"\"\" The port to connect to when using a gremlin corpus server. \"\"\" static_corpus : str = field ( default = SI ( \"${path:data_directory}/corpus_static_dump.json\" ), ) \"\"\" The parsed static corpus to be generated/used when creating info files. \"\"\" static_corpus_cache : str = field ( default = SI ( \"${path:cache_directory}/static_corpus_cache\" ), ) \"\"\" The cache directory to use when generating a static corpus. \"\"\" use_static_corpus : bool = True \"\"\" Use the static corpus for generating design info files if true, otherwise use the server. \"\"\"","title":"CraidlConfig"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.example_dir","text":"Dir containing craidl example designs (each in their own subfolder.)","title":"example_dir"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.server_host","text":"The host to connect to when using a gremlin corpus server.","title":"server_host"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.server_port","text":"The port to connect to when using a gremlin corpus server.","title":"server_port"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.static_corpus","text":"The parsed static corpus to be generated/used when creating info files.","title":"static_corpus"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.static_corpus_cache","text":"The cache directory to use when generating a static corpus.","title":"static_corpus_cache"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.stub_server","text":"Stub server settings.","title":"stub_server"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.CraidlConfig.use_static_corpus","text":"Use the static corpus for generating design info files if true, otherwise use the server.","title":"use_static_corpus"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig","text":"Settings for the corpus stub server. Source code in simple_uam/util/config/craidl_config.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @define class StubServerConfig (): \"\"\" Settings for the corpus stub server. \"\"\" cache_dir : str = field ( default = SI ( \"${path:cache_directory}/corpus_stub_cache\" ), ) \"\"\" Cache for downloaded installers and static corpus dumps. \"\"\" server_dir : str = field ( default = SI ( \"${path:data_directory}/corpus_stub_server\" ) ) \"\"\" Dir where the gremlin stub server will be unpacked. \"\"\" graphml_corpus : str = field ( default = SI ( \"${path:data_directory}/corpus_stub.graphml\" ) ) \"\"\" The corpus to use with the stub server. Relative paths are assumed to be with respect to repo_root. \"\"\" host : str = field ( default = 'localhost' , ) \"\"\" The host the stub server will server from. \"\"\" port : int = field ( default = 8182 , ) \"\"\" The port the stub server will serve from. \"\"\" read_only : bool = field ( default = False , ) \"\"\" Should the stub server be run as a read only database? \"\"\" service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( \"${path:log_directory}/craidl_stub_db/stdout.log\" ), stderr_file = SI ( \"${path:log_directory}/craidl_stub_db/stderr.log\" ), ) ) \"\"\" Settings for running the stub server as a service. \"\"\"","title":"StubServerConfig"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.cache_dir","text":"Cache for downloaded installers and static corpus dumps.","title":"cache_dir"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.graphml_corpus","text":"The corpus to use with the stub server. Relative paths are assumed to be with respect to repo_root.","title":"graphml_corpus"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.host","text":"The host the stub server will server from.","title":"host"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.port","text":"The port the stub server will serve from.","title":"port"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.read_only","text":"Should the stub server be run as a read only database?","title":"read_only"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.server_dir","text":"Dir where the gremlin stub server will be unpacked.","title":"server_dir"},{"location":"reference/simple_uam/util/config/craidl_config/#simple_uam.util.config.craidl_config.StubServerConfig.service","text":"Settings for running the stub server as a service.","title":"service"},{"location":"reference/simple_uam/util/config/d2c_worker_config/","text":"BackendConfig \u00b6 Source code in simple_uam/util/config/d2c_worker_config.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @define class BackendConfig (): enabled : bool = False \"\"\" Is the result backend enabled at all? \"\"\" protocol : str = 'redis' \"\"\" The result backend protocol only redis is supported for now \"\"\" host : str = \"127.0.0.1\" \"\"\" The backend host. \"\"\" port : int = 6379 \"\"\" The broker's port. \"\"\" db : str = \"0\" \"\"\" The database (on redis). \"\"\" url : str = '${.protocol}://${.host}:${.port}/${.db}' \"\"\" The url to connect to for the backend. Note that this supersedes all the finer grade connection parameters if provided. \"\"\" db : str = '0' class-attribute \u00b6 The database (on redis). enabled : bool = False class-attribute \u00b6 Is the result backend enabled at all? host : str = '127.0.0.1' class-attribute \u00b6 The backend host. port : int = 6379 class-attribute \u00b6 The broker's port. protocol : str = 'redis' class-attribute \u00b6 The result backend protocol only redis is supported for now url : str = '${.protocol}://${.host}:${.port}/${.db}' class-attribute \u00b6 The url to connect to for the backend. Note that this supersedes all the finer grade connection parameters if provided. BrokerConfig \u00b6 Source code in simple_uam/util/config/d2c_worker_config.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @define class BrokerConfig (): protocol : str = 'amqp' \"\"\" The broker protocol, must be either 'amqp' (for rabbitmq) or 'redis' (for redis). \"\"\" host : str = \"127.0.0.1\" \"\"\" The broker host. \"\"\" port : int = 5672 \"\"\" The broker's port. \"\"\" db : str = \"\" \"\"\" The database (on redis) or virtualhost (on rabbit mq). \"\"\" url : str = '${.protocol}://${.host}:${.port}${.db}' \"\"\" The url to connect to for the broker. Note that this supersedes all the finer grade connection parameters if provided. \"\"\" db : str = '' class-attribute \u00b6 The database (on redis) or virtualhost (on rabbit mq). host : str = '127.0.0.1' class-attribute \u00b6 The broker host. port : int = 5672 class-attribute \u00b6 The broker's port. protocol : str = 'amqp' class-attribute \u00b6 The broker protocol, must be either 'amqp' (for rabbitmq) or 'redis' (for redis). url : str = '${.protocol}://${.host}:${.port}${.db}' class-attribute \u00b6 The url to connect to for the broker. Note that this supersedes all the finer grade connection parameters if provided. D2CWorkerConfig \u00b6 A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/d2c_worker_config.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @define class D2CWorkerConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" broker : BrokerConfig = BrokerConfig () \"\"\" Configuration options for *connecting to* a broker. This applies to both workers and clients. \"\"\" backend : BackendConfig = BackendConfig () \"\"\" Configuration options for a result backend. Needed if you want to get return values for remote calls directly. \"\"\" max_processes : int = SI ( \"${d2c_workspace:max_workspaces}\" ) \"\"\" Max number of forked processes on a worker node. Default is the number of workspaces on the machine. \"\"\" max_threads : int = 1 \"\"\" Max number of threads per process. Default is 1. \"\"\" shutdown_timeout : int = 600000 \"\"\" Timeout for worker shutdown in milliseconds. \"\"\" skip_logging : bool = False \"\"\" Do we keep dramatiq specific logs? This doesn't affect structlog logs. \"\"\" service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( \"${path:log_directory}/d2c_worker/stdout.log\" ), stderr_file = SI ( \"${path:log_directory}/d2c_worker/stderr.log\" ), ) ) \"\"\" Settings for running the worker node service. \"\"\" backend : BackendConfig = BackendConfig () class-attribute \u00b6 Configuration options for a result backend. Needed if you want to get return values for remote calls directly. broker : BrokerConfig = BrokerConfig () class-attribute \u00b6 Configuration options for connecting to a broker. This applies to both workers and clients. max_processes : int = SI ( '${d2c_workspace:max_workspaces}' ) class-attribute \u00b6 Max number of forked processes on a worker node. Default is the number of workspaces on the machine. max_threads : int = 1 class-attribute \u00b6 Max number of threads per process. Default is 1. service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( '${path:log_directory}/d2c_worker/stdout.log' ), stderr_file = SI ( '${path:log_directory}/d2c_worker/stderr.log' ))) class-attribute \u00b6 Settings for running the worker node service. shutdown_timeout : int = 600000 class-attribute \u00b6 Timeout for worker shutdown in milliseconds. skip_logging : bool = False class-attribute \u00b6 Do we keep dramatiq specific logs? This doesn't affect structlog logs.","title":"d2c_worker_config"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig","text":"Source code in simple_uam/util/config/d2c_worker_config.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @define class BackendConfig (): enabled : bool = False \"\"\" Is the result backend enabled at all? \"\"\" protocol : str = 'redis' \"\"\" The result backend protocol only redis is supported for now \"\"\" host : str = \"127.0.0.1\" \"\"\" The backend host. \"\"\" port : int = 6379 \"\"\" The broker's port. \"\"\" db : str = \"0\" \"\"\" The database (on redis). \"\"\" url : str = '${.protocol}://${.host}:${.port}/${.db}' \"\"\" The url to connect to for the backend. Note that this supersedes all the finer grade connection parameters if provided. \"\"\"","title":"BackendConfig"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig.db","text":"The database (on redis).","title":"db"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig.enabled","text":"Is the result backend enabled at all?","title":"enabled"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig.host","text":"The backend host.","title":"host"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig.port","text":"The broker's port.","title":"port"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig.protocol","text":"The result backend protocol only redis is supported for now","title":"protocol"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BackendConfig.url","text":"The url to connect to for the backend. Note that this supersedes all the finer grade connection parameters if provided.","title":"url"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BrokerConfig","text":"Source code in simple_uam/util/config/d2c_worker_config.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @define class BrokerConfig (): protocol : str = 'amqp' \"\"\" The broker protocol, must be either 'amqp' (for rabbitmq) or 'redis' (for redis). \"\"\" host : str = \"127.0.0.1\" \"\"\" The broker host. \"\"\" port : int = 5672 \"\"\" The broker's port. \"\"\" db : str = \"\" \"\"\" The database (on redis) or virtualhost (on rabbit mq). \"\"\" url : str = '${.protocol}://${.host}:${.port}${.db}' \"\"\" The url to connect to for the broker. Note that this supersedes all the finer grade connection parameters if provided. \"\"\"","title":"BrokerConfig"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BrokerConfig.db","text":"The database (on redis) or virtualhost (on rabbit mq).","title":"db"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BrokerConfig.host","text":"The broker host.","title":"host"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BrokerConfig.port","text":"The broker's port.","title":"port"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BrokerConfig.protocol","text":"The broker protocol, must be either 'amqp' (for rabbitmq) or 'redis' (for redis).","title":"protocol"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.BrokerConfig.url","text":"The url to connect to for the broker. Note that this supersedes all the finer grade connection parameters if provided.","title":"url"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig","text":"A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/d2c_worker_config.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @define class D2CWorkerConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" broker : BrokerConfig = BrokerConfig () \"\"\" Configuration options for *connecting to* a broker. This applies to both workers and clients. \"\"\" backend : BackendConfig = BackendConfig () \"\"\" Configuration options for a result backend. Needed if you want to get return values for remote calls directly. \"\"\" max_processes : int = SI ( \"${d2c_workspace:max_workspaces}\" ) \"\"\" Max number of forked processes on a worker node. Default is the number of workspaces on the machine. \"\"\" max_threads : int = 1 \"\"\" Max number of threads per process. Default is 1. \"\"\" shutdown_timeout : int = 600000 \"\"\" Timeout for worker shutdown in milliseconds. \"\"\" skip_logging : bool = False \"\"\" Do we keep dramatiq specific logs? This doesn't affect structlog logs. \"\"\" service : ServiceConfig = field ( default = ServiceConfig ( stdout_file = SI ( \"${path:log_directory}/d2c_worker/stdout.log\" ), stderr_file = SI ( \"${path:log_directory}/d2c_worker/stderr.log\" ), ) ) \"\"\" Settings for running the worker node service. \"\"\"","title":"D2CWorkerConfig"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.backend","text":"Configuration options for a result backend. Needed if you want to get return values for remote calls directly.","title":"backend"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.broker","text":"Configuration options for connecting to a broker. This applies to both workers and clients.","title":"broker"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.max_processes","text":"Max number of forked processes on a worker node. Default is the number of workspaces on the machine.","title":"max_processes"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.max_threads","text":"Max number of threads per process. Default is 1.","title":"max_threads"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.service","text":"Settings for running the worker node service.","title":"service"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.shutdown_timeout","text":"Timeout for worker shutdown in milliseconds.","title":"shutdown_timeout"},{"location":"reference/simple_uam/util/config/d2c_worker_config/#simple_uam.util.config.d2c_worker_config.D2CWorkerConfig.skip_logging","text":"Do we keep dramatiq specific logs? This doesn't affect structlog logs.","title":"skip_logging"},{"location":"reference/simple_uam/util/config/d2c_workspace_config/","text":"D2CWorkspaceConfig \u00b6 Bases: WorkspaceConfig A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for results to your new class. Source code in simple_uam/util/config/d2c_workspace_config.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @define class D2CWorkspaceConfig ( WorkspaceConfig ): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for `results` to your new class. \"\"\" workspaces_dir : str = field ( default = SI ( \"${path:work_directory}/d2c_workspaces\" ) ) \"\"\" Dir where workspaces are stored. \"\"\" cache_dir : str = field ( default = SI ( \"${path:cache_directory}/d2c_workspaces\" ) ) \"\"\" Dir for cached data. \"\"\" max_workspaces : int = 1 \"\"\" The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine. \"\"\" exclude : List [ str ] = [ '.git' ] exclude_from : List [ str ] = [] result_exclude : List [ str ] = [ '.git' ] result_exclude_from : List [ str ] = [] cache_dir : str = field ( default = SI ( '${path:cache_directory}/d2c_workspaces' )) class-attribute \u00b6 Dir for cached data. max_workspaces : int = 1 class-attribute \u00b6 The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine. workspaces_dir : str = field ( default = SI ( '${path:work_directory}/d2c_workspaces' )) class-attribute \u00b6 Dir where workspaces are stored.","title":"d2c_workspace_config"},{"location":"reference/simple_uam/util/config/d2c_workspace_config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig","text":"Bases: WorkspaceConfig A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for results to your new class. Source code in simple_uam/util/config/d2c_workspace_config.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @define class D2CWorkspaceConfig ( WorkspaceConfig ): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Most defaults are set in WorkspaceConfig but they can be overridden here. Likewise for ResultsConfig, just inherit from that class and set the default for `results` to your new class. \"\"\" workspaces_dir : str = field ( default = SI ( \"${path:work_directory}/d2c_workspaces\" ) ) \"\"\" Dir where workspaces are stored. \"\"\" cache_dir : str = field ( default = SI ( \"${path:cache_directory}/d2c_workspaces\" ) ) \"\"\" Dir for cached data. \"\"\" max_workspaces : int = 1 \"\"\" The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine. \"\"\" exclude : List [ str ] = [ '.git' ] exclude_from : List [ str ] = [] result_exclude : List [ str ] = [ '.git' ] result_exclude_from : List [ str ] = []","title":"D2CWorkspaceConfig"},{"location":"reference/simple_uam/util/config/d2c_workspace_config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig.cache_dir","text":"Dir for cached data.","title":"cache_dir"},{"location":"reference/simple_uam/util/config/d2c_workspace_config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig.max_workspaces","text":"The maximum number of workspaces operating simultaneously. Direct2Cad currently only supports one instance of Creo at a time per machine.","title":"max_workspaces"},{"location":"reference/simple_uam/util/config/d2c_workspace_config/#simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig.workspaces_dir","text":"Dir where workspaces are stored.","title":"workspaces_dir"},{"location":"reference/simple_uam/util/config/lin_setup_config/","text":"LinSetupConfig \u00b6 A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/lin_setup_config.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @define class LinSetupConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' , ] \"\"\" Packages that are needed for all linux nodes, please don't remove any from this default set. \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' ] class-attribute \u00b6 Packages that are needed for all linux nodes, please don't remove any from this default set.","title":"lin_setup_config"},{"location":"reference/simple_uam/util/config/lin_setup_config/#simple_uam.util.config.lin_setup_config.LinSetupConfig","text":"A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/lin_setup_config.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @define class LinSetupConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' , ] \"\"\" Packages that are needed for all linux nodes, please don't remove any from this default set. \"\"\"","title":"LinSetupConfig"},{"location":"reference/simple_uam/util/config/lin_setup_config/#simple_uam.util.config.lin_setup_config.LinSetupConfig.global_dep_packages","text":"Packages that are needed for all linux nodes, please don't remove any from this default set.","title":"global_dep_packages"},{"location":"reference/simple_uam/util/config/manager/","text":"Config \u00b6 Bases: object Singleton, project-wide manager of configuration files. Source code in simple_uam/util/config/manager.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 @define class Config ( object ): \"\"\" Singleton, project-wide manager of configuration files. \"\"\" config_types : Dict [ Type , ConfigData ] = field ( factory = dict , init = False , ) \"\"\" Dict of all the configuration types we manage \"\"\" key_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict of interpolation keys we manage.\"\"\" name_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from type name to key map.\"\"\" file_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from file name to key map.\"\"\" config_dirs : List [ Path ] = field ( factory = list , init = False , ) \"\"\" The search path of configuration directories \"\"\" mode_flags : List [ str ] = field ( factory = list , init = False ) \"\"\" The lode flags from lowest to highest priority \"\"\" def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance def __init__ ( self ): pass def __post_init__ ( self ) -> None : # Read args (non-destructively) parser = self . _get_argparser () ( args , remainder ) = parser . parse_known_args () # Update sys args w/ unused options # sys.argv = remainder # Set path from args self . mode_flags = args . run_mode self . config_dirs = [ * self . _get_conf_dir_chain (), * args . config_dir ] # Load PathConfig path_conf_data = ConfigData ( # type: ignore[call-arg] data_cls = PathConfig , interpolation_key = PATHCONFIG_INTERPOLATION_KEY , conf_file = PATHCONFIG_FILE_NAME , conf_deps = [], default_conf = OmegaConf . structured ( PathConfig ()), load_path = self . _search_path ( PATHCONFIG_FILE_NAME ), overrides = None , ) # Add PathConfig to config_types self . _add_conf_data ( path_conf_data ) def _add_conf_data ( self , conf_data : ConfigData ) -> None : data_cls = conf_data . data_cls # Register resolvers for deps OmegaConf . register_new_resolver ( conf_data . interpolation_key , conf_data . resolver , ) # print(f\"registering: {data_cls}\") self . config_types [ data_cls ] = conf_data self . key_map [ conf_data . interpolation_key ] = data_cls self . file_map [ str ( conf_data . conf_file )] = data_cls self . name_map [ data_cls . __name__ ] = data_cls # print(f\"registering: {self.name_map.keys()}\") def _get_conf_dir_chain ( self ) -> List [ Path ]: \"\"\" Returns the chain of config_dirs to load. \"\"\" current_dir = Path ( PathConfig () . config_dir ) conf_dirs = [ current_dir ] path_conf = current_dir / PATHCONFIG_FILE_NAME while current_dir and path_conf . is_file (): conf = OmegaConf . load ( path_conf ) current_dir = OmegaConf . select ( conf , \"config_directory\" ) if current_dir : current_dir = Path ( current_dir ) conf_dirs . append ( current_dir ) path_conf = current_dir / PATHCONFIG_FILE_NAME return conf_dirs def _search_path ( self , config_file : Union [ str , Path ]) -> List [ Path ]: \"\"\" Gives the search path to use for a particular file, with later paths over-riding earlier ones. \"\"\" path_list = list () for mode_flag in [ None , * self . mode_flags ]: for conf_dir in self . config_dirs : if not mode_flag : path_list . append ( conf_dir / config_file ) else : path_list . append ( conf_dir / mode_flag / config_file ) return path_list @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ()) @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ()) @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ()) @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ()) @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls ) def _load_path ( self , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return self . config_types [ self . _get_config_class ( data_cls )] . load_path @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , ) def _write_configs ( self , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" conf_types = None if configs == None : conf_types = self . config_types . values () else : conf_types = [ self . config_types [ self . _get_config_class ( config )] for config in configs ] path = None if out_dir : path = Path ( out_dir ) path . mkdir ( parents = True , exist_ok = True ) for conf_type in conf_types : conf_type . write_config ( path = path , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , # out_dir=out_dir, ) @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , ) def _register ( self , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" See register for details \"\"\" # If already registered error if data_cls in self . config_types : raise RuntimeError ( f \"Class { data_cls . __name__ } already registered with Config.\" ) # Check dependent configs are installed for par_cls in conf_deps : if par_cls not in self . config_types : raise RuntimeError ( f \"Dependent Config Class { par_cls . __name__ } not registered.\" ) # Create object class_data = ConfigData ( # type: ignore[call-arg] data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , default_conf = None , load_path = self . _search_path ( conf_file ), overrides = None , ) # Add to map self . _add_conf_data ( class_data ) @classmethod def _get_config_class ( cls , key : Union [ str , Type ]) -> Type : \"\"\" Unpacks the key based on its type and value to find the appropriate config class. \"\"\" inst = cls () data_cls = None if isinstance ( key , str ): if key in inst . name_map : data_cls = inst . name_map [ key ] elif key in inst . file_map : data_cls = inst . file_map [ key ] elif key in inst . key_map : data_cls = inst . key_map [ key ] else : RuntimeError ( f \"Config with key { key } not found.\" ) else : data_cls = key return data_cls @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key ) def _get ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . obj def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key ) def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key ) @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key ) def _get_yaml ( self , key : Union [ str , Type [ T ]]) -> str : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . yaml @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key ) def _get_omegaconf ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get_omegaconf for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . config @staticmethod def _get_argparser () -> argparse . ArgumentParser : \"\"\" Get the arg parser for the conf_dir and run_mode options. \"\"\" parser = argparse . ArgumentParser ( add_help = False ) # Config dir argument parser . add_argument ( \"--config-dir\" , action = \"append\" , default = [], type = Path , required = False , help = \"The directory containing all the config files used for this run. Can be given multiple times.\" , ) # Run Mode Argument parser . add_argument ( \"--run-mode\" , action = \"append\" , default = [], type = Path , required = False , help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'. Can be given multiple times.\" , ) return parser @staticmethod def _invoke_args () -> List [ Argument ]: \"\"\" Arguments to be used with an invoke Program class \"\"\" return [ Argument ( name = \"config-dir\" , kind = str , default = [], help = \"The directory containing all the config files used for this run.\" , ), Argument ( name = \"run-mode\" , kind = str , default = [], help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'.\" , ), ] config_dirs : List [ Path ] = field ( factory = list , init = False ) class-attribute \u00b6 The search path of configuration directories config_types : Dict [ Type , ConfigData ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict of all the configuration types we manage file_map : Dict [ str , Type ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict from file name to key map. key_map : Dict [ str , Type ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict of interpolation keys we manage. mode_flags : List [ str ] = field ( factory = list , init = False ) class-attribute \u00b6 The lode flags from lowest to highest priority name_map : Dict [ str , Type ] = field ( factory = dict , init = False ) class-attribute \u00b6 Dict from type name to key map. __class_getitem__ ( key ) \u00b6 See get for details Source code in simple_uam/util/config/manager.py 619 620 621 622 623 def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key ) __getitem__ ( key ) \u00b6 See get for details Source code in simple_uam/util/config/manager.py 613 614 615 616 617 def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key ) __new__ () \u00b6 Creates a singleton object, if it is not created, or else returns the previous singleton object Source code in simple_uam/util/config/manager.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance config_files () classmethod \u00b6 Returns a list of config dataclass file locs. (relative to config dir root) Source code in simple_uam/util/config/manager.py 390 391 392 393 394 395 396 @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ()) config_keys () classmethod \u00b6 Returns a list of config dataclass interpolation keys Source code in simple_uam/util/config/manager.py 398 399 400 401 402 403 @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ()) config_names () classmethod \u00b6 Returns a list of config dataclass type names Source code in simple_uam/util/config/manager.py 383 384 385 386 387 388 @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ()) configs () classmethod \u00b6 Returns a list of config dataclass types Source code in simple_uam/util/config/manager.py 376 377 378 379 380 381 @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ()) get ( key ) classmethod \u00b6 Retrieves an OmegaConf object for the associated dataclass type. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 593 594 595 596 597 598 599 600 601 602 603 604 @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key ) get_omegaconf ( key ) classmethod \u00b6 Returns the object for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 645 646 647 648 649 650 651 652 653 654 655 656 @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key ) get_yaml ( key ) classmethod \u00b6 Returns the yaml string for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 625 626 627 628 629 630 631 632 633 634 635 636 @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key ) load_path ( data_cls ) classmethod \u00b6 returns the list of files that are loaded for a given config dataclass. Source code in simple_uam/util/config/manager.py 405 406 407 408 409 410 @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls ) register ( data_cls , interpolation_key , conf_file , conf_deps = []) classmethod \u00b6 Registers a configuration dataclass with the manager. Raises an error if the operation fails. Parameters: Name Type Description Default data_cls Type The dataclass or attrs class that represents the contents of the file. required interpolation_key str The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. required conf_file Union [ str , Path ] The config file associated with this object, specified relative to the config_dir. required conf_deps List [ Type ] The config classes used for interpolations in this one. [] Source code in simple_uam/util/config/manager.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , ) write_configs ( configs = None , mkdir = True , write_all = False , overwrite = False , comment = True , fields = None , data = None , out_dir = None ) classmethod \u00b6 Writes a sample config out to the filesystem. Parameters: Name Type Description Default configs Optional [ List [ Union [ str , Type ]]] The list of config files to write (default: All) None mkdir default =True Create directories for config files if needed. If false will fail with warning when directory is missing. True write_all default=False Do we create all config files on path or just the terminal one? (ignored if path is provided) False overwrite bool Do we overwrite files if they already exist? False comment bool Should our sample be written as a comment or as a literal yaml document? True fields default=None what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. None data default=None Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. None out_dir default=None directory to write output to if provided. None Source code in simple_uam/util/config/manager.py 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , ) ConfigData \u00b6 Metadata for a single config file used by this project. Source code in simple_uam/util/config/manager.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 @define class ConfigData : \"\"\" Metadata for a single config file used by this project. \"\"\" data_cls : Type = field ( on_setattr = frozen , ) \"\"\" Structured Config Class for this Config \"\"\" interpolation_key : str = field ( on_setattr = frozen , ) \"\"\" The key for associated interpolation resolver \"\"\" conf_file : Path = field ( converter = Path , on_setattr = frozen , ) \"\"\" The config file for this obj, taken relative to the config_dir root \"\"\" conf_deps : List [ Type ] = field ( factory = dict , on_setattr = frozen , ) \"\"\" Config Classes that this depends on (for interpolation) \"\"\" default_conf : Optional [ OmegaConf ] = field ( default = None , on_setattr = frozen , ) \"\"\" The default config that serves as the root of the config tree \"\"\" load_path : List [ Path ] = field ( factory = list , on_setattr = frozen , ) \"\"\" The list of files to load on conf init \"\"\" overrides : Optional [ OmegaConf ] = field ( default = None , on_setattr = frozen , ) \"\"\" A dictionary of values that will override all others when this config is instantiated. I.e. config files can't change it. This is usually where the command line flags, or unchangable settings go. \"\"\" conf_obj : Optional [ OmegaConf ] = field ( default = None , init = False , ) \"\"\" Runtime OmegaConf Object for this Data \"\"\" @property def config ( self ) -> OmegaConf : \"\"\" Retrieves the config for external use, \"\"\" if self . conf_obj : return self . conf_obj # Assemble list of configs to merge configs = list () configs . append ( OmegaConf . structured ( self . data_cls )) if self . default_conf : configs . append ( self . default_conf ) configs = [ * configs , * self . _load_configs ()] if self . overrides : configs . append ( self . overrides ) # Generate merged conf conf = OmegaConf . merge ( * configs ) # Set as read-only # OmegaConf.set_readonly(conf, True) # Save conf to instance and return self . conf_obj = conf return self . conf_obj @property def resolver ( self ) -> Callable [[ str ], str ]: \"\"\" Registers the resolver for this class if possible. Should only be called once per class. \"\"\" def resolve_func ( key : Optional [ str ] = None , conf_dat : ConfigData = self ): if key : val = OmegaConf . select ( conf_dat . config , key , default = None ) if val is None : val = getattr ( conf_dat . obj , key , None ) return val else : return conf_dat . config return functools . partial ( resolve_func , conf_dat = self ) def _load_configs ( self ) -> List [ OmegaConf ]: \"\"\" Will load all existing configs in load_path \"\"\" configs = list () for conf_path in self . load_path : if conf_path . exists (): if not conf_path . is_file (): raise RuntimeError ( f \"Object at ' { conf_path } ' is not a file.\" ) else : configs . append ( OmegaConf . load ( conf_path )) return configs @property def yaml ( self ) -> str : \"\"\"Returns a YAML rep of the current config object.\"\"\" return OmegaConf . to_yaml ( self . config ) @property def obj ( self ) -> Any : \"\"\" Returns a true instance of the config object, rather than a duck typed wrapper with lazy loading. \"\"\" return OmegaConf . to_object ( self . config ) def write_config ( self , path : Union [ None , List [ Path ], List [ str ], Path , str ] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: path (default = None): Paths to write config stubs to. If none and write_all then all files on load path are written. If None and not write_all only the final file on load_path is written. mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. \"\"\" # Normalize path argument. if path == None and write_all : path = self . load_path elif path == None and not write_all : path = list ( self . load_path [ - 1 :]) paths = None if isinstance ( path , list ): paths = [ Path ( f ) for f in path ] else : paths = [ Path ( f )] path = list () for p in paths : if p . exists () and p . is_dir (): path . append ( p / conf_file ) else : path . append ( p ) # Organize data to write conf = OmegaConf . create () if data == None and fields : for field in fields : OmegaConf . update ( conf , field , OmegaConf . select ( self . config , field ), ) elif data == None and not fields : conf = self . config else : conf = OmegaConf . create ( data ) # Get Printed Str yaml = OmegaConf . to_yaml ( conf ) if comment : yaml = \"\" . join ([ f \"# { line } \" for line in yaml . splitlines ( True )]) # Write to each file in the path. for f in path : # Create dir if needed if mkdir : f . parent . mkdir ( parents = True , exist_ok = True ) # Create backup if overwriting if overwrite and f . exists (): backup_file ( f , delete = True ) # Write if no file exists if not f . exists (): f . write_text ( yaml ) conf_deps : List [ Type ] = field ( factory = dict , on_setattr = frozen ) class-attribute \u00b6 Config Classes that this depends on (for interpolation) conf_file : Path = field ( converter = Path , on_setattr = frozen ) class-attribute \u00b6 The config file for this obj, taken relative to the config_dir root conf_obj : Optional [ OmegaConf ] = field ( default = None , init = False ) class-attribute \u00b6 Runtime OmegaConf Object for this Data data_cls : Type = field ( on_setattr = frozen ) class-attribute \u00b6 Structured Config Class for this Config default_conf : Optional [ OmegaConf ] = field ( default = None , on_setattr = frozen ) class-attribute \u00b6 The default config that serves as the root of the config tree interpolation_key : str = field ( on_setattr = frozen ) class-attribute \u00b6 The key for associated interpolation resolver load_path : List [ Path ] = field ( factory = list , on_setattr = frozen ) class-attribute \u00b6 The list of files to load on conf init overrides : Optional [ OmegaConf ] = field ( default = None , on_setattr = frozen ) class-attribute \u00b6 A dictionary of values that will override all others when this config is instantiated. I.e. config files can't change it. This is usually where the command line flags, or unchangable settings go. config () property \u00b6 Retrieves the config for external use, Source code in simple_uam/util/config/manager.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 @property def config ( self ) -> OmegaConf : \"\"\" Retrieves the config for external use, \"\"\" if self . conf_obj : return self . conf_obj # Assemble list of configs to merge configs = list () configs . append ( OmegaConf . structured ( self . data_cls )) if self . default_conf : configs . append ( self . default_conf ) configs = [ * configs , * self . _load_configs ()] if self . overrides : configs . append ( self . overrides ) # Generate merged conf conf = OmegaConf . merge ( * configs ) # Set as read-only # OmegaConf.set_readonly(conf, True) # Save conf to instance and return self . conf_obj = conf return self . conf_obj obj () property \u00b6 Returns a true instance of the config object, rather than a duck typed wrapper with lazy loading. Source code in simple_uam/util/config/manager.py 142 143 144 145 146 147 148 @property def obj ( self ) -> Any : \"\"\" Returns a true instance of the config object, rather than a duck typed wrapper with lazy loading. \"\"\" return OmegaConf . to_object ( self . config ) resolver () property \u00b6 Registers the resolver for this class if possible. Should only be called once per class. Source code in simple_uam/util/config/manager.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @property def resolver ( self ) -> Callable [[ str ], str ]: \"\"\" Registers the resolver for this class if possible. Should only be called once per class. \"\"\" def resolve_func ( key : Optional [ str ] = None , conf_dat : ConfigData = self ): if key : val = OmegaConf . select ( conf_dat . config , key , default = None ) if val is None : val = getattr ( conf_dat . obj , key , None ) return val else : return conf_dat . config return functools . partial ( resolve_func , conf_dat = self ) write_config ( path = None , mkdir = True , write_all = False , overwrite = False , comment = True , fields = None , data = None ) \u00b6 Writes a sample config out to the filesystem. Parameters: Name Type Description Default path default = None Paths to write config stubs to. If none and write_all then all files on load path are written. If None and not write_all only the final file on load_path is written. None mkdir default =True Create directories for config files if needed. If false will fail with warning when directory is missing. True write_all default=False Do we create all config files on path or just the terminal one? (ignored if path is provided) False overwrite bool Do we overwrite files if they already exist? False comment bool Should our sample be written as a comment or as a literal yaml document? True fields default=None what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. None data default=None Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. None Source code in simple_uam/util/config/manager.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def write_config ( self , path : Union [ None , List [ Path ], List [ str ], Path , str ] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: path (default = None): Paths to write config stubs to. If none and write_all then all files on load path are written. If None and not write_all only the final file on load_path is written. mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. \"\"\" # Normalize path argument. if path == None and write_all : path = self . load_path elif path == None and not write_all : path = list ( self . load_path [ - 1 :]) paths = None if isinstance ( path , list ): paths = [ Path ( f ) for f in path ] else : paths = [ Path ( f )] path = list () for p in paths : if p . exists () and p . is_dir (): path . append ( p / conf_file ) else : path . append ( p ) # Organize data to write conf = OmegaConf . create () if data == None and fields : for field in fields : OmegaConf . update ( conf , field , OmegaConf . select ( self . config , field ), ) elif data == None and not fields : conf = self . config else : conf = OmegaConf . create ( data ) # Get Printed Str yaml = OmegaConf . to_yaml ( conf ) if comment : yaml = \"\" . join ([ f \"# { line } \" for line in yaml . splitlines ( True )]) # Write to each file in the path. for f in path : # Create dir if needed if mkdir : f . parent . mkdir ( parents = True , exist_ok = True ) # Create backup if overwriting if overwrite and f . exists (): backup_file ( f , delete = True ) # Write if no file exists if not f . exists (): f . write_text ( yaml ) yaml () property \u00b6 Returns a YAML rep of the current config object. Source code in simple_uam/util/config/manager.py 137 138 139 140 @property def yaml ( self ) -> str : \"\"\"Returns a YAML rep of the current config object.\"\"\" return OmegaConf . to_yaml ( self . config )","title":"manager"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config","text":"Bases: object Singleton, project-wide manager of configuration files. Source code in simple_uam/util/config/manager.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 @define class Config ( object ): \"\"\" Singleton, project-wide manager of configuration files. \"\"\" config_types : Dict [ Type , ConfigData ] = field ( factory = dict , init = False , ) \"\"\" Dict of all the configuration types we manage \"\"\" key_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict of interpolation keys we manage.\"\"\" name_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from type name to key map.\"\"\" file_map : Dict [ str , Type ] = field ( factory = dict , init = False , ) \"\"\" Dict from file name to key map.\"\"\" config_dirs : List [ Path ] = field ( factory = list , init = False , ) \"\"\" The search path of configuration directories \"\"\" mode_flags : List [ str ] = field ( factory = list , init = False ) \"\"\" The lode flags from lowest to highest priority \"\"\" def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance def __init__ ( self ): pass def __post_init__ ( self ) -> None : # Read args (non-destructively) parser = self . _get_argparser () ( args , remainder ) = parser . parse_known_args () # Update sys args w/ unused options # sys.argv = remainder # Set path from args self . mode_flags = args . run_mode self . config_dirs = [ * self . _get_conf_dir_chain (), * args . config_dir ] # Load PathConfig path_conf_data = ConfigData ( # type: ignore[call-arg] data_cls = PathConfig , interpolation_key = PATHCONFIG_INTERPOLATION_KEY , conf_file = PATHCONFIG_FILE_NAME , conf_deps = [], default_conf = OmegaConf . structured ( PathConfig ()), load_path = self . _search_path ( PATHCONFIG_FILE_NAME ), overrides = None , ) # Add PathConfig to config_types self . _add_conf_data ( path_conf_data ) def _add_conf_data ( self , conf_data : ConfigData ) -> None : data_cls = conf_data . data_cls # Register resolvers for deps OmegaConf . register_new_resolver ( conf_data . interpolation_key , conf_data . resolver , ) # print(f\"registering: {data_cls}\") self . config_types [ data_cls ] = conf_data self . key_map [ conf_data . interpolation_key ] = data_cls self . file_map [ str ( conf_data . conf_file )] = data_cls self . name_map [ data_cls . __name__ ] = data_cls # print(f\"registering: {self.name_map.keys()}\") def _get_conf_dir_chain ( self ) -> List [ Path ]: \"\"\" Returns the chain of config_dirs to load. \"\"\" current_dir = Path ( PathConfig () . config_dir ) conf_dirs = [ current_dir ] path_conf = current_dir / PATHCONFIG_FILE_NAME while current_dir and path_conf . is_file (): conf = OmegaConf . load ( path_conf ) current_dir = OmegaConf . select ( conf , \"config_directory\" ) if current_dir : current_dir = Path ( current_dir ) conf_dirs . append ( current_dir ) path_conf = current_dir / PATHCONFIG_FILE_NAME return conf_dirs def _search_path ( self , config_file : Union [ str , Path ]) -> List [ Path ]: \"\"\" Gives the search path to use for a particular file, with later paths over-riding earlier ones. \"\"\" path_list = list () for mode_flag in [ None , * self . mode_flags ]: for conf_dir in self . config_dirs : if not mode_flag : path_list . append ( conf_dir / config_file ) else : path_list . append ( conf_dir / mode_flag / config_file ) return path_list @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ()) @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ()) @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ()) @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ()) @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls ) def _load_path ( self , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return self . config_types [ self . _get_config_class ( data_cls )] . load_path @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , ) def _write_configs ( self , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" conf_types = None if configs == None : conf_types = self . config_types . values () else : conf_types = [ self . config_types [ self . _get_config_class ( config )] for config in configs ] path = None if out_dir : path = Path ( out_dir ) path . mkdir ( parents = True , exist_ok = True ) for conf_type in conf_types : conf_type . write_config ( path = path , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , # out_dir=out_dir, ) @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , ) def _register ( self , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" See register for details \"\"\" # If already registered error if data_cls in self . config_types : raise RuntimeError ( f \"Class { data_cls . __name__ } already registered with Config.\" ) # Check dependent configs are installed for par_cls in conf_deps : if par_cls not in self . config_types : raise RuntimeError ( f \"Dependent Config Class { par_cls . __name__ } not registered.\" ) # Create object class_data = ConfigData ( # type: ignore[call-arg] data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , default_conf = None , load_path = self . _search_path ( conf_file ), overrides = None , ) # Add to map self . _add_conf_data ( class_data ) @classmethod def _get_config_class ( cls , key : Union [ str , Type ]) -> Type : \"\"\" Unpacks the key based on its type and value to find the appropriate config class. \"\"\" inst = cls () data_cls = None if isinstance ( key , str ): if key in inst . name_map : data_cls = inst . name_map [ key ] elif key in inst . file_map : data_cls = inst . file_map [ key ] elif key in inst . key_map : data_cls = inst . key_map [ key ] else : RuntimeError ( f \"Config with key { key } not found.\" ) else : data_cls = key return data_cls @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key ) def _get ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . obj def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key ) def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key ) @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key ) def _get_yaml ( self , key : Union [ str , Type [ T ]]) -> str : \"\"\" See get for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . yaml @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key ) def _get_omegaconf ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get_omegaconf for details \"\"\" return self . config_types [ self . _get_config_class ( key )] . config @staticmethod def _get_argparser () -> argparse . ArgumentParser : \"\"\" Get the arg parser for the conf_dir and run_mode options. \"\"\" parser = argparse . ArgumentParser ( add_help = False ) # Config dir argument parser . add_argument ( \"--config-dir\" , action = \"append\" , default = [], type = Path , required = False , help = \"The directory containing all the config files used for this run. Can be given multiple times.\" , ) # Run Mode Argument parser . add_argument ( \"--run-mode\" , action = \"append\" , default = [], type = Path , required = False , help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'. Can be given multiple times.\" , ) return parser @staticmethod def _invoke_args () -> List [ Argument ]: \"\"\" Arguments to be used with an invoke Program class \"\"\" return [ Argument ( name = \"config-dir\" , kind = str , default = [], help = \"The directory containing all the config files used for this run.\" , ), Argument ( name = \"run-mode\" , kind = str , default = [], help = \"The mode in which this is being run, e.g. 'local', 'remote', or 'production'.\" , ), ]","title":"Config"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.config_dirs","text":"The search path of configuration directories","title":"config_dirs"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.config_types","text":"Dict of all the configuration types we manage","title":"config_types"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.file_map","text":"Dict from file name to key map.","title":"file_map"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.key_map","text":"Dict of interpolation keys we manage.","title":"key_map"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.mode_flags","text":"The lode flags from lowest to highest priority","title":"mode_flags"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.name_map","text":"Dict from type name to key map.","title":"name_map"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.__class_getitem__","text":"See get for details Source code in simple_uam/util/config/manager.py 619 620 621 622 623 def __class_getitem__ ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return cls . get ( key )","title":"__class_getitem__()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.__getitem__","text":"See get for details Source code in simple_uam/util/config/manager.py 613 614 615 616 617 def __getitem__ ( self , key : Union [ str , Type [ T ]]) -> T : \"\"\" See get for details \"\"\" return self . _get ( key )","title":"__getitem__()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.__new__","text":"Creates a singleton object, if it is not created, or else returns the previous singleton object Source code in simple_uam/util/config/manager.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __new__ ( cls ): \"\"\" Creates a singleton object, if it is not created, or else returns the previous singleton object \"\"\" if not hasattr ( cls , \"instance\" ): instance = super ( Config , cls ) . __new__ ( cls ) instance . __init__ () instance . __attrs_init__ () instance . __post_init__ () cls . instance = instance return cls . instance","title":"__new__()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.config_files","text":"Returns a list of config dataclass file locs. (relative to config dir root) Source code in simple_uam/util/config/manager.py 390 391 392 393 394 395 396 @classmethod def config_files ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass file locs. (relative to config dir root) \"\"\" return list ( cls () . file_map . keys ())","title":"config_files()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.config_keys","text":"Returns a list of config dataclass interpolation keys Source code in simple_uam/util/config/manager.py 398 399 400 401 402 403 @classmethod def config_keys ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass interpolation keys \"\"\" return list ( cls () . key_map . keys ())","title":"config_keys()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.config_names","text":"Returns a list of config dataclass type names Source code in simple_uam/util/config/manager.py 383 384 385 386 387 388 @classmethod def config_names ( cls ) -> List [ str ]: \"\"\" Returns a list of config dataclass type names \"\"\" return list ( cls () . name_map . keys ())","title":"config_names()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.configs","text":"Returns a list of config dataclass types Source code in simple_uam/util/config/manager.py 376 377 378 379 380 381 @classmethod def configs ( cls ) -> List [ Type ]: \"\"\" Returns a list of config dataclass types \"\"\" return list ( cls () . config_types . keys ())","title":"configs()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.get","text":"Retrieves an OmegaConf object for the associated dataclass type. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 593 594 595 596 597 598 599 600 601 602 603 604 @classmethod def get ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Retrieves an OmegaConf object for the associated dataclass type. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get ( key )","title":"get()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.get_omegaconf","text":"Returns the object for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 645 646 647 648 649 650 651 652 653 654 655 656 @classmethod def get_omegaconf ( cls , key : Union [ str , Type [ T ]]) -> T : \"\"\" Returns the object for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_omegaconf ( key )","title":"get_omegaconf()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.get_yaml","text":"Returns the yaml string for a given config key. Parameters: Name Type Description Default key Union [ str , Type [ T ]] The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. required Source code in simple_uam/util/config/manager.py 625 626 627 628 629 630 631 632 633 634 635 636 @classmethod def get_yaml ( cls , key : Union [ str , Type [ T ]]) -> str : \"\"\" Returns the yaml string for a given config key. Arguments: key: The dataclass or attrs class that represents the contents of the file. Note: this must have been previously registered. \"\"\" return cls () . _get_yaml ( key )","title":"get_yaml()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.load_path","text":"returns the list of files that are loaded for a given config dataclass. Source code in simple_uam/util/config/manager.py 405 406 407 408 409 410 @classmethod def load_path ( cls , data_cls : Union [ str , Type ]) -> List [ Path ]: \"\"\" returns the list of files that are loaded for a given config dataclass. \"\"\" return cls () . _load_path ( data_cls )","title":"load_path()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.register","text":"Registers a configuration dataclass with the manager. Raises an error if the operation fails. Parameters: Name Type Description Default data_cls Type The dataclass or attrs class that represents the contents of the file. required interpolation_key str The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. required conf_file Union [ str , Path ] The config file associated with this object, specified relative to the config_dir. required conf_deps List [ Type ] The config classes used for interpolations in this one. [] Source code in simple_uam/util/config/manager.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 @classmethod def register ( cls , data_cls : Type , interpolation_key : str , conf_file : Union [ str , Path ], conf_deps : List [ Type ] = [] ) -> None : \"\"\" Registers a configuration dataclass with the manager. Raises an error if the operation fails. Arguments: data_cls: The dataclass or attrs class that represents the contents of the file. interpolation_key: The string used to specify the resolver for this config in other configs. E.g. If the data_cls has a field \"name\" and the resolve_key is \"example_conf\" then other configurations can use \"${example_conf:name}\" in their string interpolations. conf_file: The config file associated with this object, specified relative to the config_dir. conf_deps: The config classes used for interpolations in this one. \"\"\" cls () . _register ( data_cls = data_cls , interpolation_key = interpolation_key , conf_file = conf_file , conf_deps = conf_deps , )","title":"register()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.Config.write_configs","text":"Writes a sample config out to the filesystem. Parameters: Name Type Description Default configs Optional [ List [ Union [ str , Type ]]] The list of config files to write (default: All) None mkdir default =True Create directories for config files if needed. If false will fail with warning when directory is missing. True write_all default=False Do we create all config files on path or just the terminal one? (ignored if path is provided) False overwrite bool Do we overwrite files if they already exist? False comment bool Should our sample be written as a comment or as a literal yaml document? True fields default=None what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. None data default=None Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. None out_dir default=None directory to write output to if provided. None Source code in simple_uam/util/config/manager.py 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 @classmethod def write_configs ( cls , configs : Optional [ List [ Union [ str , Type ]]] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , out_dir : Union [ str , Path , None ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: configs: The list of config files to write (default: All) mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. out_dir (default=None): directory to write output to if provided. \"\"\" return cls () . _write_configs ( configs = configs , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , fields = fields , data = data , out_dir = out_dir , )","title":"write_configs()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData","text":"Metadata for a single config file used by this project. Source code in simple_uam/util/config/manager.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 @define class ConfigData : \"\"\" Metadata for a single config file used by this project. \"\"\" data_cls : Type = field ( on_setattr = frozen , ) \"\"\" Structured Config Class for this Config \"\"\" interpolation_key : str = field ( on_setattr = frozen , ) \"\"\" The key for associated interpolation resolver \"\"\" conf_file : Path = field ( converter = Path , on_setattr = frozen , ) \"\"\" The config file for this obj, taken relative to the config_dir root \"\"\" conf_deps : List [ Type ] = field ( factory = dict , on_setattr = frozen , ) \"\"\" Config Classes that this depends on (for interpolation) \"\"\" default_conf : Optional [ OmegaConf ] = field ( default = None , on_setattr = frozen , ) \"\"\" The default config that serves as the root of the config tree \"\"\" load_path : List [ Path ] = field ( factory = list , on_setattr = frozen , ) \"\"\" The list of files to load on conf init \"\"\" overrides : Optional [ OmegaConf ] = field ( default = None , on_setattr = frozen , ) \"\"\" A dictionary of values that will override all others when this config is instantiated. I.e. config files can't change it. This is usually where the command line flags, or unchangable settings go. \"\"\" conf_obj : Optional [ OmegaConf ] = field ( default = None , init = False , ) \"\"\" Runtime OmegaConf Object for this Data \"\"\" @property def config ( self ) -> OmegaConf : \"\"\" Retrieves the config for external use, \"\"\" if self . conf_obj : return self . conf_obj # Assemble list of configs to merge configs = list () configs . append ( OmegaConf . structured ( self . data_cls )) if self . default_conf : configs . append ( self . default_conf ) configs = [ * configs , * self . _load_configs ()] if self . overrides : configs . append ( self . overrides ) # Generate merged conf conf = OmegaConf . merge ( * configs ) # Set as read-only # OmegaConf.set_readonly(conf, True) # Save conf to instance and return self . conf_obj = conf return self . conf_obj @property def resolver ( self ) -> Callable [[ str ], str ]: \"\"\" Registers the resolver for this class if possible. Should only be called once per class. \"\"\" def resolve_func ( key : Optional [ str ] = None , conf_dat : ConfigData = self ): if key : val = OmegaConf . select ( conf_dat . config , key , default = None ) if val is None : val = getattr ( conf_dat . obj , key , None ) return val else : return conf_dat . config return functools . partial ( resolve_func , conf_dat = self ) def _load_configs ( self ) -> List [ OmegaConf ]: \"\"\" Will load all existing configs in load_path \"\"\" configs = list () for conf_path in self . load_path : if conf_path . exists (): if not conf_path . is_file (): raise RuntimeError ( f \"Object at ' { conf_path } ' is not a file.\" ) else : configs . append ( OmegaConf . load ( conf_path )) return configs @property def yaml ( self ) -> str : \"\"\"Returns a YAML rep of the current config object.\"\"\" return OmegaConf . to_yaml ( self . config ) @property def obj ( self ) -> Any : \"\"\" Returns a true instance of the config object, rather than a duck typed wrapper with lazy loading. \"\"\" return OmegaConf . to_object ( self . config ) def write_config ( self , path : Union [ None , List [ Path ], List [ str ], Path , str ] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: path (default = None): Paths to write config stubs to. If none and write_all then all files on load path are written. If None and not write_all only the final file on load_path is written. mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. \"\"\" # Normalize path argument. if path == None and write_all : path = self . load_path elif path == None and not write_all : path = list ( self . load_path [ - 1 :]) paths = None if isinstance ( path , list ): paths = [ Path ( f ) for f in path ] else : paths = [ Path ( f )] path = list () for p in paths : if p . exists () and p . is_dir (): path . append ( p / conf_file ) else : path . append ( p ) # Organize data to write conf = OmegaConf . create () if data == None and fields : for field in fields : OmegaConf . update ( conf , field , OmegaConf . select ( self . config , field ), ) elif data == None and not fields : conf = self . config else : conf = OmegaConf . create ( data ) # Get Printed Str yaml = OmegaConf . to_yaml ( conf ) if comment : yaml = \"\" . join ([ f \"# { line } \" for line in yaml . splitlines ( True )]) # Write to each file in the path. for f in path : # Create dir if needed if mkdir : f . parent . mkdir ( parents = True , exist_ok = True ) # Create backup if overwriting if overwrite and f . exists (): backup_file ( f , delete = True ) # Write if no file exists if not f . exists (): f . write_text ( yaml )","title":"ConfigData"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.conf_deps","text":"Config Classes that this depends on (for interpolation)","title":"conf_deps"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.conf_file","text":"The config file for this obj, taken relative to the config_dir root","title":"conf_file"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.conf_obj","text":"Runtime OmegaConf Object for this Data","title":"conf_obj"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.data_cls","text":"Structured Config Class for this Config","title":"data_cls"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.default_conf","text":"The default config that serves as the root of the config tree","title":"default_conf"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.interpolation_key","text":"The key for associated interpolation resolver","title":"interpolation_key"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.load_path","text":"The list of files to load on conf init","title":"load_path"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.overrides","text":"A dictionary of values that will override all others when this config is instantiated. I.e. config files can't change it. This is usually where the command line flags, or unchangable settings go.","title":"overrides"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.config","text":"Retrieves the config for external use, Source code in simple_uam/util/config/manager.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 @property def config ( self ) -> OmegaConf : \"\"\" Retrieves the config for external use, \"\"\" if self . conf_obj : return self . conf_obj # Assemble list of configs to merge configs = list () configs . append ( OmegaConf . structured ( self . data_cls )) if self . default_conf : configs . append ( self . default_conf ) configs = [ * configs , * self . _load_configs ()] if self . overrides : configs . append ( self . overrides ) # Generate merged conf conf = OmegaConf . merge ( * configs ) # Set as read-only # OmegaConf.set_readonly(conf, True) # Save conf to instance and return self . conf_obj = conf return self . conf_obj","title":"config()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.obj","text":"Returns a true instance of the config object, rather than a duck typed wrapper with lazy loading. Source code in simple_uam/util/config/manager.py 142 143 144 145 146 147 148 @property def obj ( self ) -> Any : \"\"\" Returns a true instance of the config object, rather than a duck typed wrapper with lazy loading. \"\"\" return OmegaConf . to_object ( self . config )","title":"obj()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.resolver","text":"Registers the resolver for this class if possible. Should only be called once per class. Source code in simple_uam/util/config/manager.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @property def resolver ( self ) -> Callable [[ str ], str ]: \"\"\" Registers the resolver for this class if possible. Should only be called once per class. \"\"\" def resolve_func ( key : Optional [ str ] = None , conf_dat : ConfigData = self ): if key : val = OmegaConf . select ( conf_dat . config , key , default = None ) if val is None : val = getattr ( conf_dat . obj , key , None ) return val else : return conf_dat . config return functools . partial ( resolve_func , conf_dat = self )","title":"resolver()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.write_config","text":"Writes a sample config out to the filesystem. Parameters: Name Type Description Default path default = None Paths to write config stubs to. If none and write_all then all files on load path are written. If None and not write_all only the final file on load_path is written. None mkdir default =True Create directories for config files if needed. If false will fail with warning when directory is missing. True write_all default=False Do we create all config files on path or just the terminal one? (ignored if path is provided) False overwrite bool Do we overwrite files if they already exist? False comment bool Should our sample be written as a comment or as a literal yaml document? True fields default=None what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. None data default=None Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. None Source code in simple_uam/util/config/manager.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def write_config ( self , path : Union [ None , List [ Path ], List [ str ], Path , str ] = None , mkdir : bool = True , write_all : bool = False , overwrite : bool = False , comment : bool = True , fields : Optional [ List [ str ]] = None , data : Optional [ Any ] = None , ): \"\"\" Writes a sample config out to the filesystem. Arguments: path (default = None): Paths to write config stubs to. If none and write_all then all files on load path are written. If None and not write_all only the final file on load_path is written. mkdir (default =True): Create directories for config files if needed. If false will fail with warning when directory is missing. write_all (default=False): Do we create all config files on path or just the terminal one? (ignored if path is provided) overwrite: Do we overwrite files if they already exist? comment: Should our sample be written as a comment or as a literal yaml document? fields (default=None): what fields should be written out to the config file (as a subset of the available fields, in omegaconf selector format). Ignored if data is provided. data (default=None): Data to write to config, if not the currently loaded configuration. In some format that can be written to yaml. \"\"\" # Normalize path argument. if path == None and write_all : path = self . load_path elif path == None and not write_all : path = list ( self . load_path [ - 1 :]) paths = None if isinstance ( path , list ): paths = [ Path ( f ) for f in path ] else : paths = [ Path ( f )] path = list () for p in paths : if p . exists () and p . is_dir (): path . append ( p / conf_file ) else : path . append ( p ) # Organize data to write conf = OmegaConf . create () if data == None and fields : for field in fields : OmegaConf . update ( conf , field , OmegaConf . select ( self . config , field ), ) elif data == None and not fields : conf = self . config else : conf = OmegaConf . create ( data ) # Get Printed Str yaml = OmegaConf . to_yaml ( conf ) if comment : yaml = \"\" . join ([ f \"# { line } \" for line in yaml . splitlines ( True )]) # Write to each file in the path. for f in path : # Create dir if needed if mkdir : f . parent . mkdir ( parents = True , exist_ok = True ) # Create backup if overwriting if overwrite and f . exists (): backup_file ( f , delete = True ) # Write if no file exists if not f . exists (): f . write_text ( yaml )","title":"write_config()"},{"location":"reference/simple_uam/util/config/manager/#simple_uam.util.config.manager.ConfigData.yaml","text":"Returns a YAML rep of the current config object. Source code in simple_uam/util/config/manager.py 137 138 139 140 @property def yaml ( self ) -> str : \"\"\"Returns a YAML rep of the current config object.\"\"\" return OmegaConf . to_yaml ( self . config )","title":"yaml()"},{"location":"reference/simple_uam/util/config/path_config/","text":"PathConfig \u00b6 Source code in simple_uam/util/config/path_config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @define class PathConfig : config_directory : str = field ( default = str ( dirs . site_config_path / 'config' ), converter = str , ) cache_directory : str = field ( default = str ( dirs . site_data_path / 'cache' ), converter = str , ) log_directory : str = field ( default = dirs . user_log_dir , converter = str , ) work_directory : str = field ( default = str ( dirs . site_data_path ), converter = str , ) data_directory : str = field ( default = str ( dirs . site_data_path / 'data' ), converter = str , ) @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory ) @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory ) @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory ) @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory ) @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory ) @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data' cache_dir () property \u00b6 Cache Directory. Source code in simple_uam/util/config/path_config.py 50 51 52 53 @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory ) config_dir () property \u00b6 Config file root dir. Source code in simple_uam/util/config/path_config.py 45 46 47 48 @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory ) data_dir () property \u00b6 Ysytem user static data storage. Source code in simple_uam/util/config/path_config.py 65 66 67 68 @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory ) log_dir () property \u00b6 Log Storage Directory. Source code in simple_uam/util/config/path_config.py 55 56 57 58 @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory ) repo_data_dir () property \u00b6 Repository static data storage. Source code in simple_uam/util/config/path_config.py 70 71 72 73 @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data' repo_dir () property \u00b6 Repository root dir. Source code in simple_uam/util/config/path_config.py 40 41 42 43 @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent work_dir () property \u00b6 Working directory for assorted operations. Source code in simple_uam/util/config/path_config.py 60 61 62 63 @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory )","title":"path_config"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig","text":"Source code in simple_uam/util/config/path_config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @define class PathConfig : config_directory : str = field ( default = str ( dirs . site_config_path / 'config' ), converter = str , ) cache_directory : str = field ( default = str ( dirs . site_data_path / 'cache' ), converter = str , ) log_directory : str = field ( default = dirs . user_log_dir , converter = str , ) work_directory : str = field ( default = str ( dirs . site_data_path ), converter = str , ) data_directory : str = field ( default = str ( dirs . site_data_path / 'data' ), converter = str , ) @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory ) @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory ) @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory ) @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory ) @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory ) @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data'","title":"PathConfig"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.cache_dir","text":"Cache Directory. Source code in simple_uam/util/config/path_config.py 50 51 52 53 @property def cache_dir ( self ): \"\"\" Cache Directory. \"\"\" return Path ( self . cache_directory )","title":"cache_dir()"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.config_dir","text":"Config file root dir. Source code in simple_uam/util/config/path_config.py 45 46 47 48 @property def config_dir ( self ): \"\"\" Config file root dir. \"\"\" return Path ( self . config_directory )","title":"config_dir()"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.data_dir","text":"Ysytem user static data storage. Source code in simple_uam/util/config/path_config.py 65 66 67 68 @property def data_dir ( self ): \"\"\" Ysytem user static data storage. \"\"\" return Path ( self . data_directory )","title":"data_dir()"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.log_dir","text":"Log Storage Directory. Source code in simple_uam/util/config/path_config.py 55 56 57 58 @property def log_dir ( self ): \"\"\" Log Storage Directory. \"\"\" return Path ( self . log_directory )","title":"log_dir()"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.repo_data_dir","text":"Repository static data storage. Source code in simple_uam/util/config/path_config.py 70 71 72 73 @property def repo_data_dir ( self ): \"\"\" Repository static data storage. \"\"\" return Path ( self . repo_dir ) / 'data'","title":"repo_data_dir()"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.repo_dir","text":"Repository root dir. Source code in simple_uam/util/config/path_config.py 40 41 42 43 @property def repo_dir ( self ): \"\"\" Repository root dir. \"\"\" return Path ( __file__ ) . resolve () . parent . parent . parent . parent . parent","title":"repo_dir()"},{"location":"reference/simple_uam/util/config/path_config/#simple_uam.util.config.path_config.PathConfig.work_dir","text":"Working directory for assorted operations. Source code in simple_uam/util/config/path_config.py 60 61 62 63 @property def work_dir ( self ): \"\"\" Working directory for assorted operations. \"\"\" return Path ( self . work_directory )","title":"work_dir()"},{"location":"reference/simple_uam/util/config/service_config/","text":"ServiceConfig \u00b6 Dataclass for options concerning how to run a server service. Source code in simple_uam/util/config/service_config.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @define class ServiceConfig (): \"\"\" Dataclass for options concerning how to run a server service. \"\"\" priority : str = field ( default = 'NORMAL' , kw_only = True , ) \"\"\" Service run priority, options are: 'REALTIME', 'HIGH', 'ABOVE_NORMAL', 'NORMAL', 'BELOW_NORMAL', and 'IDLE'. \"\"\" exit_action : str = field ( default = 'Restart' , kw_only = True , ) \"\"\" The action to perform if the service exits. Options are 'Restart', 'Ignore', and 'Exit'. \"\"\" restart_throttle : int = field ( default = 5000 , kw_only = True , ) \"\"\" Delay restart if app runs for less than given time, in ms. \"\"\" restart_delay : int = field ( default = 1000 , kw_only = True , ) \"\"\" The wait between restarts if the service exits. \"\"\" redirect_io : bool = field ( default = False , kw_only = True , ) \"\"\" Redirect IO to files. \"\"\" stdout_file : str = field ( ) \"\"\" File to redirect stdout to if log redirection is enabled. \"\"\" stderr_file : str = field ( ) \"\"\" File to redirect stderr to if log redirection is enabled. \"\"\" rotate_io : bool = field ( default = True , kw_only = True , ) \"\"\" Rotate out large or old io redirection files. \"\"\" # env_vars : Dict[str,str] = field( # factory=dict, # kw_only=True, # ) # \"\"\" # Environment vars to set while running the service as dict from var name to # var value. # \"\"\" auto_start : bool = field ( default = False , kw_only = True , ) \"\"\" Should this service automatically start on boot? \"\"\" console : bool = field ( default = True , ) \"\"\" Do we run this app in a visible console. \"\"\" interactive : bool = field ( default = False , ) \"\"\" Is this a standalone service or one which can interact the desktop? \"\"\" auto_start : bool = field ( default = False , kw_only = True ) class-attribute \u00b6 Should this service automatically start on boot? console : bool = field ( default = True ) class-attribute \u00b6 Do we run this app in a visible console. exit_action : str = field ( default = 'Restart' , kw_only = True ) class-attribute \u00b6 The action to perform if the service exits. Options are 'Restart', 'Ignore', and 'Exit'. interactive : bool = field ( default = False ) class-attribute \u00b6 Is this a standalone service or one which can interact the desktop? priority : str = field ( default = 'NORMAL' , kw_only = True ) class-attribute \u00b6 Service run priority, options are: 'REALTIME', 'HIGH', 'ABOVE_NORMAL', 'NORMAL', 'BELOW_NORMAL', and 'IDLE'. redirect_io : bool = field ( default = False , kw_only = True ) class-attribute \u00b6 Redirect IO to files. restart_delay : int = field ( default = 1000 , kw_only = True ) class-attribute \u00b6 The wait between restarts if the service exits. restart_throttle : int = field ( default = 5000 , kw_only = True ) class-attribute \u00b6 Delay restart if app runs for less than given time, in ms. rotate_io : bool = field ( default = True , kw_only = True ) class-attribute \u00b6 Rotate out large or old io redirection files. stderr_file : str = field () class-attribute \u00b6 File to redirect stderr to if log redirection is enabled. stdout_file : str = field () class-attribute \u00b6 File to redirect stdout to if log redirection is enabled.","title":"service_config"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig","text":"Dataclass for options concerning how to run a server service. Source code in simple_uam/util/config/service_config.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @define class ServiceConfig (): \"\"\" Dataclass for options concerning how to run a server service. \"\"\" priority : str = field ( default = 'NORMAL' , kw_only = True , ) \"\"\" Service run priority, options are: 'REALTIME', 'HIGH', 'ABOVE_NORMAL', 'NORMAL', 'BELOW_NORMAL', and 'IDLE'. \"\"\" exit_action : str = field ( default = 'Restart' , kw_only = True , ) \"\"\" The action to perform if the service exits. Options are 'Restart', 'Ignore', and 'Exit'. \"\"\" restart_throttle : int = field ( default = 5000 , kw_only = True , ) \"\"\" Delay restart if app runs for less than given time, in ms. \"\"\" restart_delay : int = field ( default = 1000 , kw_only = True , ) \"\"\" The wait between restarts if the service exits. \"\"\" redirect_io : bool = field ( default = False , kw_only = True , ) \"\"\" Redirect IO to files. \"\"\" stdout_file : str = field ( ) \"\"\" File to redirect stdout to if log redirection is enabled. \"\"\" stderr_file : str = field ( ) \"\"\" File to redirect stderr to if log redirection is enabled. \"\"\" rotate_io : bool = field ( default = True , kw_only = True , ) \"\"\" Rotate out large or old io redirection files. \"\"\" # env_vars : Dict[str,str] = field( # factory=dict, # kw_only=True, # ) # \"\"\" # Environment vars to set while running the service as dict from var name to # var value. # \"\"\" auto_start : bool = field ( default = False , kw_only = True , ) \"\"\" Should this service automatically start on boot? \"\"\" console : bool = field ( default = True , ) \"\"\" Do we run this app in a visible console. \"\"\" interactive : bool = field ( default = False , ) \"\"\" Is this a standalone service or one which can interact the desktop? \"\"\"","title":"ServiceConfig"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.auto_start","text":"Should this service automatically start on boot?","title":"auto_start"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.console","text":"Do we run this app in a visible console.","title":"console"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.exit_action","text":"The action to perform if the service exits. Options are 'Restart', 'Ignore', and 'Exit'.","title":"exit_action"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.interactive","text":"Is this a standalone service or one which can interact the desktop?","title":"interactive"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.priority","text":"Service run priority, options are: 'REALTIME', 'HIGH', 'ABOVE_NORMAL', 'NORMAL', 'BELOW_NORMAL', and 'IDLE'.","title":"priority"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.redirect_io","text":"Redirect IO to files.","title":"redirect_io"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.restart_delay","text":"The wait between restarts if the service exits.","title":"restart_delay"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.restart_throttle","text":"Delay restart if app runs for less than given time, in ms.","title":"restart_throttle"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.rotate_io","text":"Rotate out large or old io redirection files.","title":"rotate_io"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.stderr_file","text":"File to redirect stderr to if log redirection is enabled.","title":"stderr_file"},{"location":"reference/simple_uam/util/config/service_config/#simple_uam.util.config.service_config.ServiceConfig.stdout_file","text":"File to redirect stdout to if log redirection is enabled.","title":"stdout_file"},{"location":"reference/simple_uam/util/config/tasks/","text":"dir ( ctx , all = False ) \u00b6 Prints the current terminal config directory. Parameters: Name Type Description Default all Print all config directories in load order (higher priority last). False Source code in simple_uam/util/config/tasks.py 9 10 11 12 13 14 15 16 17 18 19 20 21 @task def dir ( ctx , all = False ): \"\"\" Prints the current terminal config directory. Arguments: all: Print all config directories in load order (higher priority last). \"\"\" if all : for d in Config () . config_dirs : print ( str ( d )) else : print ( str ( Config () . config_dirs [ - 1 ])) file ( ctx , config , all = False ) \u00b6 Prints the terminal file examined when loading a particular config. Parameters: Name Type Description Default config the file to examine required all Print all the files examined in load order (highest priority last). False Source code in simple_uam/util/config/tasks.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @task def file ( ctx , config , all = False ): \"\"\" Prints the terminal file examined when loading a particular config. Arguments: config: the file to examine all: Print all the files examined in load order (highest priority last). \"\"\" if all : for p in Config () . load_path ( config ): print ( str ( p )) else : print ( str ( Config () . load_path ( config )[ - 1 ])) list_classes ( ctx ) \u00b6 Prints a list of registered configuration classes Source code in simple_uam/util/config/tasks.py 38 39 40 41 42 43 44 @task def list_classes ( ctx ): \"\"\" Prints a list of registered configuration classes \"\"\" for n in Config . config_names (): print ( n ) list_files ( ctx ) \u00b6 Prints a list of file locations (relative to config_dir) for config classes. Source code in simple_uam/util/config/tasks.py 54 55 56 57 58 59 60 @task def list_files ( ctx ): \"\"\" Prints a list of file locations (relative to config_dir) for config classes. \"\"\" for f in Config . config_files (): print ( f ) list_keys ( ctx ) \u00b6 Prints a list of interpolation keys for available config classes Source code in simple_uam/util/config/tasks.py 46 47 48 49 50 51 52 @task def list_keys ( ctx ): \"\"\" Prints a list of interpolation keys for available config classes \"\"\" for k in Config . config_keys (): print ( k ) print_config ( ctx , config , resolved = False , all = False ) \u00b6 Prints the currently loaded config data for a given class to STDOUT Parameters: Name Type Description Default config The class name, interpolation key, or config file name of the config to print out. Can be given multiple times. required resolve Should we resolve all the interpolations before printing? required all Print all the configs, mutually exclusive with key. False Source code in simple_uam/util/config/tasks.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @task ( name = \"print\" , iterable = [ 'config' ]) def print_config ( ctx , config , resolved = False , all = False ): \"\"\" Prints the currently loaded config data for a given class to STDOUT Arguments: config: The class name, interpolation key, or config file name of the config to print out. Can be given multiple times. resolve: Should we resolve all the interpolations before printing? all: Print all the configs, mutually exclusive with key. \"\"\" keys = config if len ( keys ) == 0 and all : keys = [ str ( k ) for k in Config () . file_map . keys ()] elif len ( keys ) == 0 : raise RuntimeError ( \"Please specify keys to print or '--all'\" ) for k in keys : config_class = Config () . _get_config_class ( k ) config_data = Config () . config_types [ config_class ] filename = str ( config_data . conf_file ) print ( f \"### { filename } ### \\n \" ) # Print here to help w/ debug conf = config_data . config if resolved : with read_write ( conf ): OmegaConf . resolve ( conf ) print ( OmegaConf . to_yaml ( conf )) write ( ctx , config = None , mkdir = True , write_all = False , overwrite = False , comment = True , output = None ) \u00b6 Writes the current configuration out to file in the appropriate location. Parameters: Name Type Description Default config default=all configs The identifier for the configuration to be printed out. This argument can be provided multiple times to specify multiple config files. None mkdir default=True Do we create the config directories if needed. True write_all default=False Do we write the config files for all possible modes? Otherwise, only write the file for the current run-mode. False overwrite default=False Do we overwrite existing files (creating backups as needed)? Otherwise, skip existing config files. False comment default=True Should the configs be written as a block comment? Otherwise, write the raw yaml directly. True output default=None The directory to write the output files to. None Source code in simple_uam/util/config/tasks.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 @task ( # positional=[\"config\"], iterable = [ \"config\" ], ) def write ( ctx , config = None , mkdir = True , write_all = False , overwrite = False , comment = True , output = None ): \"\"\" Writes the current configuration out to file in the appropriate location. Arguments: config (default=all configs): The identifier for the configuration to be printed out. This argument can be provided multiple times to specify multiple config files. mkdir (default=True): Do we create the config directories if needed. write_all (default=False): Do we write the config files for all possible modes? Otherwise, only write the file for the current run-mode. overwrite (default=False): Do we overwrite existing files (creating backups as needed)? Otherwise, skip existing config files. comment (default=True): Should the configs be written as a block comment? Otherwise, write the raw yaml directly. output (default=None): The directory to write the output files to. \"\"\" # Normalize arg for call if config == []: config = None Config . write_configs ( config , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , out_dir = output , )","title":"tasks"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.dir","text":"Prints the current terminal config directory. Parameters: Name Type Description Default all Print all config directories in load order (higher priority last). False Source code in simple_uam/util/config/tasks.py 9 10 11 12 13 14 15 16 17 18 19 20 21 @task def dir ( ctx , all = False ): \"\"\" Prints the current terminal config directory. Arguments: all: Print all config directories in load order (higher priority last). \"\"\" if all : for d in Config () . config_dirs : print ( str ( d )) else : print ( str ( Config () . config_dirs [ - 1 ]))","title":"dir()"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.file","text":"Prints the terminal file examined when loading a particular config. Parameters: Name Type Description Default config the file to examine required all Print all the files examined in load order (highest priority last). False Source code in simple_uam/util/config/tasks.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @task def file ( ctx , config , all = False ): \"\"\" Prints the terminal file examined when loading a particular config. Arguments: config: the file to examine all: Print all the files examined in load order (highest priority last). \"\"\" if all : for p in Config () . load_path ( config ): print ( str ( p )) else : print ( str ( Config () . load_path ( config )[ - 1 ]))","title":"file()"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.list_classes","text":"Prints a list of registered configuration classes Source code in simple_uam/util/config/tasks.py 38 39 40 41 42 43 44 @task def list_classes ( ctx ): \"\"\" Prints a list of registered configuration classes \"\"\" for n in Config . config_names (): print ( n )","title":"list_classes()"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.list_files","text":"Prints a list of file locations (relative to config_dir) for config classes. Source code in simple_uam/util/config/tasks.py 54 55 56 57 58 59 60 @task def list_files ( ctx ): \"\"\" Prints a list of file locations (relative to config_dir) for config classes. \"\"\" for f in Config . config_files (): print ( f )","title":"list_files()"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.list_keys","text":"Prints a list of interpolation keys for available config classes Source code in simple_uam/util/config/tasks.py 46 47 48 49 50 51 52 @task def list_keys ( ctx ): \"\"\" Prints a list of interpolation keys for available config classes \"\"\" for k in Config . config_keys (): print ( k )","title":"list_keys()"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.print_config","text":"Prints the currently loaded config data for a given class to STDOUT Parameters: Name Type Description Default config The class name, interpolation key, or config file name of the config to print out. Can be given multiple times. required resolve Should we resolve all the interpolations before printing? required all Print all the configs, mutually exclusive with key. False Source code in simple_uam/util/config/tasks.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @task ( name = \"print\" , iterable = [ 'config' ]) def print_config ( ctx , config , resolved = False , all = False ): \"\"\" Prints the currently loaded config data for a given class to STDOUT Arguments: config: The class name, interpolation key, or config file name of the config to print out. Can be given multiple times. resolve: Should we resolve all the interpolations before printing? all: Print all the configs, mutually exclusive with key. \"\"\" keys = config if len ( keys ) == 0 and all : keys = [ str ( k ) for k in Config () . file_map . keys ()] elif len ( keys ) == 0 : raise RuntimeError ( \"Please specify keys to print or '--all'\" ) for k in keys : config_class = Config () . _get_config_class ( k ) config_data = Config () . config_types [ config_class ] filename = str ( config_data . conf_file ) print ( f \"### { filename } ### \\n \" ) # Print here to help w/ debug conf = config_data . config if resolved : with read_write ( conf ): OmegaConf . resolve ( conf ) print ( OmegaConf . to_yaml ( conf ))","title":"print_config()"},{"location":"reference/simple_uam/util/config/tasks/#simple_uam.util.config.tasks.write","text":"Writes the current configuration out to file in the appropriate location. Parameters: Name Type Description Default config default=all configs The identifier for the configuration to be printed out. This argument can be provided multiple times to specify multiple config files. None mkdir default=True Do we create the config directories if needed. True write_all default=False Do we write the config files for all possible modes? Otherwise, only write the file for the current run-mode. False overwrite default=False Do we overwrite existing files (creating backups as needed)? Otherwise, skip existing config files. False comment default=True Should the configs be written as a block comment? Otherwise, write the raw yaml directly. True output default=None The directory to write the output files to. None Source code in simple_uam/util/config/tasks.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 @task ( # positional=[\"config\"], iterable = [ \"config\" ], ) def write ( ctx , config = None , mkdir = True , write_all = False , overwrite = False , comment = True , output = None ): \"\"\" Writes the current configuration out to file in the appropriate location. Arguments: config (default=all configs): The identifier for the configuration to be printed out. This argument can be provided multiple times to specify multiple config files. mkdir (default=True): Do we create the config directories if needed. write_all (default=False): Do we write the config files for all possible modes? Otherwise, only write the file for the current run-mode. overwrite (default=False): Do we overwrite existing files (creating backups as needed)? Otherwise, skip existing config files. comment (default=True): Should the configs be written as a block comment? Otherwise, write the raw yaml directly. output (default=None): The directory to write the output files to. \"\"\" # Normalize arg for call if config == []: config = None Config . write_configs ( config , mkdir = mkdir , write_all = write_all , overwrite = overwrite , comment = comment , out_dir = output , )","title":"write()"},{"location":"reference/simple_uam/util/config/win_setup_config/","text":"WinSetupConfig \u00b6 A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/win_setup_config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @define class WinSetupConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' , ] \"\"\" Packages that are needed for all windows nodes, please don't remove any from this default set. \"\"\" broker_dep_packages : List [ str ] = [ 'rabbitmq' , ] \"\"\" Chocolatey packages needed for a windows broker node. \"\"\" worker_dep_packages : List [ str ] = [ 'openjdk11' , # 'openmodelica', 'rsync' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows worker node. \"\"\" worker_pip_packages : List [ str ] = [ # Direct2Cad 'psutil' , # 'creopyson', # Need to use the creopyson from the swri repo 'numpy' , # Craidl # 'gremlinpython', # Handling this internally now ] \"\"\" Pip packages needed for a windows worker node. \"\"\" license_dep_packages : List [ str ] = [ ] \"\"\" Chocolatey packages needed for a windows license server node. \"\"\" graph_dep_packages : List [ str ] = [ 'openjdk11' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows graph server node. \"\"\" qol_packages : List [ str ] = [ 'firefox' , 'notepadplusplus' , 'foxitreader' , 'tess' , 'freecad' , ] \"\"\" Quality of life packages that make actually using a windows node bearable. \"\"\" broker_dep_packages : List [ str ] = [ 'rabbitmq' ] class-attribute \u00b6 Chocolatey packages needed for a windows broker node. global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' ] class-attribute \u00b6 Packages that are needed for all windows nodes, please don't remove any from this default set. graph_dep_packages : List [ str ] = [ 'openjdk11' , 'nssm' ] class-attribute \u00b6 Chocolatey packages needed for a windows graph server node. license_dep_packages : List [ str ] = [] class-attribute \u00b6 Chocolatey packages needed for a windows license server node. qol_packages : List [ str ] = [ 'firefox' , 'notepadplusplus' , 'foxitreader' , 'tess' , 'freecad' ] class-attribute \u00b6 Quality of life packages that make actually using a windows node bearable. worker_dep_packages : List [ str ] = [ 'openjdk11' , 'rsync' , 'nssm' ] class-attribute \u00b6 Chocolatey packages needed for a windows worker node. worker_pip_packages : List [ str ] = [ 'psutil' , 'numpy' ] class-attribute \u00b6 Pip packages needed for a windows worker node.","title":"win_setup_config"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig","text":"A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html Source code in simple_uam/util/config/win_setup_config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @define class WinSetupConfig (): \"\"\" A dataclass carrying configuration fields and defaults. As described here: https://omegaconf.readthedocs.io/en/2.1_branch/structured_config.html \"\"\" global_dep_packages : List [ str ] = [ 'checksum' , 'wget' , '7zip' , ] \"\"\" Packages that are needed for all windows nodes, please don't remove any from this default set. \"\"\" broker_dep_packages : List [ str ] = [ 'rabbitmq' , ] \"\"\" Chocolatey packages needed for a windows broker node. \"\"\" worker_dep_packages : List [ str ] = [ 'openjdk11' , # 'openmodelica', 'rsync' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows worker node. \"\"\" worker_pip_packages : List [ str ] = [ # Direct2Cad 'psutil' , # 'creopyson', # Need to use the creopyson from the swri repo 'numpy' , # Craidl # 'gremlinpython', # Handling this internally now ] \"\"\" Pip packages needed for a windows worker node. \"\"\" license_dep_packages : List [ str ] = [ ] \"\"\" Chocolatey packages needed for a windows license server node. \"\"\" graph_dep_packages : List [ str ] = [ 'openjdk11' , 'nssm' , ] \"\"\" Chocolatey packages needed for a windows graph server node. \"\"\" qol_packages : List [ str ] = [ 'firefox' , 'notepadplusplus' , 'foxitreader' , 'tess' , 'freecad' , ] \"\"\" Quality of life packages that make actually using a windows node bearable. \"\"\"","title":"WinSetupConfig"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.broker_dep_packages","text":"Chocolatey packages needed for a windows broker node.","title":"broker_dep_packages"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.global_dep_packages","text":"Packages that are needed for all windows nodes, please don't remove any from this default set.","title":"global_dep_packages"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.graph_dep_packages","text":"Chocolatey packages needed for a windows graph server node.","title":"graph_dep_packages"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.license_dep_packages","text":"Chocolatey packages needed for a windows license server node.","title":"license_dep_packages"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.qol_packages","text":"Quality of life packages that make actually using a windows node bearable.","title":"qol_packages"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.worker_dep_packages","text":"Chocolatey packages needed for a windows worker node.","title":"worker_dep_packages"},{"location":"reference/simple_uam/util/config/win_setup_config/#simple_uam.util.config.win_setup_config.WinSetupConfig.worker_pip_packages","text":"Pip packages needed for a windows worker node.","title":"worker_pip_packages"},{"location":"reference/simple_uam/util/config/workspace_config/","text":"ResultsConfig \u00b6 Dataclass for options concerning the generation of transaction results. Source code in simple_uam/util/config/workspace_config.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @define class ResultsConfig (): \"\"\" Dataclass for options concerning the generation of transaction results. \"\"\" max_count : int = - 1 \"\"\" Number of transactio results to save, with the oldest deleted first. Negative values mean transactions will never be deleted. Zero means that transactions won't be saved at all (and produced only lazily) \"\"\" min_staletime : int = 60 * 60 # 1hr in seconds \"\"\" Number of seconds to keep a transaction result after its last access. Lots of non-stale results can lead to keeping more than max_count. \"\"\" metadata_file : str = \"metadata.json\" \"\"\" The file within each result that stores metadata. \"\"\" log_file : str = \"log.json\" \"\"\" The file which stores log information. \"\"\" log_file : str = 'log.json' class-attribute \u00b6 The file which stores log information. max_count : int = - 1 class-attribute \u00b6 Number of transactio results to save, with the oldest deleted first. Negative values mean transactions will never be deleted. Zero means that transactions won't be saved at all (and produced only lazily) metadata_file : str = 'metadata.json' class-attribute \u00b6 The file within each result that stores metadata. min_staletime : int = 60 * 60 class-attribute \u00b6 Number of seconds to keep a transaction result after its last access. Lots of non-stale results can lead to keeping more than max_count. WorkspaceConfig \u00b6 Source code in simple_uam/util/config/workspace_config.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 @define class WorkspaceConfig (): workspaces_dir : str = field () \"\"\" Directory containing all the workspaces \"\"\" workspace_subdir_pattern : str = \"workspace_ {} \" \"\"\" The pattern for the subdirectory for each individual workspace, the {} will be replaced by a number. \"\"\" reference_subdir : str = \"reference_workspace\" \"\"\" The subdirectory which contains the reference, unmodified copy of the workspace. \"\"\" assets_subdir : str = \"assets\" \"\"\" The subdirectory which contains various static files for use in the workspace, usually by symlinking. \"\"\" locks_subdir : str = \"workspace_locks\" \"\"\" Subdir of workspaces_dir where the various lockfiles are kept. \"\"\" results_dir : str = \"$ {workspaces_dir} /results\" \"\"\" Dir for cached transaction results. \"\"\" results : ResultsConfig = ResultsConfig () \"\"\" Options concerning results. \"\"\" max_workspaces : int = 4 \"\"\" The maximum number of workspaces operating simultaneously \"\"\" exclude : List [ str ] = [ '.git' ] \"\"\" File patterns to not copy from the reference dir to each workspace. Effectively specified relative to the reference dir. See rsync's '--exclude' argument for more info. \"\"\" result_exclude : List [ str ] = [ '.git' ] \"\"\" File patterns to not include in a session's result archive. Effectively specified relative to the current workspace dir, as such all patterns should be relative. See rsync's '--exclude' argument for more info. \"\"\" @property def workspaces_path ( self ): \"\"\" Path form of Workspaces_dir. \"\"\" return Path ( self . workspaces_dir ) . resolve () @property def locks_path ( self ): \"\"\" Absolute form of lockfile directory. \"\"\" if Path ( self . locks_subdir ) . is_absolute (): return Path ( self . locks_subdir ) . resolve () else : return Path ( self . workspaces_path / self . locks_subdir ) . resolve () @property def reference_path ( self ): \"\"\" Absolute form of reference workspace directory. \"\"\" if Path ( self . reference_subdir ) . is_absolute (): return Path ( self . reference_subdir ) . resolve () else : return Path ( self . workspaces_path / self . reference_subdir ) . resolve () @property def reference_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . locks_path / \"reference.lock\" @property def assets_path ( self ): \"\"\" Absolute form of workspace asset directory. Note: The asset subdir should share the reference lock. \"\"\" if Path ( self . assets_subdir ) . is_absolute (): return Path ( self . assets_subdir ) . resolve () else : return Path ( self . workspaces_path / self . assets_subdir ) . resolve () @property def results_path ( self ): \"\"\" Absolute form of results directory. \"\"\" if Path ( self . results_dir ) . is_absolute (): return Path ( self . results_dir ) . resolve () else : return Path ( self . workspaces_path / self . results_dir ) . resolve () @property def results_lockdir ( self ): \"\"\" The lockfile for the reference directory. Unlike the other lockfiles this is kept in the results directory. This allows multiple computers to share a results dir by, for instance, keeping it on a shared drive. \"\"\" return self . results_path / 'locks' @property def results_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . results_lockdir / \"results.lock\" def validate_workspace_subdir_num ( self , num : int ): \"\"\" Validate whether a particular worker subdir can exist. \"\"\" if num < 0 : err = RuntimeError ( f \"Workspace number { num } too low.\" ) log . exception ( \"Workspace num must be 1 or greater.\" , num = num , err = err , ) raise err elif num >= self . max_workspaces : err = RuntimeError ( f \"Workspace number { num } too high.\" ) log . exception ( \"Workspace num higher than max_workspaces\" , max_workspaces = Config [ UAMWorkspaceConfig ] . max_workspaces , num = num , err = err , ) raise err def workspace_subdir ( self , num : int ) -> str : \"\"\" Get the subdir name for a particular workspace. \"\"\" self . validate_workspace_subdir_num ( num ) return self . workspace_subdir_pattern . format ( num ) def workspace_path ( self , num : int ) -> Path : \"\"\" Get the full dirpath for a particular workspace. \"\"\" return ( self . workspaces_path / self . workspace_subdir ( num )) . resolve () def workspace_lockfile ( self , num : int ) -> Path : \"\"\" Get the lockfile for a particular workspace. \"\"\" return self . locks_path / f \" { self . workspace_subdir ( num ) } .lock\" @property def workspace_nums ( self ) -> List [ int ]: \"\"\" List of all workspace numbers. \"\"\" return range ( 0 , self . max_workspaces ) @property def workspace_subdirs ( self ) -> List [ str ]: \"\"\" List of all workspace subdir names. \"\"\" return [ self . workspace_subdir ( n ) for n in self . workspace_nums ] @property def workspace_paths ( self ) -> List [ Path ]: \"\"\" List of all workspace dirs. \"\"\" return [ self . workspace_path ( n ) for n in self . workspace_nums ] @property def workspace_lockfiles ( self ) -> List [ Path ]: \"\"\" List of all workspace lockfiles. \"\"\" return [ self . workspace_lockfile ( n ) for n in self . workspace_nums ] @property def exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths @property def result_exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths assets_subdir : str = 'assets' class-attribute \u00b6 The subdirectory which contains various static files for use in the workspace, usually by symlinking. exclude : List [ str ] = [ '.git' ] class-attribute \u00b6 File patterns to not copy from the reference dir to each workspace. Effectively specified relative to the reference dir. See rsync's '--exclude' argument for more info. locks_subdir : str = 'workspace_locks' class-attribute \u00b6 Subdir of workspaces_dir where the various lockfiles are kept. max_workspaces : int = 4 class-attribute \u00b6 The maximum number of workspaces operating simultaneously reference_subdir : str = 'reference_workspace' class-attribute \u00b6 The subdirectory which contains the reference, unmodified copy of the workspace. result_exclude : List [ str ] = [ '.git' ] class-attribute \u00b6 File patterns to not include in a session's result archive. Effectively specified relative to the current workspace dir, as such all patterns should be relative. See rsync's '--exclude' argument for more info. results : ResultsConfig = ResultsConfig () class-attribute \u00b6 Options concerning results. results_dir : str = '$ {workspaces_dir} /results' class-attribute \u00b6 Dir for cached transaction results. workspace_subdir_pattern : str = 'workspace_ {} ' class-attribute \u00b6 The pattern for the subdirectory for each individual workspace, the {} will be replaced by a number. workspaces_dir : str = field () class-attribute \u00b6 Directory containing all the workspaces assets_path () property \u00b6 Absolute form of workspace asset directory. Note: The asset subdir should share the reference lock. Source code in simple_uam/util/config/workspace_config.py 117 118 119 120 121 122 123 124 125 126 127 128 @property def assets_path ( self ): \"\"\" Absolute form of workspace asset directory. Note: The asset subdir should share the reference lock. \"\"\" if Path ( self . assets_subdir ) . is_absolute (): return Path ( self . assets_subdir ) . resolve () else : return Path ( self . workspaces_path / self . assets_subdir ) . resolve () exclude_from_paths () property \u00b6 self.exclude_from with all absolute Paths. Source code in simple_uam/util/config/workspace_config.py 217 218 219 220 221 222 223 224 225 226 227 228 @property def exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths locks_path () property \u00b6 Absolute form of lockfile directory. Source code in simple_uam/util/config/workspace_config.py 93 94 95 96 97 98 99 100 @property def locks_path ( self ): \"\"\" Absolute form of lockfile directory. \"\"\" if Path ( self . locks_subdir ) . is_absolute (): return Path ( self . locks_subdir ) . resolve () else : return Path ( self . workspaces_path / self . locks_subdir ) . resolve () reference_lockfile () property \u00b6 The lockfile for the reference directory. Source code in simple_uam/util/config/workspace_config.py 111 112 113 114 115 @property def reference_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . locks_path / \"reference.lock\" reference_path () property \u00b6 Absolute form of reference workspace directory. Source code in simple_uam/util/config/workspace_config.py 102 103 104 105 106 107 108 109 @property def reference_path ( self ): \"\"\" Absolute form of reference workspace directory. \"\"\" if Path ( self . reference_subdir ) . is_absolute (): return Path ( self . reference_subdir ) . resolve () else : return Path ( self . workspaces_path / self . reference_subdir ) . resolve () result_exclude_from_paths () property \u00b6 self.exclude_from with all absolute Paths. Source code in simple_uam/util/config/workspace_config.py 230 231 232 233 234 235 236 237 238 239 240 241 @property def result_exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths results_lockdir () property \u00b6 The lockfile for the reference directory. Unlike the other lockfiles this is kept in the results directory. This allows multiple computers to share a results dir by, for instance, keeping it on a shared drive. Source code in simple_uam/util/config/workspace_config.py 139 140 141 142 143 144 145 146 147 148 149 @property def results_lockdir ( self ): \"\"\" The lockfile for the reference directory. Unlike the other lockfiles this is kept in the results directory. This allows multiple computers to share a results dir by, for instance, keeping it on a shared drive. \"\"\" return self . results_path / 'locks' results_lockfile () property \u00b6 The lockfile for the reference directory. Source code in simple_uam/util/config/workspace_config.py 151 152 153 154 155 @property def results_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . results_lockdir / \"results.lock\" results_path () property \u00b6 Absolute form of results directory. Source code in simple_uam/util/config/workspace_config.py 130 131 132 133 134 135 136 137 @property def results_path ( self ): \"\"\" Absolute form of results directory. \"\"\" if Path ( self . results_dir ) . is_absolute (): return Path ( self . results_dir ) . resolve () else : return Path ( self . workspaces_path / self . results_dir ) . resolve () validate_workspace_subdir_num ( num ) \u00b6 Validate whether a particular worker subdir can exist. Source code in simple_uam/util/config/workspace_config.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def validate_workspace_subdir_num ( self , num : int ): \"\"\" Validate whether a particular worker subdir can exist. \"\"\" if num < 0 : err = RuntimeError ( f \"Workspace number { num } too low.\" ) log . exception ( \"Workspace num must be 1 or greater.\" , num = num , err = err , ) raise err elif num >= self . max_workspaces : err = RuntimeError ( f \"Workspace number { num } too high.\" ) log . exception ( \"Workspace num higher than max_workspaces\" , max_workspaces = Config [ UAMWorkspaceConfig ] . max_workspaces , num = num , err = err , ) raise err workspace_lockfile ( num ) \u00b6 Get the lockfile for a particular workspace. Source code in simple_uam/util/config/workspace_config.py 189 190 191 192 def workspace_lockfile ( self , num : int ) -> Path : \"\"\" Get the lockfile for a particular workspace. \"\"\" return self . locks_path / f \" { self . workspace_subdir ( num ) } .lock\" workspace_lockfiles () property \u00b6 List of all workspace lockfiles. Source code in simple_uam/util/config/workspace_config.py 211 212 213 214 215 @property def workspace_lockfiles ( self ) -> List [ Path ]: \"\"\" List of all workspace lockfiles. \"\"\" return [ self . workspace_lockfile ( n ) for n in self . workspace_nums ] workspace_nums () property \u00b6 List of all workspace numbers. Source code in simple_uam/util/config/workspace_config.py 194 195 196 197 198 @property def workspace_nums ( self ) -> List [ int ]: \"\"\" List of all workspace numbers. \"\"\" return range ( 0 , self . max_workspaces ) workspace_path ( num ) \u00b6 Get the full dirpath for a particular workspace. Source code in simple_uam/util/config/workspace_config.py 184 185 186 187 def workspace_path ( self , num : int ) -> Path : \"\"\" Get the full dirpath for a particular workspace. \"\"\" return ( self . workspaces_path / self . workspace_subdir ( num )) . resolve () workspace_paths () property \u00b6 List of all workspace dirs. Source code in simple_uam/util/config/workspace_config.py 206 207 208 209 @property def workspace_paths ( self ) -> List [ Path ]: \"\"\" List of all workspace dirs. \"\"\" return [ self . workspace_path ( n ) for n in self . workspace_nums ] workspace_subdir ( num ) \u00b6 Get the subdir name for a particular workspace. Source code in simple_uam/util/config/workspace_config.py 178 179 180 181 182 def workspace_subdir ( self , num : int ) -> str : \"\"\" Get the subdir name for a particular workspace. \"\"\" self . validate_workspace_subdir_num ( num ) return self . workspace_subdir_pattern . format ( num ) workspace_subdirs () property \u00b6 List of all workspace subdir names. Source code in simple_uam/util/config/workspace_config.py 200 201 202 203 204 @property def workspace_subdirs ( self ) -> List [ str ]: \"\"\" List of all workspace subdir names. \"\"\" return [ self . workspace_subdir ( n ) for n in self . workspace_nums ] workspaces_path () property \u00b6 Path form of Workspaces_dir. Source code in simple_uam/util/config/workspace_config.py 88 89 90 91 @property def workspaces_path ( self ): \"\"\" Path form of Workspaces_dir. \"\"\" return Path ( self . workspaces_dir ) . resolve ()","title":"workspace_config"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.ResultsConfig","text":"Dataclass for options concerning the generation of transaction results. Source code in simple_uam/util/config/workspace_config.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @define class ResultsConfig (): \"\"\" Dataclass for options concerning the generation of transaction results. \"\"\" max_count : int = - 1 \"\"\" Number of transactio results to save, with the oldest deleted first. Negative values mean transactions will never be deleted. Zero means that transactions won't be saved at all (and produced only lazily) \"\"\" min_staletime : int = 60 * 60 # 1hr in seconds \"\"\" Number of seconds to keep a transaction result after its last access. Lots of non-stale results can lead to keeping more than max_count. \"\"\" metadata_file : str = \"metadata.json\" \"\"\" The file within each result that stores metadata. \"\"\" log_file : str = \"log.json\" \"\"\" The file which stores log information. \"\"\"","title":"ResultsConfig"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.ResultsConfig.log_file","text":"The file which stores log information.","title":"log_file"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.ResultsConfig.max_count","text":"Number of transactio results to save, with the oldest deleted first. Negative values mean transactions will never be deleted. Zero means that transactions won't be saved at all (and produced only lazily)","title":"max_count"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.ResultsConfig.metadata_file","text":"The file within each result that stores metadata.","title":"metadata_file"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.ResultsConfig.min_staletime","text":"Number of seconds to keep a transaction result after its last access. Lots of non-stale results can lead to keeping more than max_count.","title":"min_staletime"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig","text":"Source code in simple_uam/util/config/workspace_config.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 @define class WorkspaceConfig (): workspaces_dir : str = field () \"\"\" Directory containing all the workspaces \"\"\" workspace_subdir_pattern : str = \"workspace_ {} \" \"\"\" The pattern for the subdirectory for each individual workspace, the {} will be replaced by a number. \"\"\" reference_subdir : str = \"reference_workspace\" \"\"\" The subdirectory which contains the reference, unmodified copy of the workspace. \"\"\" assets_subdir : str = \"assets\" \"\"\" The subdirectory which contains various static files for use in the workspace, usually by symlinking. \"\"\" locks_subdir : str = \"workspace_locks\" \"\"\" Subdir of workspaces_dir where the various lockfiles are kept. \"\"\" results_dir : str = \"$ {workspaces_dir} /results\" \"\"\" Dir for cached transaction results. \"\"\" results : ResultsConfig = ResultsConfig () \"\"\" Options concerning results. \"\"\" max_workspaces : int = 4 \"\"\" The maximum number of workspaces operating simultaneously \"\"\" exclude : List [ str ] = [ '.git' ] \"\"\" File patterns to not copy from the reference dir to each workspace. Effectively specified relative to the reference dir. See rsync's '--exclude' argument for more info. \"\"\" result_exclude : List [ str ] = [ '.git' ] \"\"\" File patterns to not include in a session's result archive. Effectively specified relative to the current workspace dir, as such all patterns should be relative. See rsync's '--exclude' argument for more info. \"\"\" @property def workspaces_path ( self ): \"\"\" Path form of Workspaces_dir. \"\"\" return Path ( self . workspaces_dir ) . resolve () @property def locks_path ( self ): \"\"\" Absolute form of lockfile directory. \"\"\" if Path ( self . locks_subdir ) . is_absolute (): return Path ( self . locks_subdir ) . resolve () else : return Path ( self . workspaces_path / self . locks_subdir ) . resolve () @property def reference_path ( self ): \"\"\" Absolute form of reference workspace directory. \"\"\" if Path ( self . reference_subdir ) . is_absolute (): return Path ( self . reference_subdir ) . resolve () else : return Path ( self . workspaces_path / self . reference_subdir ) . resolve () @property def reference_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . locks_path / \"reference.lock\" @property def assets_path ( self ): \"\"\" Absolute form of workspace asset directory. Note: The asset subdir should share the reference lock. \"\"\" if Path ( self . assets_subdir ) . is_absolute (): return Path ( self . assets_subdir ) . resolve () else : return Path ( self . workspaces_path / self . assets_subdir ) . resolve () @property def results_path ( self ): \"\"\" Absolute form of results directory. \"\"\" if Path ( self . results_dir ) . is_absolute (): return Path ( self . results_dir ) . resolve () else : return Path ( self . workspaces_path / self . results_dir ) . resolve () @property def results_lockdir ( self ): \"\"\" The lockfile for the reference directory. Unlike the other lockfiles this is kept in the results directory. This allows multiple computers to share a results dir by, for instance, keeping it on a shared drive. \"\"\" return self . results_path / 'locks' @property def results_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . results_lockdir / \"results.lock\" def validate_workspace_subdir_num ( self , num : int ): \"\"\" Validate whether a particular worker subdir can exist. \"\"\" if num < 0 : err = RuntimeError ( f \"Workspace number { num } too low.\" ) log . exception ( \"Workspace num must be 1 or greater.\" , num = num , err = err , ) raise err elif num >= self . max_workspaces : err = RuntimeError ( f \"Workspace number { num } too high.\" ) log . exception ( \"Workspace num higher than max_workspaces\" , max_workspaces = Config [ UAMWorkspaceConfig ] . max_workspaces , num = num , err = err , ) raise err def workspace_subdir ( self , num : int ) -> str : \"\"\" Get the subdir name for a particular workspace. \"\"\" self . validate_workspace_subdir_num ( num ) return self . workspace_subdir_pattern . format ( num ) def workspace_path ( self , num : int ) -> Path : \"\"\" Get the full dirpath for a particular workspace. \"\"\" return ( self . workspaces_path / self . workspace_subdir ( num )) . resolve () def workspace_lockfile ( self , num : int ) -> Path : \"\"\" Get the lockfile for a particular workspace. \"\"\" return self . locks_path / f \" { self . workspace_subdir ( num ) } .lock\" @property def workspace_nums ( self ) -> List [ int ]: \"\"\" List of all workspace numbers. \"\"\" return range ( 0 , self . max_workspaces ) @property def workspace_subdirs ( self ) -> List [ str ]: \"\"\" List of all workspace subdir names. \"\"\" return [ self . workspace_subdir ( n ) for n in self . workspace_nums ] @property def workspace_paths ( self ) -> List [ Path ]: \"\"\" List of all workspace dirs. \"\"\" return [ self . workspace_path ( n ) for n in self . workspace_nums ] @property def workspace_lockfiles ( self ) -> List [ Path ]: \"\"\" List of all workspace lockfiles. \"\"\" return [ self . workspace_lockfile ( n ) for n in self . workspace_nums ] @property def exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths @property def result_exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths","title":"WorkspaceConfig"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.assets_subdir","text":"The subdirectory which contains various static files for use in the workspace, usually by symlinking.","title":"assets_subdir"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.exclude","text":"File patterns to not copy from the reference dir to each workspace. Effectively specified relative to the reference dir. See rsync's '--exclude' argument for more info.","title":"exclude"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.locks_subdir","text":"Subdir of workspaces_dir where the various lockfiles are kept.","title":"locks_subdir"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.max_workspaces","text":"The maximum number of workspaces operating simultaneously","title":"max_workspaces"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.reference_subdir","text":"The subdirectory which contains the reference, unmodified copy of the workspace.","title":"reference_subdir"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.result_exclude","text":"File patterns to not include in a session's result archive. Effectively specified relative to the current workspace dir, as such all patterns should be relative. See rsync's '--exclude' argument for more info.","title":"result_exclude"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.results","text":"Options concerning results.","title":"results"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.results_dir","text":"Dir for cached transaction results.","title":"results_dir"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_subdir_pattern","text":"The pattern for the subdirectory for each individual workspace, the {} will be replaced by a number.","title":"workspace_subdir_pattern"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspaces_dir","text":"Directory containing all the workspaces","title":"workspaces_dir"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.assets_path","text":"Absolute form of workspace asset directory. Note: The asset subdir should share the reference lock. Source code in simple_uam/util/config/workspace_config.py 117 118 119 120 121 122 123 124 125 126 127 128 @property def assets_path ( self ): \"\"\" Absolute form of workspace asset directory. Note: The asset subdir should share the reference lock. \"\"\" if Path ( self . assets_subdir ) . is_absolute (): return Path ( self . assets_subdir ) . resolve () else : return Path ( self . workspaces_path / self . assets_subdir ) . resolve ()","title":"assets_path()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.exclude_from_paths","text":"self.exclude_from with all absolute Paths. Source code in simple_uam/util/config/workspace_config.py 217 218 219 220 221 222 223 224 225 226 227 228 @property def exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths","title":"exclude_from_paths()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.locks_path","text":"Absolute form of lockfile directory. Source code in simple_uam/util/config/workspace_config.py 93 94 95 96 97 98 99 100 @property def locks_path ( self ): \"\"\" Absolute form of lockfile directory. \"\"\" if Path ( self . locks_subdir ) . is_absolute (): return Path ( self . locks_subdir ) . resolve () else : return Path ( self . workspaces_path / self . locks_subdir ) . resolve ()","title":"locks_path()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.reference_lockfile","text":"The lockfile for the reference directory. Source code in simple_uam/util/config/workspace_config.py 111 112 113 114 115 @property def reference_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . locks_path / \"reference.lock\"","title":"reference_lockfile()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.reference_path","text":"Absolute form of reference workspace directory. Source code in simple_uam/util/config/workspace_config.py 102 103 104 105 106 107 108 109 @property def reference_path ( self ): \"\"\" Absolute form of reference workspace directory. \"\"\" if Path ( self . reference_subdir ) . is_absolute (): return Path ( self . reference_subdir ) . resolve () else : return Path ( self . workspaces_path / self . reference_subdir ) . resolve ()","title":"reference_path()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.result_exclude_from_paths","text":"self.exclude_from with all absolute Paths. Source code in simple_uam/util/config/workspace_config.py 230 231 232 233 234 235 236 237 238 239 240 241 @property def result_exclude_from_paths ( self ): \"\"\" self.exclude_from with all absolute Paths. \"\"\" paths = list () for f in self . exclude_from : f = Path ( f ) if not f . is_absolute (): f = self . reference_path / f paths . append ( f ) return paths","title":"result_exclude_from_paths()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.results_lockdir","text":"The lockfile for the reference directory. Unlike the other lockfiles this is kept in the results directory. This allows multiple computers to share a results dir by, for instance, keeping it on a shared drive. Source code in simple_uam/util/config/workspace_config.py 139 140 141 142 143 144 145 146 147 148 149 @property def results_lockdir ( self ): \"\"\" The lockfile for the reference directory. Unlike the other lockfiles this is kept in the results directory. This allows multiple computers to share a results dir by, for instance, keeping it on a shared drive. \"\"\" return self . results_path / 'locks'","title":"results_lockdir()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.results_lockfile","text":"The lockfile for the reference directory. Source code in simple_uam/util/config/workspace_config.py 151 152 153 154 155 @property def results_lockfile ( self ): \"\"\" The lockfile for the reference directory. \"\"\" return self . results_lockdir / \"results.lock\"","title":"results_lockfile()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.results_path","text":"Absolute form of results directory. Source code in simple_uam/util/config/workspace_config.py 130 131 132 133 134 135 136 137 @property def results_path ( self ): \"\"\" Absolute form of results directory. \"\"\" if Path ( self . results_dir ) . is_absolute (): return Path ( self . results_dir ) . resolve () else : return Path ( self . workspaces_path / self . results_dir ) . resolve ()","title":"results_path()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.validate_workspace_subdir_num","text":"Validate whether a particular worker subdir can exist. Source code in simple_uam/util/config/workspace_config.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def validate_workspace_subdir_num ( self , num : int ): \"\"\" Validate whether a particular worker subdir can exist. \"\"\" if num < 0 : err = RuntimeError ( f \"Workspace number { num } too low.\" ) log . exception ( \"Workspace num must be 1 or greater.\" , num = num , err = err , ) raise err elif num >= self . max_workspaces : err = RuntimeError ( f \"Workspace number { num } too high.\" ) log . exception ( \"Workspace num higher than max_workspaces\" , max_workspaces = Config [ UAMWorkspaceConfig ] . max_workspaces , num = num , err = err , ) raise err","title":"validate_workspace_subdir_num()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_lockfile","text":"Get the lockfile for a particular workspace. Source code in simple_uam/util/config/workspace_config.py 189 190 191 192 def workspace_lockfile ( self , num : int ) -> Path : \"\"\" Get the lockfile for a particular workspace. \"\"\" return self . locks_path / f \" { self . workspace_subdir ( num ) } .lock\"","title":"workspace_lockfile()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_lockfiles","text":"List of all workspace lockfiles. Source code in simple_uam/util/config/workspace_config.py 211 212 213 214 215 @property def workspace_lockfiles ( self ) -> List [ Path ]: \"\"\" List of all workspace lockfiles. \"\"\" return [ self . workspace_lockfile ( n ) for n in self . workspace_nums ]","title":"workspace_lockfiles()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_nums","text":"List of all workspace numbers. Source code in simple_uam/util/config/workspace_config.py 194 195 196 197 198 @property def workspace_nums ( self ) -> List [ int ]: \"\"\" List of all workspace numbers. \"\"\" return range ( 0 , self . max_workspaces )","title":"workspace_nums()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_path","text":"Get the full dirpath for a particular workspace. Source code in simple_uam/util/config/workspace_config.py 184 185 186 187 def workspace_path ( self , num : int ) -> Path : \"\"\" Get the full dirpath for a particular workspace. \"\"\" return ( self . workspaces_path / self . workspace_subdir ( num )) . resolve ()","title":"workspace_path()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_paths","text":"List of all workspace dirs. Source code in simple_uam/util/config/workspace_config.py 206 207 208 209 @property def workspace_paths ( self ) -> List [ Path ]: \"\"\" List of all workspace dirs. \"\"\" return [ self . workspace_path ( n ) for n in self . workspace_nums ]","title":"workspace_paths()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_subdir","text":"Get the subdir name for a particular workspace. Source code in simple_uam/util/config/workspace_config.py 178 179 180 181 182 def workspace_subdir ( self , num : int ) -> str : \"\"\" Get the subdir name for a particular workspace. \"\"\" self . validate_workspace_subdir_num ( num ) return self . workspace_subdir_pattern . format ( num )","title":"workspace_subdir()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspace_subdirs","text":"List of all workspace subdir names. Source code in simple_uam/util/config/workspace_config.py 200 201 202 203 204 @property def workspace_subdirs ( self ) -> List [ str ]: \"\"\" List of all workspace subdir names. \"\"\" return [ self . workspace_subdir ( n ) for n in self . workspace_nums ]","title":"workspace_subdirs()"},{"location":"reference/simple_uam/util/config/workspace_config/#simple_uam.util.config.workspace_config.WorkspaceConfig.workspaces_path","text":"Path form of Workspaces_dir. Source code in simple_uam/util/config/workspace_config.py 88 89 90 91 @property def workspaces_path ( self ): \"\"\" Path form of Workspaces_dir. \"\"\" return Path ( self . workspaces_dir ) . resolve ()","title":"workspaces_path()"},{"location":"reference/simple_uam/util/invoke/","text":"InvokeProg \u00b6 Bases: Program Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. Source code in simple_uam/util/invoke/program.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class InvokeProg ( Program ): \"\"\" Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. \"\"\" def core_args ( self ): return super ( InvokeProg , self ) . core_args () + Config . _invoke_args () def run ( self , argv = None , exit = True ): # Special case for how pdm seems to do things when calling as a script. # The args in the pyproject.toml are passed directly while command # line args are found in sys.args after a '-c' flag. if ( isinstance ( argv , list ) and argv [ 0 ] . startswith ( \"pdm run\" ) and sys . argv [ 0 ] == '-c' ): argv = argv + sys . argv [ 1 :] return super ( InvokeProg , self ) . run ( argv , exit )","title":"invoke"},{"location":"reference/simple_uam/util/invoke/#simple_uam.util.invoke.InvokeProg","text":"Bases: Program Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. Source code in simple_uam/util/invoke/program.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class InvokeProg ( Program ): \"\"\" Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. \"\"\" def core_args ( self ): return super ( InvokeProg , self ) . core_args () + Config . _invoke_args () def run ( self , argv = None , exit = True ): # Special case for how pdm seems to do things when calling as a script. # The args in the pyproject.toml are passed directly while command # line args are found in sys.args after a '-c' flag. if ( isinstance ( argv , list ) and argv [ 0 ] . startswith ( \"pdm run\" ) and sys . argv [ 0 ] == '-c' ): argv = argv + sys . argv [ 1 :] return super ( InvokeProg , self ) . run ( argv , exit )","title":"InvokeProg"},{"location":"reference/simple_uam/util/invoke/program/","text":"InvokeProg \u00b6 Bases: Program Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. Source code in simple_uam/util/invoke/program.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class InvokeProg ( Program ): \"\"\" Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. \"\"\" def core_args ( self ): return super ( InvokeProg , self ) . core_args () + Config . _invoke_args () def run ( self , argv = None , exit = True ): # Special case for how pdm seems to do things when calling as a script. # The args in the pyproject.toml are passed directly while command # line args are found in sys.args after a '-c' flag. if ( isinstance ( argv , list ) and argv [ 0 ] . startswith ( \"pdm run\" ) and sys . argv [ 0 ] == '-c' ): argv = argv + sys . argv [ 1 :] return super ( InvokeProg , self ) . run ( argv , exit )","title":"program"},{"location":"reference/simple_uam/util/invoke/program/#simple_uam.util.invoke.program.InvokeProg","text":"Bases: Program Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. Source code in simple_uam/util/invoke/program.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class InvokeProg ( Program ): \"\"\" Can be used like Program from invoke, it just adds help text for the two arguments that the Config mechanism uses to extend the search path and add modes. Has some fixes that make executables easier to use as pdm tool scripts. \"\"\" def core_args ( self ): return super ( InvokeProg , self ) . core_args () + Config . _invoke_args () def run ( self , argv = None , exit = True ): # Special case for how pdm seems to do things when calling as a script. # The args in the pyproject.toml are passed directly while command # line args are found in sys.args after a '-c' flag. if ( isinstance ( argv , list ) and argv [ 0 ] . startswith ( \"pdm run\" ) and sys . argv [ 0 ] == '-c' ): argv = argv + sys . argv [ 1 :] return super ( InvokeProg , self ) . run ( argv , exit )","title":"InvokeProg"},{"location":"reference/simple_uam/util/logging/","text":"","title":"logging"},{"location":"reference/simple_uam/util/logging/logger/","text":"CustomRenderer \u00b6 Bases: ConsoleRenderer Wraps the console renderer to add a bit more whitespace and formatting. Source code in simple_uam/util/logging/logger.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class CustomRenderer ( ConsoleRenderer ): \"\"\" Wraps the console renderer to add a bit more whitespace and formatting. \"\"\" def __call__ ( self , logger : WrappedLogger , name : str , event_dict : EventDict ) -> str : sio = StringIO () prompt = \"\" ts = event_dict . pop ( \"timestamp\" , None ) if ts is not None : prompt += ( # can be a number if timestamp is UNIXy self . _styles . timestamp + str ( ts ) + self . _styles . reset + \" \" ) level = event_dict . pop ( \"level\" , None ) if level is not None : prompt += ( \"[\" + self . _level_to_color . get ( level , \"\" ) + _pad ( level , self . _longest_level ) + self . _styles . reset + \"] \" ) # force event to str for compatibility with standard library event = event_dict . pop ( \"event\" , None ) if not isinstance ( event , str ): event = str ( event ) event = try_dent ( event ) if multiline ( event ): prompt += _pad ( \"\" , self . _pad_event ) event = \" \\n\\n \" + self . _styles . bright + event event += self . _styles . reset else : prompt += self . _styles . bright prompt += _pad ( event , self . _pad_event ) prompt += self . _styles . reset event = None sio . write ( prompt ) logger_name = event_dict . pop ( \"logger\" , None ) if logger_name is None : logger_name = event_dict . pop ( \"logger_name\" , None ) if logger_name is not None : sio . write ( \"[\" + self . _styles . logger_name + self . _styles . bright + logger_name + self . _styles . reset + \"] \" ) sio . write ( event ) stack = event_dict . pop ( \"stack\" , None ) exc = event_dict . pop ( \"exception\" , None ) exc_info = event_dict . pop ( \"exc_info\" , None ) event_dict_keys : Iterable [ str ] = event_dict . keys () if self . _sort_keys : event_dict_keys = sorted ( event_dict_keys ) dict_vals = list () console = Console () for key in event_dict_keys : pretty_key = ( self . _styles . kv_key + key + self . _styles . reset + \"=\" ) with console . capture () as capture : console . print ( self . _repr ( event_dict [ key ])) pretty_val = try_dent ( capture . get ()) if multiline ( pretty_val ): dict_vals . append ( pretty_key + \" \\n \" + textwrap . indent ( pretty_val , \" \" ) ) else : dict_vals . append ( pretty_key + \" \" + pretty_val ) sio . write ( \" \\n\\n \" ) sio . write ( \" \\n\\n \" . join ( dict_vals )) if stack is not None : sio . write ( \" \\n \" + stack ) if exc_info or exc is not None : sio . write ( \" \\n\\n \" + \"=\" * 79 + \" \\n \" ) if exc_info : if not isinstance ( exc_info , tuple ): exc_info = sys . exc_info () self . _exception_formatter ( sio , exc_info ) elif exc is not None : if self . _exception_formatter is not plain_traceback : warnings . warn ( \"Remove `format_exc_info` from your processor chain \" \"if you want pretty exceptions.\" ) sio . write ( \" \\n \" + exc ) return sio . getvalue () try_dent ( inp , indent = ' ' , pad_len = None ) \u00b6 Will try to dedent and clean edges of input string, will indent if result is more than a single line. Source code in simple_uam/util/logging/logger.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def try_dent ( inp : str , indent : str = \" \" , pad_len : Optional [ int ] = None ) -> str : \"\"\" Will try to dedent and clean edges of input string, will indent if result is more than a single line. \"\"\" inp = strip_empty_lines ( inp ) inp = textwrap . dedent ( inp ) if multiline ( inp ): inp = textwrap . indent ( inp , indent ) if pad : inp = \" \\n \" . join ([ _pad ( i , pad_len ) for i in inp . splitlines ()]) return inp","title":"logger"},{"location":"reference/simple_uam/util/logging/logger/#simple_uam.util.logging.logger.CustomRenderer","text":"Bases: ConsoleRenderer Wraps the console renderer to add a bit more whitespace and formatting. Source code in simple_uam/util/logging/logger.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class CustomRenderer ( ConsoleRenderer ): \"\"\" Wraps the console renderer to add a bit more whitespace and formatting. \"\"\" def __call__ ( self , logger : WrappedLogger , name : str , event_dict : EventDict ) -> str : sio = StringIO () prompt = \"\" ts = event_dict . pop ( \"timestamp\" , None ) if ts is not None : prompt += ( # can be a number if timestamp is UNIXy self . _styles . timestamp + str ( ts ) + self . _styles . reset + \" \" ) level = event_dict . pop ( \"level\" , None ) if level is not None : prompt += ( \"[\" + self . _level_to_color . get ( level , \"\" ) + _pad ( level , self . _longest_level ) + self . _styles . reset + \"] \" ) # force event to str for compatibility with standard library event = event_dict . pop ( \"event\" , None ) if not isinstance ( event , str ): event = str ( event ) event = try_dent ( event ) if multiline ( event ): prompt += _pad ( \"\" , self . _pad_event ) event = \" \\n\\n \" + self . _styles . bright + event event += self . _styles . reset else : prompt += self . _styles . bright prompt += _pad ( event , self . _pad_event ) prompt += self . _styles . reset event = None sio . write ( prompt ) logger_name = event_dict . pop ( \"logger\" , None ) if logger_name is None : logger_name = event_dict . pop ( \"logger_name\" , None ) if logger_name is not None : sio . write ( \"[\" + self . _styles . logger_name + self . _styles . bright + logger_name + self . _styles . reset + \"] \" ) sio . write ( event ) stack = event_dict . pop ( \"stack\" , None ) exc = event_dict . pop ( \"exception\" , None ) exc_info = event_dict . pop ( \"exc_info\" , None ) event_dict_keys : Iterable [ str ] = event_dict . keys () if self . _sort_keys : event_dict_keys = sorted ( event_dict_keys ) dict_vals = list () console = Console () for key in event_dict_keys : pretty_key = ( self . _styles . kv_key + key + self . _styles . reset + \"=\" ) with console . capture () as capture : console . print ( self . _repr ( event_dict [ key ])) pretty_val = try_dent ( capture . get ()) if multiline ( pretty_val ): dict_vals . append ( pretty_key + \" \\n \" + textwrap . indent ( pretty_val , \" \" ) ) else : dict_vals . append ( pretty_key + \" \" + pretty_val ) sio . write ( \" \\n\\n \" ) sio . write ( \" \\n\\n \" . join ( dict_vals )) if stack is not None : sio . write ( \" \\n \" + stack ) if exc_info or exc is not None : sio . write ( \" \\n\\n \" + \"=\" * 79 + \" \\n \" ) if exc_info : if not isinstance ( exc_info , tuple ): exc_info = sys . exc_info () self . _exception_formatter ( sio , exc_info ) elif exc is not None : if self . _exception_formatter is not plain_traceback : warnings . warn ( \"Remove `format_exc_info` from your processor chain \" \"if you want pretty exceptions.\" ) sio . write ( \" \\n \" + exc ) return sio . getvalue ()","title":"CustomRenderer"},{"location":"reference/simple_uam/util/logging/logger/#simple_uam.util.logging.logger.try_dent","text":"Will try to dedent and clean edges of input string, will indent if result is more than a single line. Source code in simple_uam/util/logging/logger.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def try_dent ( inp : str , indent : str = \" \" , pad_len : Optional [ int ] = None ) -> str : \"\"\" Will try to dedent and clean edges of input string, will indent if result is more than a single line. \"\"\" inp = strip_empty_lines ( inp ) inp = textwrap . dedent ( inp ) if multiline ( inp ): inp = textwrap . indent ( inp , indent ) if pad : inp = \" \\n \" . join ([ _pad ( i , pad_len ) for i in inp . splitlines ()]) return inp","title":"try_dent()"},{"location":"reference/simple_uam/util/system/","text":"Git \u00b6 Static class used to wrap a bunch of git commands. Source code in simple_uam/util/system/git.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class Git (): \"\"\" Static class used to wrap a bunch of git commands. \"\"\" @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , ) clone_or_pull ( repo_uri , deploy_dir , branch = None , repo_name = None , remote_name = None , run_clone = True , run_pull = True , recursive = True , is_submodule = False , password_prompt = False , password_warning = True , remote_user = None , remote_pass = None , quiet = False , progress = True , verbose = False , mkdir = False ) staticmethod \u00b6 Central function that will run a git init/update operation. Parameters: Name Type Description Default repo_uri str The url of the core repo. required deploy_dir Union [ str , Path ] Directory that the repo should be unpacked into. required branch Optional [ str ] The branch we want to checkout or pull from. None repo_name Optional [ str ] Human readable repo name. None remote_name Optional [ str ] Human readable rempte name. None run_clone bool Should we clone a repo if none exists? (init if submodule) True run_pull bool Should we pull a repo if it already exists? (update if submodule) True recursive bool Should we recursively update/init submodules? True is_submodule bool Is this a submodule we're working with? False password_prompt bool Should we ask for a password if none is provided? False password_warning bool Should we warn the user if we're using a password on a clone? True remote_user Optional [ str ] The remote user. None remote_pass Optional [ str ] The remote user's password. None quiet bool Run silently. Implies not password_prompt . False progress bool Display a progress meter. True verbose bool verbose git output if possible. False mkdir bool create the parent directory of the repo if needed. False Source code in simple_uam/util/system/git.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , ) Pip \u00b6 Static class used to wrap a bunch of pip commands. Source code in simple_uam/util/system/pip.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Pip (): \"\"\" Static class used to wrap a bunch of pip commands. \"\"\" @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , ) install ( * packages , editable = False , requirements_file = None , upgrade = True , progress = True , quiet = False , verbose = False , python = 'python' , cwd = None ) staticmethod \u00b6 Central function that will run pip install. Parameters: Name Type Description Default *packages List [ str ] The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. () editable bool If installing from vcs or local dir, should this package be installed as editable. False requirements_file Union [ str , Path , None] The requirements file to extract a list of packages from. Mutually exclusive with packages. None upgrade bool Upgrade packages in place, mutually exclusive with upgrade. True progress bool Show the progress bar. True quiet bool Run silently False verbose bool Run w/ verbose output. False python Union [ str , Path ] Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. 'python' Source code in simple_uam/util/system/pip.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , ) Rsync \u00b6 Static class used to wrap a bunch of rsync commands. Source code in simple_uam/util/system/rsync.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 class Rsync (): \"\"\" Static class used to wrap a bunch of rsync commands. \"\"\" @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" ) @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" ) @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , ) @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , ) itemize_regex = re . compile ( r '^[*><\\.+a-zA-Z\\s] {11} \\s(.*)$' , re . MULTILINE ) \"\"\" Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/ \"\"\" @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out ) itemize_regex = re . compile ( '^[*>< \\\\ .+a-zA-Z \\\\ s] {11} \\\\ s(.*)$' , re . MULTILINE ) class-attribute \u00b6 Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/ archive_changes ( ref , src , out , exclude = [], exclude_from = []) staticmethod \u00b6 Package any files that are different in src (compared to ref) into a zip file at out. Parameters: Name Type Description Default ref Union [ str , Path ] The reference directory required src Union [ str , Path ] The modified directory required out Union [ str , Path ] The location of the output zip file required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] Source code in simple_uam/util/system/rsync.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out ) copy_dir ( src , dst , exclude = [], exclude_from = [], delete = True , update = False , progress = False , verbose = False , quiet = False ) classmethod \u00b6 Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quiet bool supress non-error output False Source code in simple_uam/util/system/rsync.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , ) list_changes ( ref , src , exclude = [], exclude_from = [], preserve_dirs = False , prune_missing = True ) classmethod \u00b6 Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Parameters: Name Type Description Default ref Union [ str , Path ] the reference directory required src Union [ str , Path ] the source dir, to check for changes required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] preserve_dirs bool preserve directories in the output file list. False prune_missing bool remove entries that aren't present in dst. True Source code in simple_uam/util/system/rsync.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes norm_path ( path , is_dir = None ) staticmethod \u00b6 Rsync on windows is weird about paths, this will format an input path appropriately. Parameters: Name Type Description Default path Union [ str , Path ] The Path to format. required is_dir Optional [ bool ] if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. None Returns: Type Description str An absolute path string suitable as a cmd line arg for rsync. Source code in simple_uam/util/system/rsync.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str require () staticmethod \u00b6 Raises an error if rsync isn't installed. Source code in simple_uam/util/system/rsync.py 29 30 31 32 33 34 35 36 37 @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" ) run ( src , dst , * , exclude = [], exclude_from = [], archive = True , delete = True , update = False , progress = False , verbose = False , quiet = False , dry_run = False , itemize_changes = False , capture_output = False , links = False , times = False , owner = False , group = False , perms = False , recursive = False ) classmethod \u00b6 Run rsync with args return the completed process object. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] archive bool Run in archive mode, True delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quite supress non-error messages. required dry_run bool make no changes False itemize_changes bool print a list of all changes to stdout False capture_output bool do we capture stdout? False links bool When symlinks are encountered, recreate the symlink on the destination. False recursive bool This tells rsync to copy directories recursively. False times bool preserve modification times. False owner bool preserve owner (super-user only) False group bool preserve group False perms bool preserve permissions False Source code in simple_uam/util/system/rsync.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , ) which () staticmethod \u00b6 Returns the rsync executable if it exists or None otherwise. Source code in simple_uam/util/system/rsync.py 21 22 23 24 25 26 27 @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" ) archive_files ( cwd , files , out , missing_ok = False ) \u00b6 Archives the files in files to out , preserving any structure files have relative to cwd . Note: All files must either be relative or in a subdir of cwd . Parameters: Name Type Description Default cwd Union [ str , Path ] Root locations from which we're gathering files. required files List [ Union [ str , Path ]] list of files to archive. required out Union [ str , Path ] Output zipfile location. required missing_ok bool Do we ignore files that don't exist? False Source code in simple_uam/util/system/backup.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def archive_files ( cwd : Union [ str , Path ], files : List [ Union [ str , Path ]], out : Union [ str , Path ], missing_ok : bool = False , ): \"\"\" Archives the files in `files` to `out`, preserving any structure `files` have relative to `cwd`. Note: All `files` must either be relative or in a subdir of `cwd`. Arguments: cwd: Root locations from which we're gathering files. files: list of files to archive. out: Output zipfile location. missing_ok: Do we ignore files that don't exist? \"\"\" cwd = Path ( cwd ) . resolve () out = Path ( out ) . resolve () def make_arc ( f : Path ) -> Path : \"\"\" Get the relative archive file path for a given file. \"\"\" if f . is_absolute () and f . is_relative_to ( cwd ): return f . relative_to ( cwd ) elif not f . is_absolute (): return f else : err = RuntimeError ( \"Invalid file location\" ) log . exception ( \"File not subdir of cwd.\" , file = str ( f ), cwd = str ( cwd ), err = err , ) raise err files = [ make_arc ( Path ( f )) for f in files ] # Iterate through the files to archive, adding them to the zip file as # needed. with ZipFile ( out , 'x' ) as zf : for arc_file in files : sys_file = cwd / arc_file if not sys_file . exists () and missing_ok : pass else : zf . write ( sys_file , arc_file ) backup_file ( file_path , backup_dir = None , delete = False , missing_ok = True ) \u00b6 Makes a backup copy of a file. Parameters: Name Type Description Default file_path Union [ Path , str ] The path to file. required backup_dir Union [ Path , str , None] The directory backups are placed in, default is the same dir as the file. None delete bool Do we delete the original? (default: False) False missing_ok bool Do we silently ignore if the file is missing? (default: True) True Source code in simple_uam/util/system/backup.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def backup_file ( file_path : Union [ Path , str ], backup_dir : Union [ Path , str , None ] = None , delete : bool = False , missing_ok : bool = True , ): \"\"\" Makes a backup copy of a file. Arguments: file_path: The path to file. backup_dir: The directory backups are placed in, default is the same dir as the file. delete: Do we delete the original? (default: False) missing_ok: Do we silently ignore if the file is missing? (default: True) \"\"\" file_path = Path ( file_path ) bak_file = file_path . with_name ( f \" { file_path . name } .bak\" ) if not backup_dir : backup_dir = file_path . parent backup_dir = Path ( backup_dir ) if file_path . exists (): count = 1 time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } .bak\" ) while bak_file . exists (): bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } . { count } .bak\" ) count += 1 backup_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Creating Backup File.\" , file = str ( file_path ), backup = str ( bak_file ), delete_original = delete , ) if delete : shutil . move ( file_path , bak_file ) else : shutil . copy2 ( file_path , bak_file ) return bak_file elif not missing_ok : raise RuntimeError ( f \"Cannot backup { str ( file_path ) } file does not exist.\" ) else : log . info ( \"No file to backup.\" , file = str ( file_path ), backup = str ( bak_file ), ) return None configure_file ( input_file , output_file , replacements = {}, exist_ok = False , backup = True , backup_dir = None ) \u00b6 Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Parameters: Name Type Description Default input_file Union [ str , Path ] The input_file to use. required output_file Union [ str , Path ] The output location to place the configured file. required replacements Dict [ str , str ] The string to find and replace in the input file. {} exist_ok bool If true will overwrite an existing file. False backup bool If overwriting a file do we create a backup? True backup_dir Union [ str , Path , None] Place backups in this directory, if not specified defaults to output directory. None Source code in simple_uam/util/system/backup.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def configure_file ( input_file : Union [ str , Path ], output_file : Union [ str , Path ], replacements : Dict [ str , str ] = {}, exist_ok : bool = False , backup : bool = True , backup_dir : Union [ str , Path , None ] = None ): \"\"\" Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Arguments: input_file: The input_file to use. output_file: The output location to place the configured file. replacements: The string to find and replace in the input file. exist_ok: If true will overwrite an existing file. backup: If overwriting a file do we create a backup? backup_dir: Place backups in this directory, if not specified defaults to output directory. \"\"\" input_file = Path ( input_file ) . resolve () output_file = Path ( output_file ) . resolve () # Verification if not exist_ok and output_file . exists (): err = RuntimeError ( \"Target file already exists.\" ) log . exception ( \"Cannot configure file because output already exists.\" , err = err , input_file = input_file , output_file = output_file , ) raise err # Backup if backup : backup_file ( output_file , backup_dir = backup_dir , ) log . info ( \"Reading configuration input.\" , input_file = str ( input_file )) with input_file . open ( 'r' ) as file : filedata = file . read () for find , replace in replacements . items (): log . info ( \"Performing configuration replacement.\" , find = find , replace = replace , ) filedata = filedata . replace ( find , replace ) # delete output here incase output == input output_file . unlink ( missing_ok = True ) log . info ( \"Writing configuration output.\" , output_file = str ( output_file )) with output_file . open ( 'w' ) as file : file . write ( filedata )","title":"system"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.Git","text":"Static class used to wrap a bunch of git commands. Source code in simple_uam/util/system/git.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class Git (): \"\"\" Static class used to wrap a bunch of git commands. \"\"\" @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , )","title":"Git"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.git.Git.clone_or_pull","text":"Central function that will run a git init/update operation. Parameters: Name Type Description Default repo_uri str The url of the core repo. required deploy_dir Union [ str , Path ] Directory that the repo should be unpacked into. required branch Optional [ str ] The branch we want to checkout or pull from. None repo_name Optional [ str ] Human readable repo name. None remote_name Optional [ str ] Human readable rempte name. None run_clone bool Should we clone a repo if none exists? (init if submodule) True run_pull bool Should we pull a repo if it already exists? (update if submodule) True recursive bool Should we recursively update/init submodules? True is_submodule bool Is this a submodule we're working with? False password_prompt bool Should we ask for a password if none is provided? False password_warning bool Should we warn the user if we're using a password on a clone? True remote_user Optional [ str ] The remote user. None remote_pass Optional [ str ] The remote user's password. None quiet bool Run silently. Implies not password_prompt . False progress bool Display a progress meter. True verbose bool verbose git output if possible. False mkdir bool create the parent directory of the repo if needed. False Source code in simple_uam/util/system/git.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , )","title":"clone_or_pull()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.Pip","text":"Static class used to wrap a bunch of pip commands. Source code in simple_uam/util/system/pip.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Pip (): \"\"\" Static class used to wrap a bunch of pip commands. \"\"\" @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , )","title":"Pip"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.pip.Pip.install","text":"Central function that will run pip install. Parameters: Name Type Description Default *packages List [ str ] The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. () editable bool If installing from vcs or local dir, should this package be installed as editable. False requirements_file Union [ str , Path , None] The requirements file to extract a list of packages from. Mutually exclusive with packages. None upgrade bool Upgrade packages in place, mutually exclusive with upgrade. True progress bool Show the progress bar. True quiet bool Run silently False verbose bool Run w/ verbose output. False python Union [ str , Path ] Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. 'python' Source code in simple_uam/util/system/pip.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , )","title":"install()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.Rsync","text":"Static class used to wrap a bunch of rsync commands. Source code in simple_uam/util/system/rsync.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 class Rsync (): \"\"\" Static class used to wrap a bunch of rsync commands. \"\"\" @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" ) @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" ) @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , ) @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , ) itemize_regex = re . compile ( r '^[*><\\.+a-zA-Z\\s] {11} \\s(.*)$' , re . MULTILINE ) \"\"\" Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/ \"\"\" @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out )","title":"Rsync"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.itemize_regex","text":"Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/","title":"itemize_regex"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.archive_changes","text":"Package any files that are different in src (compared to ref) into a zip file at out. Parameters: Name Type Description Default ref Union [ str , Path ] The reference directory required src Union [ str , Path ] The modified directory required out Union [ str , Path ] The location of the output zip file required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] Source code in simple_uam/util/system/rsync.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out )","title":"archive_changes()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.copy_dir","text":"Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quiet bool supress non-error output False Source code in simple_uam/util/system/rsync.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , )","title":"copy_dir()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.list_changes","text":"Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Parameters: Name Type Description Default ref Union [ str , Path ] the reference directory required src Union [ str , Path ] the source dir, to check for changes required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] preserve_dirs bool preserve directories in the output file list. False prune_missing bool remove entries that aren't present in dst. True Source code in simple_uam/util/system/rsync.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes","title":"list_changes()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.norm_path","text":"Rsync on windows is weird about paths, this will format an input path appropriately. Parameters: Name Type Description Default path Union [ str , Path ] The Path to format. required is_dir Optional [ bool ] if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. None Returns: Type Description str An absolute path string suitable as a cmd line arg for rsync. Source code in simple_uam/util/system/rsync.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str","title":"norm_path()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.require","text":"Raises an error if rsync isn't installed. Source code in simple_uam/util/system/rsync.py 29 30 31 32 33 34 35 36 37 @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" )","title":"require()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.run","text":"Run rsync with args return the completed process object. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] archive bool Run in archive mode, True delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quite supress non-error messages. required dry_run bool make no changes False itemize_changes bool print a list of all changes to stdout False capture_output bool do we capture stdout? False links bool When symlinks are encountered, recreate the symlink on the destination. False recursive bool This tells rsync to copy directories recursively. False times bool preserve modification times. False owner bool preserve owner (super-user only) False group bool preserve group False perms bool preserve permissions False Source code in simple_uam/util/system/rsync.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , )","title":"run()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.rsync.Rsync.which","text":"Returns the rsync executable if it exists or None otherwise. Source code in simple_uam/util/system/rsync.py 21 22 23 24 25 26 27 @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" )","title":"which()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.archive_files","text":"Archives the files in files to out , preserving any structure files have relative to cwd . Note: All files must either be relative or in a subdir of cwd . Parameters: Name Type Description Default cwd Union [ str , Path ] Root locations from which we're gathering files. required files List [ Union [ str , Path ]] list of files to archive. required out Union [ str , Path ] Output zipfile location. required missing_ok bool Do we ignore files that don't exist? False Source code in simple_uam/util/system/backup.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def archive_files ( cwd : Union [ str , Path ], files : List [ Union [ str , Path ]], out : Union [ str , Path ], missing_ok : bool = False , ): \"\"\" Archives the files in `files` to `out`, preserving any structure `files` have relative to `cwd`. Note: All `files` must either be relative or in a subdir of `cwd`. Arguments: cwd: Root locations from which we're gathering files. files: list of files to archive. out: Output zipfile location. missing_ok: Do we ignore files that don't exist? \"\"\" cwd = Path ( cwd ) . resolve () out = Path ( out ) . resolve () def make_arc ( f : Path ) -> Path : \"\"\" Get the relative archive file path for a given file. \"\"\" if f . is_absolute () and f . is_relative_to ( cwd ): return f . relative_to ( cwd ) elif not f . is_absolute (): return f else : err = RuntimeError ( \"Invalid file location\" ) log . exception ( \"File not subdir of cwd.\" , file = str ( f ), cwd = str ( cwd ), err = err , ) raise err files = [ make_arc ( Path ( f )) for f in files ] # Iterate through the files to archive, adding them to the zip file as # needed. with ZipFile ( out , 'x' ) as zf : for arc_file in files : sys_file = cwd / arc_file if not sys_file . exists () and missing_ok : pass else : zf . write ( sys_file , arc_file )","title":"archive_files()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.backup_file","text":"Makes a backup copy of a file. Parameters: Name Type Description Default file_path Union [ Path , str ] The path to file. required backup_dir Union [ Path , str , None] The directory backups are placed in, default is the same dir as the file. None delete bool Do we delete the original? (default: False) False missing_ok bool Do we silently ignore if the file is missing? (default: True) True Source code in simple_uam/util/system/backup.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def backup_file ( file_path : Union [ Path , str ], backup_dir : Union [ Path , str , None ] = None , delete : bool = False , missing_ok : bool = True , ): \"\"\" Makes a backup copy of a file. Arguments: file_path: The path to file. backup_dir: The directory backups are placed in, default is the same dir as the file. delete: Do we delete the original? (default: False) missing_ok: Do we silently ignore if the file is missing? (default: True) \"\"\" file_path = Path ( file_path ) bak_file = file_path . with_name ( f \" { file_path . name } .bak\" ) if not backup_dir : backup_dir = file_path . parent backup_dir = Path ( backup_dir ) if file_path . exists (): count = 1 time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } .bak\" ) while bak_file . exists (): bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } . { count } .bak\" ) count += 1 backup_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Creating Backup File.\" , file = str ( file_path ), backup = str ( bak_file ), delete_original = delete , ) if delete : shutil . move ( file_path , bak_file ) else : shutil . copy2 ( file_path , bak_file ) return bak_file elif not missing_ok : raise RuntimeError ( f \"Cannot backup { str ( file_path ) } file does not exist.\" ) else : log . info ( \"No file to backup.\" , file = str ( file_path ), backup = str ( bak_file ), ) return None","title":"backup_file()"},{"location":"reference/simple_uam/util/system/#simple_uam.util.system.configure_file","text":"Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Parameters: Name Type Description Default input_file Union [ str , Path ] The input_file to use. required output_file Union [ str , Path ] The output location to place the configured file. required replacements Dict [ str , str ] The string to find and replace in the input file. {} exist_ok bool If true will overwrite an existing file. False backup bool If overwriting a file do we create a backup? True backup_dir Union [ str , Path , None] Place backups in this directory, if not specified defaults to output directory. None Source code in simple_uam/util/system/backup.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def configure_file ( input_file : Union [ str , Path ], output_file : Union [ str , Path ], replacements : Dict [ str , str ] = {}, exist_ok : bool = False , backup : bool = True , backup_dir : Union [ str , Path , None ] = None ): \"\"\" Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Arguments: input_file: The input_file to use. output_file: The output location to place the configured file. replacements: The string to find and replace in the input file. exist_ok: If true will overwrite an existing file. backup: If overwriting a file do we create a backup? backup_dir: Place backups in this directory, if not specified defaults to output directory. \"\"\" input_file = Path ( input_file ) . resolve () output_file = Path ( output_file ) . resolve () # Verification if not exist_ok and output_file . exists (): err = RuntimeError ( \"Target file already exists.\" ) log . exception ( \"Cannot configure file because output already exists.\" , err = err , input_file = input_file , output_file = output_file , ) raise err # Backup if backup : backup_file ( output_file , backup_dir = backup_dir , ) log . info ( \"Reading configuration input.\" , input_file = str ( input_file )) with input_file . open ( 'r' ) as file : filedata = file . read () for find , replace in replacements . items (): log . info ( \"Performing configuration replacement.\" , find = find , replace = replace , ) filedata = filedata . replace ( find , replace ) # delete output here incase output == input output_file . unlink ( missing_ok = True ) log . info ( \"Writing configuration output.\" , output_file = str ( output_file )) with output_file . open ( 'w' ) as file : file . write ( filedata )","title":"configure_file()"},{"location":"reference/simple_uam/util/system/backup/","text":"archive_files ( cwd , files , out , missing_ok = False ) \u00b6 Archives the files in files to out , preserving any structure files have relative to cwd . Note: All files must either be relative or in a subdir of cwd . Parameters: Name Type Description Default cwd Union [ str , Path ] Root locations from which we're gathering files. required files List [ Union [ str , Path ]] list of files to archive. required out Union [ str , Path ] Output zipfile location. required missing_ok bool Do we ignore files that don't exist? False Source code in simple_uam/util/system/backup.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def archive_files ( cwd : Union [ str , Path ], files : List [ Union [ str , Path ]], out : Union [ str , Path ], missing_ok : bool = False , ): \"\"\" Archives the files in `files` to `out`, preserving any structure `files` have relative to `cwd`. Note: All `files` must either be relative or in a subdir of `cwd`. Arguments: cwd: Root locations from which we're gathering files. files: list of files to archive. out: Output zipfile location. missing_ok: Do we ignore files that don't exist? \"\"\" cwd = Path ( cwd ) . resolve () out = Path ( out ) . resolve () def make_arc ( f : Path ) -> Path : \"\"\" Get the relative archive file path for a given file. \"\"\" if f . is_absolute () and f . is_relative_to ( cwd ): return f . relative_to ( cwd ) elif not f . is_absolute (): return f else : err = RuntimeError ( \"Invalid file location\" ) log . exception ( \"File not subdir of cwd.\" , file = str ( f ), cwd = str ( cwd ), err = err , ) raise err files = [ make_arc ( Path ( f )) for f in files ] # Iterate through the files to archive, adding them to the zip file as # needed. with ZipFile ( out , 'x' ) as zf : for arc_file in files : sys_file = cwd / arc_file if not sys_file . exists () and missing_ok : pass else : zf . write ( sys_file , arc_file ) backup_file ( file_path , backup_dir = None , delete = False , missing_ok = True ) \u00b6 Makes a backup copy of a file. Parameters: Name Type Description Default file_path Union [ Path , str ] The path to file. required backup_dir Union [ Path , str , None] The directory backups are placed in, default is the same dir as the file. None delete bool Do we delete the original? (default: False) False missing_ok bool Do we silently ignore if the file is missing? (default: True) True Source code in simple_uam/util/system/backup.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def backup_file ( file_path : Union [ Path , str ], backup_dir : Union [ Path , str , None ] = None , delete : bool = False , missing_ok : bool = True , ): \"\"\" Makes a backup copy of a file. Arguments: file_path: The path to file. backup_dir: The directory backups are placed in, default is the same dir as the file. delete: Do we delete the original? (default: False) missing_ok: Do we silently ignore if the file is missing? (default: True) \"\"\" file_path = Path ( file_path ) bak_file = file_path . with_name ( f \" { file_path . name } .bak\" ) if not backup_dir : backup_dir = file_path . parent backup_dir = Path ( backup_dir ) if file_path . exists (): count = 1 time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } .bak\" ) while bak_file . exists (): bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } . { count } .bak\" ) count += 1 backup_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Creating Backup File.\" , file = str ( file_path ), backup = str ( bak_file ), delete_original = delete , ) if delete : shutil . move ( file_path , bak_file ) else : shutil . copy2 ( file_path , bak_file ) return bak_file elif not missing_ok : raise RuntimeError ( f \"Cannot backup { str ( file_path ) } file does not exist.\" ) else : log . info ( \"No file to backup.\" , file = str ( file_path ), backup = str ( bak_file ), ) return None configure_file ( input_file , output_file , replacements = {}, exist_ok = False , backup = True , backup_dir = None ) \u00b6 Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Parameters: Name Type Description Default input_file Union [ str , Path ] The input_file to use. required output_file Union [ str , Path ] The output location to place the configured file. required replacements Dict [ str , str ] The string to find and replace in the input file. {} exist_ok bool If true will overwrite an existing file. False backup bool If overwriting a file do we create a backup? True backup_dir Union [ str , Path , None] Place backups in this directory, if not specified defaults to output directory. None Source code in simple_uam/util/system/backup.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def configure_file ( input_file : Union [ str , Path ], output_file : Union [ str , Path ], replacements : Dict [ str , str ] = {}, exist_ok : bool = False , backup : bool = True , backup_dir : Union [ str , Path , None ] = None ): \"\"\" Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Arguments: input_file: The input_file to use. output_file: The output location to place the configured file. replacements: The string to find and replace in the input file. exist_ok: If true will overwrite an existing file. backup: If overwriting a file do we create a backup? backup_dir: Place backups in this directory, if not specified defaults to output directory. \"\"\" input_file = Path ( input_file ) . resolve () output_file = Path ( output_file ) . resolve () # Verification if not exist_ok and output_file . exists (): err = RuntimeError ( \"Target file already exists.\" ) log . exception ( \"Cannot configure file because output already exists.\" , err = err , input_file = input_file , output_file = output_file , ) raise err # Backup if backup : backup_file ( output_file , backup_dir = backup_dir , ) log . info ( \"Reading configuration input.\" , input_file = str ( input_file )) with input_file . open ( 'r' ) as file : filedata = file . read () for find , replace in replacements . items (): log . info ( \"Performing configuration replacement.\" , find = find , replace = replace , ) filedata = filedata . replace ( find , replace ) # delete output here incase output == input output_file . unlink ( missing_ok = True ) log . info ( \"Writing configuration output.\" , output_file = str ( output_file )) with output_file . open ( 'w' ) as file : file . write ( filedata )","title":"backup"},{"location":"reference/simple_uam/util/system/backup/#simple_uam.util.system.backup.archive_files","text":"Archives the files in files to out , preserving any structure files have relative to cwd . Note: All files must either be relative or in a subdir of cwd . Parameters: Name Type Description Default cwd Union [ str , Path ] Root locations from which we're gathering files. required files List [ Union [ str , Path ]] list of files to archive. required out Union [ str , Path ] Output zipfile location. required missing_ok bool Do we ignore files that don't exist? False Source code in simple_uam/util/system/backup.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def archive_files ( cwd : Union [ str , Path ], files : List [ Union [ str , Path ]], out : Union [ str , Path ], missing_ok : bool = False , ): \"\"\" Archives the files in `files` to `out`, preserving any structure `files` have relative to `cwd`. Note: All `files` must either be relative or in a subdir of `cwd`. Arguments: cwd: Root locations from which we're gathering files. files: list of files to archive. out: Output zipfile location. missing_ok: Do we ignore files that don't exist? \"\"\" cwd = Path ( cwd ) . resolve () out = Path ( out ) . resolve () def make_arc ( f : Path ) -> Path : \"\"\" Get the relative archive file path for a given file. \"\"\" if f . is_absolute () and f . is_relative_to ( cwd ): return f . relative_to ( cwd ) elif not f . is_absolute (): return f else : err = RuntimeError ( \"Invalid file location\" ) log . exception ( \"File not subdir of cwd.\" , file = str ( f ), cwd = str ( cwd ), err = err , ) raise err files = [ make_arc ( Path ( f )) for f in files ] # Iterate through the files to archive, adding them to the zip file as # needed. with ZipFile ( out , 'x' ) as zf : for arc_file in files : sys_file = cwd / arc_file if not sys_file . exists () and missing_ok : pass else : zf . write ( sys_file , arc_file )","title":"archive_files()"},{"location":"reference/simple_uam/util/system/backup/#simple_uam.util.system.backup.backup_file","text":"Makes a backup copy of a file. Parameters: Name Type Description Default file_path Union [ Path , str ] The path to file. required backup_dir Union [ Path , str , None] The directory backups are placed in, default is the same dir as the file. None delete bool Do we delete the original? (default: False) False missing_ok bool Do we silently ignore if the file is missing? (default: True) True Source code in simple_uam/util/system/backup.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def backup_file ( file_path : Union [ Path , str ], backup_dir : Union [ Path , str , None ] = None , delete : bool = False , missing_ok : bool = True , ): \"\"\" Makes a backup copy of a file. Arguments: file_path: The path to file. backup_dir: The directory backups are placed in, default is the same dir as the file. delete: Do we delete the original? (default: False) missing_ok: Do we silently ignore if the file is missing? (default: True) \"\"\" file_path = Path ( file_path ) bak_file = file_path . with_name ( f \" { file_path . name } .bak\" ) if not backup_dir : backup_dir = file_path . parent backup_dir = Path ( backup_dir ) if file_path . exists (): count = 1 time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } .bak\" ) while bak_file . exists (): bak_file = backup_dir . with_name ( f \" { file_path . name } . { time_str } . { count } .bak\" ) count += 1 backup_dir . mkdir ( parents = True , exist_ok = True ) log . info ( \"Creating Backup File.\" , file = str ( file_path ), backup = str ( bak_file ), delete_original = delete , ) if delete : shutil . move ( file_path , bak_file ) else : shutil . copy2 ( file_path , bak_file ) return bak_file elif not missing_ok : raise RuntimeError ( f \"Cannot backup { str ( file_path ) } file does not exist.\" ) else : log . info ( \"No file to backup.\" , file = str ( file_path ), backup = str ( bak_file ), ) return None","title":"backup_file()"},{"location":"reference/simple_uam/util/system/backup/#simple_uam.util.system.backup.configure_file","text":"Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Parameters: Name Type Description Default input_file Union [ str , Path ] The input_file to use. required output_file Union [ str , Path ] The output location to place the configured file. required replacements Dict [ str , str ] The string to find and replace in the input file. {} exist_ok bool If true will overwrite an existing file. False backup bool If overwriting a file do we create a backup? True backup_dir Union [ str , Path , None] Place backups in this directory, if not specified defaults to output directory. None Source code in simple_uam/util/system/backup.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def configure_file ( input_file : Union [ str , Path ], output_file : Union [ str , Path ], replacements : Dict [ str , str ] = {}, exist_ok : bool = False , backup : bool = True , backup_dir : Union [ str , Path , None ] = None ): \"\"\" Will \"configure\" and install a file by performing a set of string replacements and moving it to a final location. Arguments: input_file: The input_file to use. output_file: The output location to place the configured file. replacements: The string to find and replace in the input file. exist_ok: If true will overwrite an existing file. backup: If overwriting a file do we create a backup? backup_dir: Place backups in this directory, if not specified defaults to output directory. \"\"\" input_file = Path ( input_file ) . resolve () output_file = Path ( output_file ) . resolve () # Verification if not exist_ok and output_file . exists (): err = RuntimeError ( \"Target file already exists.\" ) log . exception ( \"Cannot configure file because output already exists.\" , err = err , input_file = input_file , output_file = output_file , ) raise err # Backup if backup : backup_file ( output_file , backup_dir = backup_dir , ) log . info ( \"Reading configuration input.\" , input_file = str ( input_file )) with input_file . open ( 'r' ) as file : filedata = file . read () for find , replace in replacements . items (): log . info ( \"Performing configuration replacement.\" , find = find , replace = replace , ) filedata = filedata . replace ( find , replace ) # delete output here incase output == input output_file . unlink ( missing_ok = True ) log . info ( \"Writing configuration output.\" , output_file = str ( output_file )) with output_file . open ( 'w' ) as file : file . write ( filedata )","title":"configure_file()"},{"location":"reference/simple_uam/util/system/git/","text":"Git \u00b6 Static class used to wrap a bunch of git commands. Source code in simple_uam/util/system/git.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class Git (): \"\"\" Static class used to wrap a bunch of git commands. \"\"\" @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , ) clone_or_pull ( repo_uri , deploy_dir , branch = None , repo_name = None , remote_name = None , run_clone = True , run_pull = True , recursive = True , is_submodule = False , password_prompt = False , password_warning = True , remote_user = None , remote_pass = None , quiet = False , progress = True , verbose = False , mkdir = False ) staticmethod \u00b6 Central function that will run a git init/update operation. Parameters: Name Type Description Default repo_uri str The url of the core repo. required deploy_dir Union [ str , Path ] Directory that the repo should be unpacked into. required branch Optional [ str ] The branch we want to checkout or pull from. None repo_name Optional [ str ] Human readable repo name. None remote_name Optional [ str ] Human readable rempte name. None run_clone bool Should we clone a repo if none exists? (init if submodule) True run_pull bool Should we pull a repo if it already exists? (update if submodule) True recursive bool Should we recursively update/init submodules? True is_submodule bool Is this a submodule we're working with? False password_prompt bool Should we ask for a password if none is provided? False password_warning bool Should we warn the user if we're using a password on a clone? True remote_user Optional [ str ] The remote user. None remote_pass Optional [ str ] The remote user's password. None quiet bool Run silently. Implies not password_prompt . False progress bool Display a progress meter. True verbose bool verbose git output if possible. False mkdir bool create the parent directory of the repo if needed. False Source code in simple_uam/util/system/git.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , )","title":"git"},{"location":"reference/simple_uam/util/system/git/#simple_uam.util.system.git.Git","text":"Static class used to wrap a bunch of git commands. Source code in simple_uam/util/system/git.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class Git (): \"\"\" Static class used to wrap a bunch of git commands. \"\"\" @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , )","title":"Git"},{"location":"reference/simple_uam/util/system/git/#simple_uam.util.system.git.Git.clone_or_pull","text":"Central function that will run a git init/update operation. Parameters: Name Type Description Default repo_uri str The url of the core repo. required deploy_dir Union [ str , Path ] Directory that the repo should be unpacked into. required branch Optional [ str ] The branch we want to checkout or pull from. None repo_name Optional [ str ] Human readable repo name. None remote_name Optional [ str ] Human readable rempte name. None run_clone bool Should we clone a repo if none exists? (init if submodule) True run_pull bool Should we pull a repo if it already exists? (update if submodule) True recursive bool Should we recursively update/init submodules? True is_submodule bool Is this a submodule we're working with? False password_prompt bool Should we ask for a password if none is provided? False password_warning bool Should we warn the user if we're using a password on a clone? True remote_user Optional [ str ] The remote user. None remote_pass Optional [ str ] The remote user's password. None quiet bool Run silently. Implies not password_prompt . False progress bool Display a progress meter. True verbose bool verbose git output if possible. False mkdir bool create the parent directory of the repo if needed. False Source code in simple_uam/util/system/git.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @staticmethod def clone_or_pull ( repo_uri : str , deploy_dir : Union [ str , Path ], branch : Optional [ str ] = None , repo_name : Optional [ str ] = None , remote_name : Optional [ str ] = None , run_clone : bool = True , run_pull : bool = True , recursive : bool = True , is_submodule : bool = False , password_prompt : bool = False , password_warning : bool = True , remote_user : Optional [ str ] = None , remote_pass : Optional [ str ] = None , quiet : bool = False , progress : bool = True , verbose : bool = False , mkdir : bool = False , ): \"\"\" Central function that will run a git init/update operation. Arguments: repo_uri: The url of the core repo. deploy_dir: Directory that the repo should be unpacked into. branch: The branch we want to checkout or pull from. repo_name: Human readable repo name. remote_name: Human readable rempte name. run_clone: Should we clone a repo if none exists? (init if submodule) run_pull: Should we pull a repo if it already exists? (update if submodule) recursive: Should we recursively update/init submodules? is_submodule: Is this a submodule we're working with? password_prompt: Should we ask for a password if none is provided? password_warning: Should we warn the user if we're using a password on a clone? remote_user: The remote user. remote_pass: The remote user's password. quiet: Run silently. Implies `not password_prompt`. progress: Display a progress meter. verbose: verbose git output if possible. mkdir: create the parent directory of the repo if needed. \"\"\" ### Param Init ### repo_uri = urlparse ( repo_uri ) if not remote_name : remote_name = repo_uri . netloc if not repo_name : repo_name = repo_uri . path remote_user = remote_user or repo_uri . username remote_pass = remote_pass or repo_uri . password password_prompt = password_prompt and quiet deploy_dir = Path ( deploy_dir ) work_dir = deploy_dir . parent # working dir for clone operations target_dir = deploy_dir . name # target dir for clone operations if mkdir : work_dir . mkdir ( parents = True , exist_ok = True ) is_clone_op = not deploy_dir . exists () # is this a clone-style or pull-style op? opts = dict ( repo_uri = repo_uri , remote_name = remote_name , remote_user = remote_user , remote_pass = remote_pass is not None , repo_name = repo_name , password_prompt = password_prompt , deploy_dir = str ( deploy_dir ), branch = branch , is_clone_op = is_clone_op , recursive = recursive , quiet = quiet , progress = progress , rui_user = repo_uri . username , rui_pass = repo_uri . password , ) ### Select Cmd ### git_cmd = [ 'git' ] op_name = \"Cloning\" err = RuntimeError ( \"Invalid git op.\" ) if is_clone_op : if not run_clone or is_submodule : log . exception ( \"Cannot run pull or submodule op without existing deploy dir.\" , err = err , ** opts , ) raise err else : op_name = \"Cloning\" git_cmd . append ( \"clone\" ) if recursive : git_cmd . append ( \"--recurse-submodules\" ) else : if not is_submodule and run_pull : op_name = \"Pulling\" git_cmd . append ( \"pull\" ) branch = None if recursive : git_cmd . append ( \"--recurse-submodules\" ) elif is_submodule : op_name = \"Updating Submodule\" git_cmd . append ( \"submodule\" ) git_cmd . append ( \"update\" ) git_cmd . append ( \"--init\" ) if recursive : git_cmd . append ( \"--recursive\" ) else : log . exception ( \"Cannot clone when deploy directory exists.\" , err = err , ** opts , ) raise err if quiet : git_cmd . append ( \"--quiet\" ) if progress : git_cmd . append ( \"--progress\" ) if verbose : git_cmd . append ( \"--verbose\" ) if branch : git_cmd . append ( \"--branch\" ) git_cmd . append ( branch ) ### Prompt for password ### def has_pass (): return remote_user and remote_pass password_prompt = password_prompt and is_clone_op if password_prompt and not has_pass (): choice = input ( textwrap . dedent ( f \"\"\" { op_Name } for repo at { str ( deploy_dir ) } . Server: { remote_name } Repo: { repo_name } Full URL: { repo_url . geturl () } Repo not found, how would you like to authenticate: 1) Automatically via SSH, or because no authentication is needed. 2) With a username and password. Please enter your choice (1 or 2): \"\"\" )) if choice == '1' : pass elif choice != '2' : err = RuntimeError ( \"Please enter a valid choice.\" ) log . exception ( \"Please enter a valid choice.\" , err = err , ** opts , ) raise err else : remote_user = input ( f \"Enter Username for { remote_name } : \" ) remote_pass = input ( f \"Enter Password for { remote_name } : \" ) ### Validate Password Entry ### if is_clone_op and has_pass (): log . warning ( textwrap . dedent ( f \"\"\" A plaintext password has been provided for a clone operation. Git will save this locally within the remote, this is insecure. Please consider installing an SSH key for { remote_name } instead. \"\"\" )) choice = input ( \"Are you sure you want to continue? (y/N)\" ) if choice == \"N\" : raise RuntimeError ( \"Confirmation not given, aborting operation.\" ) elif choice != \"y\" : raise RuntimeError ( \"Invalid confirmation, aborting operation.\" ) ## Fix up the uri if we have a username and password. if has_pass (): repo_uri = repo_uri . _replace ( netloc = f \" { remote_user } : { remote_pass } @ { repo_uri . hostname } \" ) ### Run the command ### if is_clone_op : subprocess . run ( [ * git_cmd , repo_uri . geturl (), target_dir ], cwd = work_dir , ) else : subprocess . run ( git_cmd , cwd = deploy_dir , )","title":"clone_or_pull()"},{"location":"reference/simple_uam/util/system/nssm/","text":"NssmService \u00b6 Class used to wrap the Non-Sucky Service Manager (nssm) Source code in simple_uam/util/system/nssm.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @define class NssmService (): \"\"\" Class used to wrap the Non-Sucky Service Manager (nssm) \"\"\" service_name : str = field () \"\"\" The name of the service. \"\"\" display_name : str = field () \"\"\" The displayed name of the service. \"\"\" description : str = field ( converter = textwrap . dedent , ) \"\"\" The displayed service description. \"\"\" config : ServiceConfig = field () \"\"\" The user modifiable configuration settings for the service. \"\"\" cwd : Path = field ( converter = [ Path , Path . resolve ], ) \"\"\" The working dir when running the service. \"\"\" exe : Path = field ( converter = _exe_convert , ) \"\"\" The path to the app executable or the exe name. \"\"\" @exe . validator def _exe_validator ( self , attr , val ): if platform . system () != 'Windows' : return raise RuntimeError ( \"NSSM service managment is Windows only.\" ) elif val . is_absolute () and val . exists (): pass elif ( cwd / val ) . exists (): pass else : raise RuntimeError ( f \"Could not find executable for { str ( val ) } \" ) args : Union [ str , List [ str ], None ] = field ( default = None , ) \"\"\" Arguments to use when starting the service. Windows quotation escapes are stupid: https://stackoverflow.com/questions/7760545/escape-double-quotes-in-parameter \"\"\" account : str = field ( default = \"LocalSystem\" , ) \"\"\" The user account to run the service with. See [here](https://nssm.cc/commands) under \"Native parameters\" and \"ObjectName\" for more info. \"\"\" app_stop_method_skip : int = field ( default = 0 , ) \"\"\" Which app stop methods to skip. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" app_stop_console_delay : int = field ( default = 30000 , ) \"\"\" Delay after attempting Control-C stop in ms. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" app_stop_window_delay : int = field ( default = 30000 , ) \"\"\" Delay after attempting window close stop in ms. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" app_stop_thread_delay : int = field ( default = 30000 , ) \"\"\" Delay after attempting thread kill stop in ms. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" def install ( self ): \"\"\" Install the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'install' , self . service_name , str ( self . exe )], ) self . configure () def uninstall ( self , confirm = True ): \"\"\" Uninstall the service. Arguments: confirm: show a GUI confirmation dialog. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) conf_arg = [ 'confirm' ] if confirm else [] subprocess . run ( [ 'nssm' , 'remove' , self . service_name , * conf_arg ], ) def gui_edit ( self ): \"\"\" Opens the NSSM GUI editor. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'edit' , self . service_name ], ) def configure ( self ): \"\"\" Configures all the service parameters. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) assignments = { \"Application\" : str ( self . exe ), \"AppDirectory\" : str ( self . cwd ), \"AppParameters\" : self . args , \"DisplayName\" : self . display_name , \"Description\" : self . description , \"Start\" : 'SERVICE_AUTO_START' if self . config . auto_start else 'SERVICE_DEMAND_START' , \"ObjectName\" : self . account , \"AppPriority\" : f \" { self . config . priority } _PRIORITY_CLASS\" , \"AppNoConsole\" : '0' if self . config . console else '1' , \"AppStopMethodSkip\" : self . app_stop_method_skip , \"AppStopMethodConsole\" : self . app_stop_console_delay , \"AppStopMethodWindow\" : self . app_stop_window_delay , \"AppStopMethodThreads\" : self . app_stop_thread_delay , \"AppThrottle\" : self . config . restart_throttle , \"AppExit\" :[ 'Default' , self . config . exit_action ], \"AppRestartDelay\" : self . config . restart_delay , \"AppRotateFiles\" : '1' if self . config . rotate_io else '0' , \"AppStdout\" : str ( Path ( self . config . stdout_file ) . resolve ()), \"AppStderr\" : str ( Path ( self . config . stderr_file ) . resolve ()), \"Type\" : 'SERVICE_INTERACTIVE_PROCESS' if self . config . interactive else 'SERVICE_WIN32_OWN_PROCESS' , } for key , val in assignments . items (): args = None if val == None : args = [ 'nssm.exe' , 'reset' , self . service_name , key ] else : args = None if isinstance ( val , list ): args = [ _escape ( a ) for a in val ] else : args = [ _escape ( val )] args = [ 'nssm.exe' , 'set' , self . service_name , key , * args ] log . info ( \"Updating NSSM Config.\" , args = args , ) subprocess . run ( args ) def start ( self ): \"\"\" Start the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) if self . config . redirect_io : stdout_dir = Path ( self . config . stdout_file ) . parent stdout_dir . mkdir ( parents = True , exist_ok = True ) stderr_dir = Path ( self . config . stderr_file ) . parent stderr_dir . mkdir ( parents = True , exist_ok = True ) subprocess . run ( [ 'nssm' , 'start' , self . service_name ], ) def stop ( self ): \"\"\" Stop the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'stop' , self . service_name ], ) def restart ( self ): \"\"\" Restart the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'restart' , self . service_name ], ) def status ( self ): \"\"\" Get service status. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) process = subprocess . run ( [ 'nssm' , 'status' , self . service_name ], ) return process . exitcode account : str = field ( default = 'LocalSystem' ) class-attribute \u00b6 The user account to run the service with. See here under \"Native parameters\" and \"ObjectName\" for more info. app_stop_console_delay : int = field ( default = 30000 ) class-attribute \u00b6 Delay after attempting Control-C stop in ms. See here for more. app_stop_method_skip : int = field ( default = 0 ) class-attribute \u00b6 Which app stop methods to skip. See here for more. app_stop_thread_delay : int = field ( default = 30000 ) class-attribute \u00b6 Delay after attempting thread kill stop in ms. See here for more. app_stop_window_delay : int = field ( default = 30000 ) class-attribute \u00b6 Delay after attempting window close stop in ms. See here for more. args : Union [ str , List [ str ], None ] = field ( default = None ) class-attribute \u00b6 Arguments to use when starting the service. Windows quotation escapes are stupid: https://stackoverflow.com/questions/7760545/escape-double-quotes-in-parameter config : ServiceConfig = field () class-attribute \u00b6 The user modifiable configuration settings for the service. cwd : Path = field ( converter = [ Path , Path . resolve ]) class-attribute \u00b6 The working dir when running the service. description : str = field ( converter = textwrap . dedent ) class-attribute \u00b6 The displayed service description. display_name : str = field () class-attribute \u00b6 The displayed name of the service. exe : Path = field ( converter = _exe_convert ) class-attribute \u00b6 The path to the app executable or the exe name. service_name : str = field () class-attribute \u00b6 The name of the service. configure () \u00b6 Configures all the service parameters. Source code in simple_uam/util/system/nssm.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def configure ( self ): \"\"\" Configures all the service parameters. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) assignments = { \"Application\" : str ( self . exe ), \"AppDirectory\" : str ( self . cwd ), \"AppParameters\" : self . args , \"DisplayName\" : self . display_name , \"Description\" : self . description , \"Start\" : 'SERVICE_AUTO_START' if self . config . auto_start else 'SERVICE_DEMAND_START' , \"ObjectName\" : self . account , \"AppPriority\" : f \" { self . config . priority } _PRIORITY_CLASS\" , \"AppNoConsole\" : '0' if self . config . console else '1' , \"AppStopMethodSkip\" : self . app_stop_method_skip , \"AppStopMethodConsole\" : self . app_stop_console_delay , \"AppStopMethodWindow\" : self . app_stop_window_delay , \"AppStopMethodThreads\" : self . app_stop_thread_delay , \"AppThrottle\" : self . config . restart_throttle , \"AppExit\" :[ 'Default' , self . config . exit_action ], \"AppRestartDelay\" : self . config . restart_delay , \"AppRotateFiles\" : '1' if self . config . rotate_io else '0' , \"AppStdout\" : str ( Path ( self . config . stdout_file ) . resolve ()), \"AppStderr\" : str ( Path ( self . config . stderr_file ) . resolve ()), \"Type\" : 'SERVICE_INTERACTIVE_PROCESS' if self . config . interactive else 'SERVICE_WIN32_OWN_PROCESS' , } for key , val in assignments . items (): args = None if val == None : args = [ 'nssm.exe' , 'reset' , self . service_name , key ] else : args = None if isinstance ( val , list ): args = [ _escape ( a ) for a in val ] else : args = [ _escape ( val )] args = [ 'nssm.exe' , 'set' , self . service_name , key , * args ] log . info ( \"Updating NSSM Config.\" , args = args , ) subprocess . run ( args ) gui_edit () \u00b6 Opens the NSSM GUI editor. Source code in simple_uam/util/system/nssm.py 157 158 159 160 161 162 163 164 165 def gui_edit ( self ): \"\"\" Opens the NSSM GUI editor. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'edit' , self . service_name ], ) install () \u00b6 Install the service. Source code in simple_uam/util/system/nssm.py 130 131 132 133 134 135 136 137 138 139 140 def install ( self ): \"\"\" Install the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'install' , self . service_name , str ( self . exe )], ) self . configure () restart () \u00b6 Restart the service. Source code in simple_uam/util/system/nssm.py 242 243 244 245 246 247 248 249 250 251 def restart ( self ): \"\"\" Restart the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'restart' , self . service_name ], ) start () \u00b6 Start the service. Source code in simple_uam/util/system/nssm.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def start ( self ): \"\"\" Start the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) if self . config . redirect_io : stdout_dir = Path ( self . config . stdout_file ) . parent stdout_dir . mkdir ( parents = True , exist_ok = True ) stderr_dir = Path ( self . config . stderr_file ) . parent stderr_dir . mkdir ( parents = True , exist_ok = True ) subprocess . run ( [ 'nssm' , 'start' , self . service_name ], ) status () \u00b6 Get service status. Source code in simple_uam/util/system/nssm.py 253 254 255 256 257 258 259 260 261 262 263 def status ( self ): \"\"\" Get service status. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) process = subprocess . run ( [ 'nssm' , 'status' , self . service_name ], ) return process . exitcode stop () \u00b6 Stop the service. Source code in simple_uam/util/system/nssm.py 231 232 233 234 235 236 237 238 239 240 def stop ( self ): \"\"\" Stop the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'stop' , self . service_name ], ) uninstall ( confirm = True ) \u00b6 Uninstall the service. Parameters: Name Type Description Default confirm show a GUI confirmation dialog. True Source code in simple_uam/util/system/nssm.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def uninstall ( self , confirm = True ): \"\"\" Uninstall the service. Arguments: confirm: show a GUI confirmation dialog. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) conf_arg = [ 'confirm' ] if confirm else [] subprocess . run ( [ 'nssm' , 'remove' , self . service_name , * conf_arg ], )","title":"nssm"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService","text":"Class used to wrap the Non-Sucky Service Manager (nssm) Source code in simple_uam/util/system/nssm.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @define class NssmService (): \"\"\" Class used to wrap the Non-Sucky Service Manager (nssm) \"\"\" service_name : str = field () \"\"\" The name of the service. \"\"\" display_name : str = field () \"\"\" The displayed name of the service. \"\"\" description : str = field ( converter = textwrap . dedent , ) \"\"\" The displayed service description. \"\"\" config : ServiceConfig = field () \"\"\" The user modifiable configuration settings for the service. \"\"\" cwd : Path = field ( converter = [ Path , Path . resolve ], ) \"\"\" The working dir when running the service. \"\"\" exe : Path = field ( converter = _exe_convert , ) \"\"\" The path to the app executable or the exe name. \"\"\" @exe . validator def _exe_validator ( self , attr , val ): if platform . system () != 'Windows' : return raise RuntimeError ( \"NSSM service managment is Windows only.\" ) elif val . is_absolute () and val . exists (): pass elif ( cwd / val ) . exists (): pass else : raise RuntimeError ( f \"Could not find executable for { str ( val ) } \" ) args : Union [ str , List [ str ], None ] = field ( default = None , ) \"\"\" Arguments to use when starting the service. Windows quotation escapes are stupid: https://stackoverflow.com/questions/7760545/escape-double-quotes-in-parameter \"\"\" account : str = field ( default = \"LocalSystem\" , ) \"\"\" The user account to run the service with. See [here](https://nssm.cc/commands) under \"Native parameters\" and \"ObjectName\" for more info. \"\"\" app_stop_method_skip : int = field ( default = 0 , ) \"\"\" Which app stop methods to skip. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" app_stop_console_delay : int = field ( default = 30000 , ) \"\"\" Delay after attempting Control-C stop in ms. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" app_stop_window_delay : int = field ( default = 30000 , ) \"\"\" Delay after attempting window close stop in ms. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" app_stop_thread_delay : int = field ( default = 30000 , ) \"\"\" Delay after attempting thread kill stop in ms. See [here](https://nssm.cc/usage#shutdown) for more. \"\"\" def install ( self ): \"\"\" Install the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'install' , self . service_name , str ( self . exe )], ) self . configure () def uninstall ( self , confirm = True ): \"\"\" Uninstall the service. Arguments: confirm: show a GUI confirmation dialog. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) conf_arg = [ 'confirm' ] if confirm else [] subprocess . run ( [ 'nssm' , 'remove' , self . service_name , * conf_arg ], ) def gui_edit ( self ): \"\"\" Opens the NSSM GUI editor. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'edit' , self . service_name ], ) def configure ( self ): \"\"\" Configures all the service parameters. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) assignments = { \"Application\" : str ( self . exe ), \"AppDirectory\" : str ( self . cwd ), \"AppParameters\" : self . args , \"DisplayName\" : self . display_name , \"Description\" : self . description , \"Start\" : 'SERVICE_AUTO_START' if self . config . auto_start else 'SERVICE_DEMAND_START' , \"ObjectName\" : self . account , \"AppPriority\" : f \" { self . config . priority } _PRIORITY_CLASS\" , \"AppNoConsole\" : '0' if self . config . console else '1' , \"AppStopMethodSkip\" : self . app_stop_method_skip , \"AppStopMethodConsole\" : self . app_stop_console_delay , \"AppStopMethodWindow\" : self . app_stop_window_delay , \"AppStopMethodThreads\" : self . app_stop_thread_delay , \"AppThrottle\" : self . config . restart_throttle , \"AppExit\" :[ 'Default' , self . config . exit_action ], \"AppRestartDelay\" : self . config . restart_delay , \"AppRotateFiles\" : '1' if self . config . rotate_io else '0' , \"AppStdout\" : str ( Path ( self . config . stdout_file ) . resolve ()), \"AppStderr\" : str ( Path ( self . config . stderr_file ) . resolve ()), \"Type\" : 'SERVICE_INTERACTIVE_PROCESS' if self . config . interactive else 'SERVICE_WIN32_OWN_PROCESS' , } for key , val in assignments . items (): args = None if val == None : args = [ 'nssm.exe' , 'reset' , self . service_name , key ] else : args = None if isinstance ( val , list ): args = [ _escape ( a ) for a in val ] else : args = [ _escape ( val )] args = [ 'nssm.exe' , 'set' , self . service_name , key , * args ] log . info ( \"Updating NSSM Config.\" , args = args , ) subprocess . run ( args ) def start ( self ): \"\"\" Start the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) if self . config . redirect_io : stdout_dir = Path ( self . config . stdout_file ) . parent stdout_dir . mkdir ( parents = True , exist_ok = True ) stderr_dir = Path ( self . config . stderr_file ) . parent stderr_dir . mkdir ( parents = True , exist_ok = True ) subprocess . run ( [ 'nssm' , 'start' , self . service_name ], ) def stop ( self ): \"\"\" Stop the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'stop' , self . service_name ], ) def restart ( self ): \"\"\" Restart the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'restart' , self . service_name ], ) def status ( self ): \"\"\" Get service status. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) process = subprocess . run ( [ 'nssm' , 'status' , self . service_name ], ) return process . exitcode","title":"NssmService"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.account","text":"The user account to run the service with. See here under \"Native parameters\" and \"ObjectName\" for more info.","title":"account"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.app_stop_console_delay","text":"Delay after attempting Control-C stop in ms. See here for more.","title":"app_stop_console_delay"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.app_stop_method_skip","text":"Which app stop methods to skip. See here for more.","title":"app_stop_method_skip"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.app_stop_thread_delay","text":"Delay after attempting thread kill stop in ms. See here for more.","title":"app_stop_thread_delay"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.app_stop_window_delay","text":"Delay after attempting window close stop in ms. See here for more.","title":"app_stop_window_delay"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.args","text":"Arguments to use when starting the service. Windows quotation escapes are stupid: https://stackoverflow.com/questions/7760545/escape-double-quotes-in-parameter","title":"args"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.config","text":"The user modifiable configuration settings for the service.","title":"config"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.cwd","text":"The working dir when running the service.","title":"cwd"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.description","text":"The displayed service description.","title":"description"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.display_name","text":"The displayed name of the service.","title":"display_name"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.exe","text":"The path to the app executable or the exe name.","title":"exe"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.service_name","text":"The name of the service.","title":"service_name"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.configure","text":"Configures all the service parameters. Source code in simple_uam/util/system/nssm.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def configure ( self ): \"\"\" Configures all the service parameters. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) assignments = { \"Application\" : str ( self . exe ), \"AppDirectory\" : str ( self . cwd ), \"AppParameters\" : self . args , \"DisplayName\" : self . display_name , \"Description\" : self . description , \"Start\" : 'SERVICE_AUTO_START' if self . config . auto_start else 'SERVICE_DEMAND_START' , \"ObjectName\" : self . account , \"AppPriority\" : f \" { self . config . priority } _PRIORITY_CLASS\" , \"AppNoConsole\" : '0' if self . config . console else '1' , \"AppStopMethodSkip\" : self . app_stop_method_skip , \"AppStopMethodConsole\" : self . app_stop_console_delay , \"AppStopMethodWindow\" : self . app_stop_window_delay , \"AppStopMethodThreads\" : self . app_stop_thread_delay , \"AppThrottle\" : self . config . restart_throttle , \"AppExit\" :[ 'Default' , self . config . exit_action ], \"AppRestartDelay\" : self . config . restart_delay , \"AppRotateFiles\" : '1' if self . config . rotate_io else '0' , \"AppStdout\" : str ( Path ( self . config . stdout_file ) . resolve ()), \"AppStderr\" : str ( Path ( self . config . stderr_file ) . resolve ()), \"Type\" : 'SERVICE_INTERACTIVE_PROCESS' if self . config . interactive else 'SERVICE_WIN32_OWN_PROCESS' , } for key , val in assignments . items (): args = None if val == None : args = [ 'nssm.exe' , 'reset' , self . service_name , key ] else : args = None if isinstance ( val , list ): args = [ _escape ( a ) for a in val ] else : args = [ _escape ( val )] args = [ 'nssm.exe' , 'set' , self . service_name , key , * args ] log . info ( \"Updating NSSM Config.\" , args = args , ) subprocess . run ( args )","title":"configure()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.gui_edit","text":"Opens the NSSM GUI editor. Source code in simple_uam/util/system/nssm.py 157 158 159 160 161 162 163 164 165 def gui_edit ( self ): \"\"\" Opens the NSSM GUI editor. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'edit' , self . service_name ], )","title":"gui_edit()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.install","text":"Install the service. Source code in simple_uam/util/system/nssm.py 130 131 132 133 134 135 136 137 138 139 140 def install ( self ): \"\"\" Install the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'install' , self . service_name , str ( self . exe )], ) self . configure ()","title":"install()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.restart","text":"Restart the service. Source code in simple_uam/util/system/nssm.py 242 243 244 245 246 247 248 249 250 251 def restart ( self ): \"\"\" Restart the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'restart' , self . service_name ], )","title":"restart()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.start","text":"Start the service. Source code in simple_uam/util/system/nssm.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def start ( self ): \"\"\" Start the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) if self . config . redirect_io : stdout_dir = Path ( self . config . stdout_file ) . parent stdout_dir . mkdir ( parents = True , exist_ok = True ) stderr_dir = Path ( self . config . stderr_file ) . parent stderr_dir . mkdir ( parents = True , exist_ok = True ) subprocess . run ( [ 'nssm' , 'start' , self . service_name ], )","title":"start()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.status","text":"Get service status. Source code in simple_uam/util/system/nssm.py 253 254 255 256 257 258 259 260 261 262 263 def status ( self ): \"\"\" Get service status. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) process = subprocess . run ( [ 'nssm' , 'status' , self . service_name ], ) return process . exitcode","title":"status()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.stop","text":"Stop the service. Source code in simple_uam/util/system/nssm.py 231 232 233 234 235 236 237 238 239 240 def stop ( self ): \"\"\" Stop the service. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) subprocess . run ( [ 'nssm' , 'stop' , self . service_name ], )","title":"stop()"},{"location":"reference/simple_uam/util/system/nssm/#simple_uam.util.system.nssm.NssmService.uninstall","text":"Uninstall the service. Parameters: Name Type Description Default confirm show a GUI confirmation dialog. True Source code in simple_uam/util/system/nssm.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def uninstall ( self , confirm = True ): \"\"\" Uninstall the service. Arguments: confirm: show a GUI confirmation dialog. \"\"\" if platform . system () != 'Windows' : raise RuntimeError ( \"NSSM service managment is Windows only.\" ) conf_arg = [ 'confirm' ] if confirm else [] subprocess . run ( [ 'nssm' , 'remove' , self . service_name , * conf_arg ], )","title":"uninstall()"},{"location":"reference/simple_uam/util/system/pip/","text":"Pip \u00b6 Static class used to wrap a bunch of pip commands. Source code in simple_uam/util/system/pip.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Pip (): \"\"\" Static class used to wrap a bunch of pip commands. \"\"\" @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , ) install ( * packages , editable = False , requirements_file = None , upgrade = True , progress = True , quiet = False , verbose = False , python = 'python' , cwd = None ) staticmethod \u00b6 Central function that will run pip install. Parameters: Name Type Description Default *packages List [ str ] The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. () editable bool If installing from vcs or local dir, should this package be installed as editable. False requirements_file Union [ str , Path , None] The requirements file to extract a list of packages from. Mutually exclusive with packages. None upgrade bool Upgrade packages in place, mutually exclusive with upgrade. True progress bool Show the progress bar. True quiet bool Run silently False verbose bool Run w/ verbose output. False python Union [ str , Path ] Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. 'python' Source code in simple_uam/util/system/pip.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , )","title":"pip"},{"location":"reference/simple_uam/util/system/pip/#simple_uam.util.system.pip.Pip","text":"Static class used to wrap a bunch of pip commands. Source code in simple_uam/util/system/pip.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Pip (): \"\"\" Static class used to wrap a bunch of pip commands. \"\"\" @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , )","title":"Pip"},{"location":"reference/simple_uam/util/system/pip/#simple_uam.util.system.pip.Pip.install","text":"Central function that will run pip install. Parameters: Name Type Description Default *packages List [ str ] The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. () editable bool If installing from vcs or local dir, should this package be installed as editable. False requirements_file Union [ str , Path , None] The requirements file to extract a list of packages from. Mutually exclusive with packages. None upgrade bool Upgrade packages in place, mutually exclusive with upgrade. True progress bool Show the progress bar. True quiet bool Run silently False verbose bool Run w/ verbose output. False python Union [ str , Path ] Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. 'python' Source code in simple_uam/util/system/pip.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @staticmethod def install ( * packages : List [ str ], editable : bool = False , requirements_file : Union [ str , Path , None ] = None , upgrade : bool = True , progress : bool = True , quiet : bool = False , verbose : bool = False , python : Union [ str , Path ] = \"python\" , cwd : Union [ str , Path , None ] = None ): \"\"\" Central function that will run pip install. Arguments: *packages: The pypi name/project_url/local paths for the package(s) you're trying to install. Only one argument if using a vcs url or a local path. editable: If installing from vcs or local dir, should this package be installed as editable. requirements_file: The requirements file to extract a list of packages from. Mutually exclusive with packages. upgrade: Upgrade packages in place, mutually exclusive with upgrade. progress: Show the progress bar. quiet: Run silently verbose: Run w/ verbose output. python: Which python interpreter do we use? Default: Whatever \"python\" resolves to in the current env. \"\"\" if cwd is None : cwd = Path . cwd () else : cwd = Path ( cwd ) if isinstance ( python , Path ): python = str ( python . resolve ()) pip_args = list () if requirements_file : pip_args . append ( '-r' ) requirements_file = str ( Path ( requirements_file ) . resolve ()) pip_args . append ( requirements_file ) if len ( packages ) != 0 : raise RuntimeError ( \"Cannot specify both requirements file and packages.\" ) elif editable : raise RuntimeError ( \"Pip can't use a requirements file during editable install.\" ) if upgrade : pip_args . append ( \"--upgrade\" ) if editable : raise RuntimeError ( \"Can't mark an editable pip package for upgrade.\" ) if not progress : pip_args . append ( \"--progress-bar\" ) pip_args . append ( \"off\" ) if quiet : pip_args . append ( \"--quiet\" ) if verbose : pip_args . append ( \"--verbose\" ) if editable : pip_args . append ( '-e' ) if len ( packages ) != 1 : raise RuntimeError ( \"Pip can only have one local or vcs package\" + \"during an editable install.\" ) return subprocess . run ( [ python , \"-m\" , \"pip\" , \"install\" , * pip_args , * packages ], cwd = cwd , )","title":"install()"},{"location":"reference/simple_uam/util/system/rsync/","text":"Rsync \u00b6 Static class used to wrap a bunch of rsync commands. Source code in simple_uam/util/system/rsync.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 class Rsync (): \"\"\" Static class used to wrap a bunch of rsync commands. \"\"\" @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" ) @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" ) @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , ) @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , ) itemize_regex = re . compile ( r '^[*><\\.+a-zA-Z\\s] {11} \\s(.*)$' , re . MULTILINE ) \"\"\" Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/ \"\"\" @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out ) itemize_regex = re . compile ( '^[*>< \\\\ .+a-zA-Z \\\\ s] {11} \\\\ s(.*)$' , re . MULTILINE ) class-attribute \u00b6 Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/ archive_changes ( ref , src , out , exclude = [], exclude_from = []) staticmethod \u00b6 Package any files that are different in src (compared to ref) into a zip file at out. Parameters: Name Type Description Default ref Union [ str , Path ] The reference directory required src Union [ str , Path ] The modified directory required out Union [ str , Path ] The location of the output zip file required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] Source code in simple_uam/util/system/rsync.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out ) copy_dir ( src , dst , exclude = [], exclude_from = [], delete = True , update = False , progress = False , verbose = False , quiet = False ) classmethod \u00b6 Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quiet bool supress non-error output False Source code in simple_uam/util/system/rsync.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , ) list_changes ( ref , src , exclude = [], exclude_from = [], preserve_dirs = False , prune_missing = True ) classmethod \u00b6 Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Parameters: Name Type Description Default ref Union [ str , Path ] the reference directory required src Union [ str , Path ] the source dir, to check for changes required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] preserve_dirs bool preserve directories in the output file list. False prune_missing bool remove entries that aren't present in dst. True Source code in simple_uam/util/system/rsync.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes norm_path ( path , is_dir = None ) staticmethod \u00b6 Rsync on windows is weird about paths, this will format an input path appropriately. Parameters: Name Type Description Default path Union [ str , Path ] The Path to format. required is_dir Optional [ bool ] if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. None Returns: Type Description str An absolute path string suitable as a cmd line arg for rsync. Source code in simple_uam/util/system/rsync.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str require () staticmethod \u00b6 Raises an error if rsync isn't installed. Source code in simple_uam/util/system/rsync.py 29 30 31 32 33 34 35 36 37 @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" ) run ( src , dst , * , exclude = [], exclude_from = [], archive = True , delete = True , update = False , progress = False , verbose = False , quiet = False , dry_run = False , itemize_changes = False , capture_output = False , links = False , times = False , owner = False , group = False , perms = False , recursive = False ) classmethod \u00b6 Run rsync with args return the completed process object. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] archive bool Run in archive mode, True delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quite supress non-error messages. required dry_run bool make no changes False itemize_changes bool print a list of all changes to stdout False capture_output bool do we capture stdout? False links bool When symlinks are encountered, recreate the symlink on the destination. False recursive bool This tells rsync to copy directories recursively. False times bool preserve modification times. False owner bool preserve owner (super-user only) False group bool preserve group False perms bool preserve permissions False Source code in simple_uam/util/system/rsync.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , ) which () staticmethod \u00b6 Returns the rsync executable if it exists or None otherwise. Source code in simple_uam/util/system/rsync.py 21 22 23 24 25 26 27 @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" )","title":"rsync"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync","text":"Static class used to wrap a bunch of rsync commands. Source code in simple_uam/util/system/rsync.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 class Rsync (): \"\"\" Static class used to wrap a bunch of rsync commands. \"\"\" @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" ) @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" ) @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , ) @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , ) itemize_regex = re . compile ( r '^[*><\\.+a-zA-Z\\s] {11} \\s(.*)$' , re . MULTILINE ) \"\"\" Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/ \"\"\" @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out )","title":"Rsync"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.itemize_regex","text":"Regex for getting the directories out of the rsync '-i' itemize call. See: https://caissyroger.com/2020/10/06/rsync-itemize-changes/","title":"itemize_regex"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.archive_changes","text":"Package any files that are different in src (compared to ref) into a zip file at out. Parameters: Name Type Description Default ref Union [ str , Path ] The reference directory required src Union [ str , Path ] The modified directory required out Union [ str , Path ] The location of the output zip file required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] Source code in simple_uam/util/system/rsync.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @staticmethod def archive_changes ( ref : Union [ str , Path ], src : Union [ str , Path ], out : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], ): \"\"\" Package any files that are different in src (compared to ref) into a zip file at out. Arguments: ref: The reference directory src: The modified directory out: The location of the output zip file exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) \"\"\" ref = Path ( ref ) src = Path ( src ) out = Path ( out ) changes = Rsync . list_changes ( ref = ref , src = src , exclude = exclude , exclude_from = exclude_from , ) archive_files ( src , changes , out )","title":"archive_changes()"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.copy_dir","text":"Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quiet bool supress non-error output False Source code in simple_uam/util/system/rsync.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @classmethod def copy_dir ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , ): \"\"\" Use rsync to copy a directory incrementally, dst will be the same as src when finished with defaults. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quiet: supress non-error output \"\"\" cls . run ( src = src , dst = dst , exclude = exclude , exclude_from = exclude_from , delete = delete , update = update , progress = progress , verbose = verbose , quiet = quiet , )","title":"copy_dir()"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.list_changes","text":"Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Parameters: Name Type Description Default ref Union [ str , Path ] the reference directory required src Union [ str , Path ] the source dir, to check for changes required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] preserve_dirs bool preserve directories in the output file list. False prune_missing bool remove entries that aren't present in dst. True Source code in simple_uam/util/system/rsync.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 @classmethod def list_changes ( cls , ref : Union [ str , Path ], src : Union [ str , Path ], exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], preserve_dirs : bool = False , prune_missing : bool = True , ) -> List [ Path ]: \"\"\" Use rsync to generate a list of files changed in dst relative to src. Output paths are all given relative to dst. Arguments: ref: the reference directory src: the source dir, to check for changes exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) preserve_dirs: preserve directories in the output file list. prune_missing: remove entries that aren't present in dst. \"\"\" ref = Path ( ref ) src = Path ( src ) . resolve () process = cls . run ( src = ref , dst = src , exclude = exclude , exclude_from = exclude_from , archive = True , delete = True , update = False , progress = False , verbose = False , dry_run = True , itemize_changes = True , capture_output = True , ) process . check_returncode () changes = list () for changed in cls . itemize_regex . findall ( process . stdout ): if changed == \"./\" : continue if not prune_missing or ( src / changed ) . exists (): if preserve_dirs or not changed . endswith ( '/' ): changes . append ( Path ( changed )) log . info ( \"Rsync found following changes during itemizations.\" , ref = str ( ref ), src = str ( src ), # stdout=process.stdout, # args=process.args, return_code = process . returncode , # stderr=process.stderr, changes = [ str ( f ) for f in changes ], ) return changes","title":"list_changes()"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.norm_path","text":"Rsync on windows is weird about paths, this will format an input path appropriately. Parameters: Name Type Description Default path Union [ str , Path ] The Path to format. required is_dir Optional [ bool ] if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. None Returns: Type Description str An absolute path string suitable as a cmd line arg for rsync. Source code in simple_uam/util/system/rsync.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @staticmethod def norm_path ( path : Union [ str , Path ], is_dir : Optional [ bool ] = None ) -> str : \"\"\" Rsync on windows is weird about paths, this will format an input path appropriately. Arguments: path: The Path to format. is_dir: if True appends dir suffix to path. if False does not append suffix, if None will attempt to detect based on existing file. Returns: An absolute path string suitable as a cmd line arg for rsync. \"\"\" path = Path ( path ) . resolve () if is_dir == None and path . exists () and not path . is_dir (): is_dir = False else : is_dir = True path_str = path . as_posix () win_dir = re . match ( r '([A-z]):(.*)' , path_str ) # Uses cygwin style absolute paths for rsync. if win_dir and isinstance ( path , WindowsPath ): path_str = f \"/cygdrive/ { win_dir [ 1 ] }{ win_dir [ 2 ] } \" # Rsync on windows uses the trailing '/' to distingush between a file # and a directory, features like delete don't work if rsync thinks # src is a file. if is_dir and not path_str . endswith ( '/' ): path_str = path_str + '/' return path_str","title":"norm_path()"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.require","text":"Raises an error if rsync isn't installed. Source code in simple_uam/util/system/rsync.py 29 30 31 32 33 34 35 36 37 @staticmethod def require (): \"\"\" Raises an error if rsync isn't installed. \"\"\" if not Rsync . which (): raise RuntimeError ( \"Rsync not found. Please install rsync and ensure it's on PATH.\" )","title":"require()"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.run","text":"Run rsync with args return the completed process object. Parameters: Name Type Description Default src Union [ str , Path ] source dir required dst Union [ str , Path ] destination dir required exclude List [ str ] patterns for list of files to ignore [] exclude_from List [ Union [ str , Path ]] File to read exclude patterns from (great for gitignore) [] archive bool Run in archive mode, True delete bool delete files in dst that aren't in src True update bool don't overwrite files that are newer in dst than src False progress bool show progress bar False verbose bool run in verbose mode False quite supress non-error messages. required dry_run bool make no changes False itemize_changes bool print a list of all changes to stdout False capture_output bool do we capture stdout? False links bool When symlinks are encountered, recreate the symlink on the destination. False recursive bool This tells rsync to copy directories recursively. False times bool preserve modification times. False owner bool preserve owner (super-user only) False group bool preserve group False perms bool preserve permissions False Source code in simple_uam/util/system/rsync.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @classmethod def run ( cls , src : Union [ str , Path ], dst : Union [ str , Path ], * , exclude : List [ str ] = [], exclude_from : List [ Union [ str , Path ]] = [], archive : bool = True , delete : bool = True , update : bool = False , progress : bool = False , verbose : bool = False , quiet : bool = False , dry_run : bool = False , itemize_changes : bool = False , capture_output : bool = False , links : bool = False , times : bool = False , owner : bool = False , group : bool = False , perms : bool = False , recursive : bool = False , ) -> subprocess . CompletedProcess : \"\"\" Run rsync with args return the completed process object. Arguments: src: source dir dst: destination dir exclude: patterns for list of files to ignore exclude_from: File to read exclude patterns from (great for gitignore) archive: Run in archive mode, delete: delete files in dst that aren't in src update: don't overwrite files that are newer in dst than src progress: show progress bar verbose: run in verbose mode quite: supress non-error messages. dry_run: make no changes itemize_changes: print a list of all changes to stdout capture_output: do we capture stdout? links: When symlinks are encountered, recreate the symlink on the destination. recursive: This tells rsync to copy directories recursively. times: preserve modification times. owner: preserve owner (super-user only) group: preserve group perms: preserve permissions \"\"\" Rsync . require () src = Path ( src ) dst = Path ( dst ) if not ( src . exists () and src . is_dir ()): err = RuntimeError ( \"Rsync src isn't a dir.\" ) log . exception ( \"Rsync src argument must be a dir.\" , src = str ( src ), exists = src . exists (), is_dir = src . is_dir (), err = err , ) raise err else : src = src . resolve () if not ( dst . exists () and dst . is_dir ()): err = RuntimeError ( \"Rsync dst isn't a dir.\" ) log . exception ( \"Rsync dst argument must be a dir.\" , dst = str ( dst ), exists = dst . exists (), is_dir = dst . is_dir (), err = err , ) raise err else : dst = dst . resolve () flags = list () if archive : flags . append ( '--archive' ) if recursive : flags . append ( '--recursive' ) if links : flags . append ( '--links' ) if delete : flags . append ( '--delete' ) if update : flags . append ( '--update' ) if owner : flags . append ( '--owner' ) if perms : flags . append ( '--perms' ) if group : flags . append ( '--owner' ) if progress : flags . append ( '--group' ) if times : flags . append ( '--times' ) if verbose : flags . append ( '--verbose' ) if quiet : flags . append ( '--quiet' ) if dry_run : flags . append ( '--dry-run' ) if itemize_changes : flags . append ( '--itemize-changes' ) for pat in exclude : flags . append ( f '--exclude= { pat } ' ) for f in exclude_from : f = cls . norm_path ( Path ( f ) . resolve ()) flags . append ( f '--exclude-from= { f } ' ) rsync_cmd = [ 'rsync' , * flags , cls . norm_path ( src ), cls . norm_path ( dst )] log . info ( \"Running Rsync.\" , cmd = \" \" . join ( rsync_cmd ), ) return subprocess . run ( rsync_cmd , capture_output = capture_output , universal_newlines = True if capture_output else None , )","title":"run()"},{"location":"reference/simple_uam/util/system/rsync/#simple_uam.util.system.rsync.Rsync.which","text":"Returns the rsync executable if it exists or None otherwise. Source code in simple_uam/util/system/rsync.py 21 22 23 24 25 26 27 @staticmethod def which (): \"\"\" Returns the rsync executable if it exists or None otherwise. \"\"\" return shutil . which ( \"rsync\" )","title":"which()"},{"location":"reference/simple_uam/util/system/windows/","text":"append_to_file ( file_path , lines ) \u00b6 Appends lines to file if they aren't already in the file. Source code in simple_uam/util/system/windows.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def append_to_file ( file_path : Path , lines : List [ str ]): \"\"\"Appends lines to file if they aren't already in the file.\"\"\" lines = copy ( lines ) # Filter out lines already in the file with file_path . open ( \"r\" ) as f : file_lines = f . readlines () lines = [ line for line in lines if line not in file_lines ] # Write out new lines if needed if lines != []: # Save Backup backup_file = file_path . with_stem ( file_path . stem + \".bak\" ) backup_file . unlink ( missing_ok = True ) shutil . copy2 ( file_path , backup_file ) # Append Lines with file_path . open ( \"a\" ) as f : lines = [ line + \" \\n \" for line in lines ] f . writelines ( lines ) download_file ( uri , output ) \u00b6 Downloads a file from the uri to a given location. Parameters: Name Type Description Default uri Path The address to download from. required output Path The file_path to download to. required Source code in simple_uam/util/system/windows.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def download_file ( uri : Path , output : Path ): \"\"\" Downloads a file from the uri to a given location. Arguments: uri: The address to download from. output: The file_path to download to. \"\"\" log . info ( \"Downloading file from web\" , uri = uri , file_path = str ( output ), ) subprocess . run ([ \"wget.exe\" , uri , \"-O\" , output , \"--show-progress\" , \"--no-check-certificate\" ]) get_mac_address () \u00b6 Gets the windows machine's mac address via ipconfig /all Source code in simple_uam/util/system/windows.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def get_mac_address () -> str : \"\"\"Gets the windows machine's mac address via `ipconfig /all`\"\"\" ipconfig = subprocess . run ( [ \"ipconfig\" , \"/all\" ], capture_output = True , universal_newlines = True , ) regex = re . compile ( r \"\\s+Physical Address[.\\s]+:\\s(([\\dA-F] {2} -) {5} [\\dA-F] {2} )\" , flags = re . S & re . MULTILINE , ) match = regex . search ( ipconfig . stdout ) if match : return match . group ( 1 ) else : log . warning ( \"Could not find mac address in ipconfig output.\" , regex = regex , match = match , result = ipconfig . stdout , ) raise RuntimeException ( \"Could not find mac address in ipconfig output.\" ) run_gui_exe ( exe , notes = None , wait = True ) \u00b6 Runs the (windows) gui executable exe after displaying notes Source code in simple_uam/util/system/windows.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def run_gui_exe ( exe : Path , notes : Optional [ str ] = None , wait : bool = True ): \"\"\"Runs the (windows) gui executable `exe` after displaying `notes`\"\"\" log . info ( \"Running GUI Executable\" , executable = exe , ) if notes : print ( textwrap . dedent ( notes )) if wait : input ( f \" \\n\\n Hit Enter To Start Executable: { exe } \" ) subprocess . run ([ \"powershell\" , \"-Command\" , f '& {{ Start-Process \" { exe } \" -Wait }} ' ]) unpack_file ( archive , output ) \u00b6 Unpacks archive to output directory. Source code in simple_uam/util/system/windows.py 50 51 52 53 54 55 56 57 58 59 def unpack_file ( archive : Path , output : Path ): \"\"\"Unpacks archive to output directory.\"\"\" log . info ( \"Unpacking archive\" , archive = str ( archive ), output = str ( output ), ) subprocess . run ([ \"7z\" , \"x\" , archive , f \"-o { str ( output ) } \" ]) verify_file ( input , md5 ) \u00b6 Verify that the file at input has the given md5 hash. Source code in simple_uam/util/system/windows.py 36 37 38 39 40 41 42 43 44 45 46 47 def verify_file ( input : Path , md5 : str ): \"\"\" Verify that the file at input has the given md5 hash. \"\"\" log . info ( \"Verifying file checksum (sorry no progress bar for this one)\" , file_path = str ( input ), md5 = md5 , ) proc = subprocess . run ([ \"checksum\" , input , f \"-c= { md5 } \" ]) return proc . returncode == 0","title":"windows"},{"location":"reference/simple_uam/util/system/windows/#simple_uam.util.system.windows.append_to_file","text":"Appends lines to file if they aren't already in the file. Source code in simple_uam/util/system/windows.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def append_to_file ( file_path : Path , lines : List [ str ]): \"\"\"Appends lines to file if they aren't already in the file.\"\"\" lines = copy ( lines ) # Filter out lines already in the file with file_path . open ( \"r\" ) as f : file_lines = f . readlines () lines = [ line for line in lines if line not in file_lines ] # Write out new lines if needed if lines != []: # Save Backup backup_file = file_path . with_stem ( file_path . stem + \".bak\" ) backup_file . unlink ( missing_ok = True ) shutil . copy2 ( file_path , backup_file ) # Append Lines with file_path . open ( \"a\" ) as f : lines = [ line + \" \\n \" for line in lines ] f . writelines ( lines )","title":"append_to_file()"},{"location":"reference/simple_uam/util/system/windows/#simple_uam.util.system.windows.download_file","text":"Downloads a file from the uri to a given location. Parameters: Name Type Description Default uri Path The address to download from. required output Path The file_path to download to. required Source code in simple_uam/util/system/windows.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def download_file ( uri : Path , output : Path ): \"\"\" Downloads a file from the uri to a given location. Arguments: uri: The address to download from. output: The file_path to download to. \"\"\" log . info ( \"Downloading file from web\" , uri = uri , file_path = str ( output ), ) subprocess . run ([ \"wget.exe\" , uri , \"-O\" , output , \"--show-progress\" , \"--no-check-certificate\" ])","title":"download_file()"},{"location":"reference/simple_uam/util/system/windows/#simple_uam.util.system.windows.get_mac_address","text":"Gets the windows machine's mac address via ipconfig /all Source code in simple_uam/util/system/windows.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def get_mac_address () -> str : \"\"\"Gets the windows machine's mac address via `ipconfig /all`\"\"\" ipconfig = subprocess . run ( [ \"ipconfig\" , \"/all\" ], capture_output = True , universal_newlines = True , ) regex = re . compile ( r \"\\s+Physical Address[.\\s]+:\\s(([\\dA-F] {2} -) {5} [\\dA-F] {2} )\" , flags = re . S & re . MULTILINE , ) match = regex . search ( ipconfig . stdout ) if match : return match . group ( 1 ) else : log . warning ( \"Could not find mac address in ipconfig output.\" , regex = regex , match = match , result = ipconfig . stdout , ) raise RuntimeException ( \"Could not find mac address in ipconfig output.\" )","title":"get_mac_address()"},{"location":"reference/simple_uam/util/system/windows/#simple_uam.util.system.windows.run_gui_exe","text":"Runs the (windows) gui executable exe after displaying notes Source code in simple_uam/util/system/windows.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def run_gui_exe ( exe : Path , notes : Optional [ str ] = None , wait : bool = True ): \"\"\"Runs the (windows) gui executable `exe` after displaying `notes`\"\"\" log . info ( \"Running GUI Executable\" , executable = exe , ) if notes : print ( textwrap . dedent ( notes )) if wait : input ( f \" \\n\\n Hit Enter To Start Executable: { exe } \" ) subprocess . run ([ \"powershell\" , \"-Command\" , f '& {{ Start-Process \" { exe } \" -Wait }} ' ])","title":"run_gui_exe()"},{"location":"reference/simple_uam/util/system/windows/#simple_uam.util.system.windows.unpack_file","text":"Unpacks archive to output directory. Source code in simple_uam/util/system/windows.py 50 51 52 53 54 55 56 57 58 59 def unpack_file ( archive : Path , output : Path ): \"\"\"Unpacks archive to output directory.\"\"\" log . info ( \"Unpacking archive\" , archive = str ( archive ), output = str ( output ), ) subprocess . run ([ \"7z\" , \"x\" , archive , f \"-o { str ( output ) } \" ])","title":"unpack_file()"},{"location":"reference/simple_uam/util/system/windows/#simple_uam.util.system.windows.verify_file","text":"Verify that the file at input has the given md5 hash. Source code in simple_uam/util/system/windows.py 36 37 38 39 40 41 42 43 44 45 46 47 def verify_file ( input : Path , md5 : str ): \"\"\" Verify that the file at input has the given md5 hash. \"\"\" log . info ( \"Verifying file checksum (sorry no progress bar for this one)\" , file_path = str ( input ), md5 = md5 , ) proc = subprocess . run ([ \"checksum\" , input , f \"-c= { md5 } \" ]) return proc . returncode == 0","title":"verify_file()"},{"location":"reference/simple_uam/worker/","text":"SimpleUAM windows node setup scripts. actor ( fn = None , * , actor_class = Actor , actor_name = None , queue_name = 'default' , priority = 0 , ** options ) \u00b6 Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor Source code in simple_uam/worker/broker.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def actor ( fn = None , * , actor_class = Actor , actor_name = None , queue_name = 'default' , priority = 0 , ** options ): \"\"\" Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes: - Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. - There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor \"\"\" actor_name = actor_name or fn . __name__ actor_name = f \" { fn . __module__ } : { actor_name } \" return dramatiq . actor ( fn = fn , actor_class = actor_class , actor_name = actor_name , queue_name = queue_name , priority = priority , broker = _BROKER , ** options , ) message_metadata () \u00b6 When called in a running actor provides a dictionary of metadata from the message being processed. Source code in simple_uam/worker/broker.py 95 96 97 98 99 100 101 102 103 104 105 106 107 def message_metadata (): \"\"\" When called in a running actor provides a dictionary of metadata from the message being processed. \"\"\" msg = CurrentMessage . get_current_message () return dict ( actor_name = msg . actor_name , message_id = msg . message_id , message_timestamp = msg . message_timestamp , ) run_worker_node ( modules , processes , threads , shutdown_timeout = 600000 , skip_logging = False , verbose = 0 ) \u00b6 Runs a worker node with various settings. Parameters: Name Type Description Default modules list List of modules to load, either the object or their names. required processes int Number of forked processes required threads int Number of forked threads per process required shutdown_timeout int Worker shutdown timeout in milliseconds. 600000 skip_logging bool skip dramatiq's orthogonal logging process. False verbose int Controls verbosity of dramatiq worker. 0 Source code in simple_uam/worker/run_worker.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def run_worker_node ( modules : list , processes : int , threads : int , shutdown_timeout : int = 600000 , skip_logging : bool = False , verbose : int = 0 ): \"\"\" Runs a worker node with various settings. Arguments: modules: List of modules to load, either the object or their names. processes: Number of forked processes threads: Number of forked threads per process shutdown_timeout: Worker shutdown timeout in milliseconds. skip_logging: skip dramatiq's orthogonal logging process. verbose: Controls verbosity of dramatiq worker. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) cli_args = RunArgs ( processes = processes , threads = threads , worker_shutdown_timeout = shutdown_timeout , skip_logging = skip_logging , verbose = verbose , modules = [ getattr ( m , '__name__' , m ) for m in modules ], ) log . info ( \"Running dramatiq worker with args.\" , ** asdict ( cli_args ), ) return main ( args = cli_args )","title":"worker"},{"location":"reference/simple_uam/worker/#simple_uam.worker.actor","text":"Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor Source code in simple_uam/worker/broker.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def actor ( fn = None , * , actor_class = Actor , actor_name = None , queue_name = 'default' , priority = 0 , ** options ): \"\"\" Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes: - Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. - There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor \"\"\" actor_name = actor_name or fn . __name__ actor_name = f \" { fn . __module__ } : { actor_name } \" return dramatiq . actor ( fn = fn , actor_class = actor_class , actor_name = actor_name , queue_name = queue_name , priority = priority , broker = _BROKER , ** options , )","title":"actor()"},{"location":"reference/simple_uam/worker/#simple_uam.worker.message_metadata","text":"When called in a running actor provides a dictionary of metadata from the message being processed. Source code in simple_uam/worker/broker.py 95 96 97 98 99 100 101 102 103 104 105 106 107 def message_metadata (): \"\"\" When called in a running actor provides a dictionary of metadata from the message being processed. \"\"\" msg = CurrentMessage . get_current_message () return dict ( actor_name = msg . actor_name , message_id = msg . message_id , message_timestamp = msg . message_timestamp , )","title":"message_metadata()"},{"location":"reference/simple_uam/worker/#simple_uam.worker.run_worker_node","text":"Runs a worker node with various settings. Parameters: Name Type Description Default modules list List of modules to load, either the object or their names. required processes int Number of forked processes required threads int Number of forked threads per process required shutdown_timeout int Worker shutdown timeout in milliseconds. 600000 skip_logging bool skip dramatiq's orthogonal logging process. False verbose int Controls verbosity of dramatiq worker. 0 Source code in simple_uam/worker/run_worker.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def run_worker_node ( modules : list , processes : int , threads : int , shutdown_timeout : int = 600000 , skip_logging : bool = False , verbose : int = 0 ): \"\"\" Runs a worker node with various settings. Arguments: modules: List of modules to load, either the object or their names. processes: Number of forked processes threads: Number of forked threads per process shutdown_timeout: Worker shutdown timeout in milliseconds. skip_logging: skip dramatiq's orthogonal logging process. verbose: Controls verbosity of dramatiq worker. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) cli_args = RunArgs ( processes = processes , threads = threads , worker_shutdown_timeout = shutdown_timeout , skip_logging = skip_logging , verbose = verbose , modules = [ getattr ( m , '__name__' , m ) for m in modules ], ) log . info ( \"Running dramatiq worker with args.\" , ** asdict ( cli_args ), ) return main ( args = cli_args )","title":"run_worker_node()"},{"location":"reference/simple_uam/worker/broker/","text":"actor ( fn = None , * , actor_class = Actor , actor_name = None , queue_name = 'default' , priority = 0 , ** options ) \u00b6 Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor Source code in simple_uam/worker/broker.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def actor ( fn = None , * , actor_class = Actor , actor_name = None , queue_name = 'default' , priority = 0 , ** options ): \"\"\" Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes: - Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. - There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor \"\"\" actor_name = actor_name or fn . __name__ actor_name = f \" { fn . __module__ } : { actor_name } \" return dramatiq . actor ( fn = fn , actor_class = actor_class , actor_name = actor_name , queue_name = queue_name , priority = priority , broker = _BROKER , ** options , ) default_broker () \u00b6 Creates a new broker as specified by the config files Source code in simple_uam/worker/broker.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def default_broker (): \"\"\" Creates a new broker as specified by the config files \"\"\" url = Config [ D2CWorkerConfig ] . broker . url parsed = urlparse ( url ) broker = None ### Setup Broker ### if parsed . scheme == 'amqp' : broker = RabbitmqBroker ( url = url ) elif parsed . scheme == 'redis' : broker = RedisBroker ( url = url ) else : err = RuntimeError ( \"Unsupported broker protocol.\" ) log . exception ( \"Broker schema must be 'amqp' (for rabbitmp) or 'redis' (for redis).\" , url = url , schema = parsed . scheme , ) raise err broker . add_middleware ( CurrentMessage ()) ### Setup Results Backend ### if Config [ D2CWorkerConfig ] . backend . enabled : backend = RedisBackend ( url = Config [ D2CWorkerConfig ] . backend . url ) broker . add_middleware ( Results ( backend = backend )) return broker message_metadata () \u00b6 When called in a running actor provides a dictionary of metadata from the message being processed. Source code in simple_uam/worker/broker.py 95 96 97 98 99 100 101 102 103 104 105 106 107 def message_metadata (): \"\"\" When called in a running actor provides a dictionary of metadata from the message being processed. \"\"\" msg = CurrentMessage . get_current_message () return dict ( actor_name = msg . actor_name , message_id = msg . message_id , message_timestamp = msg . message_timestamp , )","title":"broker"},{"location":"reference/simple_uam/worker/broker/#simple_uam.worker.broker.actor","text":"Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor Source code in simple_uam/worker/broker.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def actor ( fn = None , * , actor_class = Actor , actor_name = None , queue_name = 'default' , priority = 0 , ** options ): \"\"\" Wraps 'dramatiq.actor', so the options and defaults are mostly the same. Changes: - Default actor names are distinguished by modules. This applies even when a specific name is given. If you want to have complete control use 'dramatiq.actor' directly. - There is no 'broker' option, it's fixed to the one initialized in this modules. See: https://dramatiq.io/reference.html#dramatiq.actor \"\"\" actor_name = actor_name or fn . __name__ actor_name = f \" { fn . __module__ } : { actor_name } \" return dramatiq . actor ( fn = fn , actor_class = actor_class , actor_name = actor_name , queue_name = queue_name , priority = priority , broker = _BROKER , ** options , )","title":"actor()"},{"location":"reference/simple_uam/worker/broker/#simple_uam.worker.broker.default_broker","text":"Creates a new broker as specified by the config files Source code in simple_uam/worker/broker.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def default_broker (): \"\"\" Creates a new broker as specified by the config files \"\"\" url = Config [ D2CWorkerConfig ] . broker . url parsed = urlparse ( url ) broker = None ### Setup Broker ### if parsed . scheme == 'amqp' : broker = RabbitmqBroker ( url = url ) elif parsed . scheme == 'redis' : broker = RedisBroker ( url = url ) else : err = RuntimeError ( \"Unsupported broker protocol.\" ) log . exception ( \"Broker schema must be 'amqp' (for rabbitmp) or 'redis' (for redis).\" , url = url , schema = parsed . scheme , ) raise err broker . add_middleware ( CurrentMessage ()) ### Setup Results Backend ### if Config [ D2CWorkerConfig ] . backend . enabled : backend = RedisBackend ( url = Config [ D2CWorkerConfig ] . backend . url ) broker . add_middleware ( Results ( backend = backend )) return broker","title":"default_broker()"},{"location":"reference/simple_uam/worker/broker/#simple_uam.worker.broker.message_metadata","text":"When called in a running actor provides a dictionary of metadata from the message being processed. Source code in simple_uam/worker/broker.py 95 96 97 98 99 100 101 102 103 104 105 106 107 def message_metadata (): \"\"\" When called in a running actor provides a dictionary of metadata from the message being processed. \"\"\" msg = CurrentMessage . get_current_message () return dict ( actor_name = msg . actor_name , message_id = msg . message_id , message_timestamp = msg . message_timestamp , )","title":"message_metadata()"},{"location":"reference/simple_uam/worker/run_worker/","text":"RunArgs \u00b6 Proxy for the ParsedArguments that dramatiq.cli.main expects as input. Source code in simple_uam/worker/run_worker.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @define class RunArgs (): \"\"\" Proxy for the ParsedArguments that dramatiq.cli.main expects as input. \"\"\" processes : int = field () threads : int = field () worker_shutdown_timeout : int = field () skip_logging : bool = field () path : List [ str ] = field ( factory = list ) use_spawn : bool = field ( default = False ) pid_file : Optional [ str ] = field ( default = None ) forks : List [ str ] = field ( factory = list ) verbose : int = field ( default = 0 ) watch : Optional [ str ] = field ( default = None ) log_file : Optional [ str ] = field ( default = None ) broker : Optional [ str ] = field ( default = \"simple_uam.worker.broker\" ) modules : List [ str ] = field ( factory = list ) queues : List [ str ] = field ( factory = list ) run_worker_node ( modules , processes , threads , shutdown_timeout = 600000 , skip_logging = False , verbose = 0 ) \u00b6 Runs a worker node with various settings. Parameters: Name Type Description Default modules list List of modules to load, either the object or their names. required processes int Number of forked processes required threads int Number of forked threads per process required shutdown_timeout int Worker shutdown timeout in milliseconds. 600000 skip_logging bool skip dramatiq's orthogonal logging process. False verbose int Controls verbosity of dramatiq worker. 0 Source code in simple_uam/worker/run_worker.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def run_worker_node ( modules : list , processes : int , threads : int , shutdown_timeout : int = 600000 , skip_logging : bool = False , verbose : int = 0 ): \"\"\" Runs a worker node with various settings. Arguments: modules: List of modules to load, either the object or their names. processes: Number of forked processes threads: Number of forked threads per process shutdown_timeout: Worker shutdown timeout in milliseconds. skip_logging: skip dramatiq's orthogonal logging process. verbose: Controls verbosity of dramatiq worker. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) cli_args = RunArgs ( processes = processes , threads = threads , worker_shutdown_timeout = shutdown_timeout , skip_logging = skip_logging , verbose = verbose , modules = [ getattr ( m , '__name__' , m ) for m in modules ], ) log . info ( \"Running dramatiq worker with args.\" , ** asdict ( cli_args ), ) return main ( args = cli_args )","title":"run_worker"},{"location":"reference/simple_uam/worker/run_worker/#simple_uam.worker.run_worker.RunArgs","text":"Proxy for the ParsedArguments that dramatiq.cli.main expects as input. Source code in simple_uam/worker/run_worker.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @define class RunArgs (): \"\"\" Proxy for the ParsedArguments that dramatiq.cli.main expects as input. \"\"\" processes : int = field () threads : int = field () worker_shutdown_timeout : int = field () skip_logging : bool = field () path : List [ str ] = field ( factory = list ) use_spawn : bool = field ( default = False ) pid_file : Optional [ str ] = field ( default = None ) forks : List [ str ] = field ( factory = list ) verbose : int = field ( default = 0 ) watch : Optional [ str ] = field ( default = None ) log_file : Optional [ str ] = field ( default = None ) broker : Optional [ str ] = field ( default = \"simple_uam.worker.broker\" ) modules : List [ str ] = field ( factory = list ) queues : List [ str ] = field ( factory = list )","title":"RunArgs"},{"location":"reference/simple_uam/worker/run_worker/#simple_uam.worker.run_worker.run_worker_node","text":"Runs a worker node with various settings. Parameters: Name Type Description Default modules list List of modules to load, either the object or their names. required processes int Number of forked processes required threads int Number of forked threads per process required shutdown_timeout int Worker shutdown timeout in milliseconds. 600000 skip_logging bool skip dramatiq's orthogonal logging process. False verbose int Controls verbosity of dramatiq worker. 0 Source code in simple_uam/worker/run_worker.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def run_worker_node ( modules : list , processes : int , threads : int , shutdown_timeout : int = 600000 , skip_logging : bool = False , verbose : int = 0 ): \"\"\" Runs a worker node with various settings. Arguments: modules: List of modules to load, either the object or their names. processes: Number of forked processes threads: Number of forked threads per process shutdown_timeout: Worker shutdown timeout in milliseconds. skip_logging: skip dramatiq's orthogonal logging process. verbose: Controls verbosity of dramatiq worker. \"\"\" broker = get_broker () log . info ( \"Setting up dramatiq broker.\" , type = type ( broker ) . __name__ , actors = broker . get_declared_actors (), queues = broker . get_declared_queues (), middleware = [ type ( m ) . __name__ for m in broker . middleware ], ) cli_args = RunArgs ( processes = processes , threads = threads , worker_shutdown_timeout = shutdown_timeout , skip_logging = skip_logging , verbose = verbose , modules = [ getattr ( m , '__name__' , m ) for m in modules ], ) log . info ( \"Running dramatiq worker with args.\" , ** asdict ( cli_args ), ) return main ( args = cli_args )","title":"run_worker_node()"},{"location":"reference/simple_uam/workspace/","text":"SimpleUAM windows node setup scripts. Session \u00b6 An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. Source code in simple_uam/workspace/session.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 @define class Session (): \"\"\" An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. \"\"\" number : int = field ( kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" reference_dir : Path = field ( kw_only = True , ) \"\"\" The reference directory that the work_dir is copied from. \"\"\" @reference_dir . validator def _ref_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) work_dir : Path = field ( kw_only = True , ) \"\"\" The directory in which the session runs. \"\"\" @work_dir . validator def _work_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) _result_archive : Path = field ( kw_only = True , ) \"\"\" Where the result archive should be written to, after session completion the workspace should update this to the final archive location. \"\"\" @_result_archive . validator def _result_archive_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) @property def result_archive ( self ): return self . _result_archive @result_archive . setter def result_archive ( self , val ): self . metadata [ 'result_archive' ] = str ( val ) self . _result_archive = val name : str = field ( default = \"generic-session\" , kw_only = True , ) \"\"\" A name for the type of session being performed. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace. \"\"\" metadata_file : Path = field ( default = Path ( \"metadata.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores the current metadata. \"\"\" log_file : Path = field ( default = Path ( \"log.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores logs. \"\"\" @metadata_file . validator def _meta_file_valid ( self , attr , val ): if val . is_absolute (): raise RuntimeError ( \"The metadata file must be specified relative to the workdir.\" ) init_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace initialization. \"\"\" @init_exclude_patterns . default def _init_exclude_pats_def ( self ): return [ \".git\" ] result_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace result archive. \"\"\" @result_exclude_patterns . default def _result_exclude_pats_def ( self ): return [ \".git\" ] old_work_dir : Optional [ Path ] = field ( default = None , init = False , ) \"\"\" The working directory before the start of the session, to be restored after the session. \"\"\" start_time : datetime = field ( factory = datetime . now , init = False , ) \"\"\" The time when the session object was created/started. \"\"\" def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data ) @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs ) @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir ) @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" ) @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args ) @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" ) init_exclude_patterns : List [ str ] = field ( kw_only = True ) class-attribute \u00b6 Patterns to exclude from workspace initialization. log_file : Path = field ( default = Path ( 'log.json' ), kw_only = True ) class-attribute \u00b6 The file in the workdir (and eventually in the results archive) that stores logs. metadata : Dict = field ( factory = dict , kw_only = True ) class-attribute \u00b6 The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace. metadata_file : Path = field ( default = Path ( 'metadata.json' ), kw_only = True ) class-attribute \u00b6 The file in the workdir (and eventually in the results archive) that stores the current metadata. name : str = field ( default = 'generic-session' , kw_only = True ) class-attribute \u00b6 A name for the type of session being performed. number : int = field ( kw_only = True ) class-attribute \u00b6 The number of the workspace this object is operating with. old_work_dir : Optional [ Path ] = field ( default = None , init = False ) class-attribute \u00b6 The working directory before the start of the session, to be restored after the session. reference_dir : Path = field ( kw_only = True ) class-attribute \u00b6 The reference directory that the work_dir is copied from. result_exclude_patterns : List [ str ] = field ( kw_only = True ) class-attribute \u00b6 Patterns to exclude from workspace result archive. start_time : datetime = field ( factory = datetime . now , init = False ) class-attribute \u00b6 The time when the session object was created/started. work_dir : Path = field ( kw_only = True ) class-attribute \u00b6 The directory in which the session runs. enter_workdir () \u00b6 Changes the current working dir of the session to the workspace. Source code in simple_uam/workspace/session.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir ) exit_workdir () \u00b6 Reset the work directory to what it was before enter_workdir. Source code in simple_uam/workspace/session.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None generate_result_archive () \u00b6 Creates the result archive from the current working directory as it is. Source code in simple_uam/workspace/session.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args ) log_exception ( * excs , exc_type = None , exc_val = None , exc_tb = None ) \u00b6 Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Parameters: Name Type Description Default *exc list of exceptions to process, mutually exclusive with following args. required exc_type The type of the exception. None exc_val The value of the exception. None exc_tb The traceback of the exception. None Source code in simple_uam/workspace/session.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data ) reset_workspace ( progress = True , verbose = False , quiet = False ) \u00b6 Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Parameters: Name Type Description Default progress bool show a progress bar. True verbose bool rsync verbose output. False quiet bool perform the copy silently. False Source code in simple_uam/workspace/session.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) run ( * vargs , cwd = None , ** kwargs ) \u00b6 Identical to subprocess.run except the working directory is set automatically to the workspace directory. Source code in simple_uam/workspace/session.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs ) session_info () \u00b6 Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. Source code in simple_uam/workspace/session.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info validate_complete () \u00b6 Runs any checks we need to ensure that we can close down the session. Source code in simple_uam/workspace/session.py 424 425 426 427 428 429 430 431 432 433 434 435 436 @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" ) write_metadata () \u00b6 Writes the current metadata to a file in the working directory. Source code in simple_uam/workspace/session.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" ) Workspace \u00b6 A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. Source code in simple_uam/workspace/workspace.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 @define class Workspace (): \"\"\" A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. \"\"\" name : str = field ( default = \"generic-session\" , ) \"\"\" A name for the type of session being performed. \"\"\" number : Optional [ int ] = field ( default = None , converter = converters . optional ( int ), kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions. \"\"\" manager : WorkspaceManager = field ( on_setattr = setters . frozen , kw_only = True , ) \"\"\" The manager object that holds all the locks. \"\"\" config : WorkspaceConfig = field ( init = False , on_setattr = setters . frozen , ) \"\"\" The workspace config object that collects all the relevant paths. \"\"\" @config . default def _config_def ( self ): return self . manager . config session_class : Type [ Session ] = field ( default = Session , kw_only = True , on_setattr = setters . frozen , ) \"\"\" The class we're using to generate the active session context. \"\"\" active_session : Optional [ Session ] = field ( default = None , init = False , ) \"\"\" The current session context. Should be non-None after session_started. \"\"\" active_workspace : Optional [ int ] = field ( default = None , init = False , ) \"\"\" The current active workspace within a session. \"\"\" active_lock : Optional [ FileLock ] = field ( default = None , init = False , ) \"\"\" The FileLock for the current workspace. \"\"\" active_temp_dir : Optional [ tempfile . TemporaryDirectory ] = field ( default = None , init = False , ) \"\"\" The temporary directory the result archive is written to before it's moved into the results directory. \"\"\" def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session def __exit__ ( self , exp_type , exp_val , exp_traceback ): self . active_session . log_exception ( exc_type = exc_type , exc_val = exc_val , exc_tb = exc_traceback , ) self . active_session . exit_workdir () self . finish () return None active_lock : Optional [ FileLock ] = field ( default = None , init = False ) class-attribute \u00b6 The FileLock for the current workspace. active_session : Optional [ Session ] = field ( default = None , init = False ) class-attribute \u00b6 The current session context. Should be non-None after session_started. active_temp_dir : Optional [ tempfile . TemporaryDirectory ] = field ( default = None , init = False ) class-attribute \u00b6 The temporary directory the result archive is written to before it's moved into the results directory. active_workspace : Optional [ int ] = field ( default = None , init = False ) class-attribute \u00b6 The current active workspace within a session. config : WorkspaceConfig = field ( init = False , on_setattr = setters . frozen ) class-attribute \u00b6 The workspace config object that collects all the relevant paths. manager : WorkspaceManager = field ( on_setattr = setters . frozen , kw_only = True ) class-attribute \u00b6 The manager object that holds all the locks. metadata : Dict = field ( factory = dict , kw_only = True ) class-attribute \u00b6 The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions. name : str = field ( default = 'generic-session' ) class-attribute \u00b6 A name for the type of session being performed. number : Optional [ int ] = field ( default = None , converter = converters . optional ( int ), kw_only = True ) class-attribute \u00b6 The number of the workspace this object is operating with. session_class : Type [ Session ] = field ( default = Session , kw_only = True , on_setattr = setters . frozen ) class-attribute \u00b6 The class we're using to generate the active session context. __enter__ () \u00b6 Provides a context manager you can use to do various tasks within the Workspace. workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) Source code in simple_uam/workspace/workspace.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session finish () \u00b6 Finishes an active session. Source code in simple_uam/workspace/workspace.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None start () \u00b6 Starts the session, meant to be used in a try-finally style. workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() Note that this won't change the working directory, unlike when a workspace is used as a context managed. Source code in simple_uam/workspace/workspace.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session WorkspaceManager \u00b6 Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. Source code in simple_uam/workspace/manager.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @frozen class WorkspaceManager (): \"\"\" Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. \"\"\" config : WorkspaceConfig = field () \"\"\" The workspace config object that collects all the relevant paths. \"\"\" def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile ) def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile ) def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num )) def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True ) def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True ) def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" ) config : WorkspaceConfig = field () class-attribute \u00b6 The workspace config object that collects all the relevant paths. acquire_workspace_lock ( num = None ) \u00b6 Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. Source code in simple_uam/workspace/manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None add_result ( archive , prefix = None , suffix = None , copy = True ) \u00b6 Adds an archive to the result directory. Parameters: Name Type Description Default archive Union [ str , Path ] The file to be moved into the result dir. required prefix Optional [ str ] The prefix to the final file name. None suffix Optional [ str ] The file extension to use. None copy bool copy the file if true, otherwise move. True Returns: Type Description Optional [ Path ] The path to the resulting file in results dir, none if Optional [ Path ] no result was saved. Source code in simple_uam/workspace/manager.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path delete_locks ( skip_reference = False , skip_results = False ) \u00b6 Deletes all the locks that this WorkspaceManager can interact with. Parameters: Name Type Description Default skip_reference If true, skip deleting the reference lockfile. False skip_results If true, skip deleting the results lock. False Source code in simple_uam/workspace/manager.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True ) init_dirs () \u00b6 Creates the various workspace directories and subdirs that this class manages. Source code in simple_uam/workspace/manager.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True ) init_ref_dir ( reference_dir , assets_dir ) \u00b6 This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by Source code in simple_uam/workspace/manager.py 255 256 257 258 259 260 261 262 263 264 def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" ) prune_results () \u00b6 Deletes the oldest files in the results dir if there are too many. Source code in simple_uam/workspace/manager.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass reference_lock () \u00b6 Lockfile for reference directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 31 32 33 34 35 36 37 38 39 def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile ) results_lock () \u00b6 Lockfile for the results storage directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 41 42 43 44 45 46 47 48 def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile ) setup_reference_dir ( ** kwargs ) \u00b6 Wraps init_ref_dir with appropriate locks and file deletion. Parameters: Name Type Description Default **kwargs Passed to the init_ref_dir call. {} Source code in simple_uam/workspace/manager.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err workspace_lock ( num ) \u00b6 Lockfile for single workspace directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 50 51 52 53 54 55 56 57 def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num ))","title":"workspace"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.Session","text":"An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. Source code in simple_uam/workspace/session.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 @define class Session (): \"\"\" An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. \"\"\" number : int = field ( kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" reference_dir : Path = field ( kw_only = True , ) \"\"\" The reference directory that the work_dir is copied from. \"\"\" @reference_dir . validator def _ref_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) work_dir : Path = field ( kw_only = True , ) \"\"\" The directory in which the session runs. \"\"\" @work_dir . validator def _work_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) _result_archive : Path = field ( kw_only = True , ) \"\"\" Where the result archive should be written to, after session completion the workspace should update this to the final archive location. \"\"\" @_result_archive . validator def _result_archive_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) @property def result_archive ( self ): return self . _result_archive @result_archive . setter def result_archive ( self , val ): self . metadata [ 'result_archive' ] = str ( val ) self . _result_archive = val name : str = field ( default = \"generic-session\" , kw_only = True , ) \"\"\" A name for the type of session being performed. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace. \"\"\" metadata_file : Path = field ( default = Path ( \"metadata.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores the current metadata. \"\"\" log_file : Path = field ( default = Path ( \"log.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores logs. \"\"\" @metadata_file . validator def _meta_file_valid ( self , attr , val ): if val . is_absolute (): raise RuntimeError ( \"The metadata file must be specified relative to the workdir.\" ) init_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace initialization. \"\"\" @init_exclude_patterns . default def _init_exclude_pats_def ( self ): return [ \".git\" ] result_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace result archive. \"\"\" @result_exclude_patterns . default def _result_exclude_pats_def ( self ): return [ \".git\" ] old_work_dir : Optional [ Path ] = field ( default = None , init = False , ) \"\"\" The working directory before the start of the session, to be restored after the session. \"\"\" start_time : datetime = field ( factory = datetime . now , init = False , ) \"\"\" The time when the session object was created/started. \"\"\" def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data ) @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs ) @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir ) @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" ) @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args ) @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" )","title":"Session"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.init_exclude_patterns","text":"Patterns to exclude from workspace initialization.","title":"init_exclude_patterns"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.log_file","text":"The file in the workdir (and eventually in the results archive) that stores logs.","title":"log_file"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.metadata","text":"The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace.","title":"metadata"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.metadata_file","text":"The file in the workdir (and eventually in the results archive) that stores the current metadata.","title":"metadata_file"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.name","text":"A name for the type of session being performed.","title":"name"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.number","text":"The number of the workspace this object is operating with.","title":"number"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.old_work_dir","text":"The working directory before the start of the session, to be restored after the session.","title":"old_work_dir"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.reference_dir","text":"The reference directory that the work_dir is copied from.","title":"reference_dir"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.result_exclude_patterns","text":"Patterns to exclude from workspace result archive.","title":"result_exclude_patterns"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.start_time","text":"The time when the session object was created/started.","title":"start_time"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.work_dir","text":"The directory in which the session runs.","title":"work_dir"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.enter_workdir","text":"Changes the current working dir of the session to the workspace. Source code in simple_uam/workspace/session.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir )","title":"enter_workdir()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.exit_workdir","text":"Reset the work directory to what it was before enter_workdir. Source code in simple_uam/workspace/session.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None","title":"exit_workdir()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.generate_result_archive","text":"Creates the result archive from the current working directory as it is. Source code in simple_uam/workspace/session.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args )","title":"generate_result_archive()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.log_exception","text":"Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Parameters: Name Type Description Default *exc list of exceptions to process, mutually exclusive with following args. required exc_type The type of the exception. None exc_val The value of the exception. None exc_tb The traceback of the exception. None Source code in simple_uam/workspace/session.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data )","title":"log_exception()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.reset_workspace","text":"Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Parameters: Name Type Description Default progress bool show a progress bar. True verbose bool rsync verbose output. False quiet bool perform the copy silently. False Source code in simple_uam/workspace/session.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args )","title":"reset_workspace()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.run","text":"Identical to subprocess.run except the working directory is set automatically to the workspace directory. Source code in simple_uam/workspace/session.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs )","title":"run()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.session_info","text":"Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. Source code in simple_uam/workspace/session.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info","title":"session_info()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.validate_complete","text":"Runs any checks we need to ensure that we can close down the session. Source code in simple_uam/workspace/session.py 424 425 426 427 428 429 430 431 432 433 434 435 436 @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" )","title":"validate_complete()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.session.Session.write_metadata","text":"Writes the current metadata to a file in the working directory. Source code in simple_uam/workspace/session.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" )","title":"write_metadata()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.Workspace","text":"A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. Source code in simple_uam/workspace/workspace.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 @define class Workspace (): \"\"\" A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. \"\"\" name : str = field ( default = \"generic-session\" , ) \"\"\" A name for the type of session being performed. \"\"\" number : Optional [ int ] = field ( default = None , converter = converters . optional ( int ), kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions. \"\"\" manager : WorkspaceManager = field ( on_setattr = setters . frozen , kw_only = True , ) \"\"\" The manager object that holds all the locks. \"\"\" config : WorkspaceConfig = field ( init = False , on_setattr = setters . frozen , ) \"\"\" The workspace config object that collects all the relevant paths. \"\"\" @config . default def _config_def ( self ): return self . manager . config session_class : Type [ Session ] = field ( default = Session , kw_only = True , on_setattr = setters . frozen , ) \"\"\" The class we're using to generate the active session context. \"\"\" active_session : Optional [ Session ] = field ( default = None , init = False , ) \"\"\" The current session context. Should be non-None after session_started. \"\"\" active_workspace : Optional [ int ] = field ( default = None , init = False , ) \"\"\" The current active workspace within a session. \"\"\" active_lock : Optional [ FileLock ] = field ( default = None , init = False , ) \"\"\" The FileLock for the current workspace. \"\"\" active_temp_dir : Optional [ tempfile . TemporaryDirectory ] = field ( default = None , init = False , ) \"\"\" The temporary directory the result archive is written to before it's moved into the results directory. \"\"\" def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session def __exit__ ( self , exp_type , exp_val , exp_traceback ): self . active_session . log_exception ( exc_type = exc_type , exc_val = exc_val , exc_tb = exc_traceback , ) self . active_session . exit_workdir () self . finish () return None","title":"Workspace"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.active_lock","text":"The FileLock for the current workspace.","title":"active_lock"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.active_session","text":"The current session context. Should be non-None after session_started.","title":"active_session"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.active_temp_dir","text":"The temporary directory the result archive is written to before it's moved into the results directory.","title":"active_temp_dir"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.active_workspace","text":"The current active workspace within a session.","title":"active_workspace"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.config","text":"The workspace config object that collects all the relevant paths.","title":"config"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.manager","text":"The manager object that holds all the locks.","title":"manager"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.metadata","text":"The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions.","title":"metadata"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.name","text":"A name for the type of session being performed.","title":"name"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.number","text":"The number of the workspace this object is operating with.","title":"number"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.session_class","text":"The class we're using to generate the active session context.","title":"session_class"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.__enter__","text":"Provides a context manager you can use to do various tasks within the Workspace. workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) Source code in simple_uam/workspace/workspace.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session","title":"__enter__()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.finish","text":"Finishes an active session. Source code in simple_uam/workspace/workspace.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None","title":"finish()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.workspace.Workspace.start","text":"Starts the session, meant to be used in a try-finally style. workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() Note that this won't change the working directory, unlike when a workspace is used as a context managed. Source code in simple_uam/workspace/workspace.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session","title":"start()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.WorkspaceManager","text":"Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. Source code in simple_uam/workspace/manager.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @frozen class WorkspaceManager (): \"\"\" Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. \"\"\" config : WorkspaceConfig = field () \"\"\" The workspace config object that collects all the relevant paths. \"\"\" def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile ) def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile ) def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num )) def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True ) def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True ) def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" )","title":"WorkspaceManager"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.config","text":"The workspace config object that collects all the relevant paths.","title":"config"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.acquire_workspace_lock","text":"Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. Source code in simple_uam/workspace/manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None","title":"acquire_workspace_lock()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.add_result","text":"Adds an archive to the result directory. Parameters: Name Type Description Default archive Union [ str , Path ] The file to be moved into the result dir. required prefix Optional [ str ] The prefix to the final file name. None suffix Optional [ str ] The file extension to use. None copy bool copy the file if true, otherwise move. True Returns: Type Description Optional [ Path ] The path to the resulting file in results dir, none if Optional [ Path ] no result was saved. Source code in simple_uam/workspace/manager.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path","title":"add_result()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.delete_locks","text":"Deletes all the locks that this WorkspaceManager can interact with. Parameters: Name Type Description Default skip_reference If true, skip deleting the reference lockfile. False skip_results If true, skip deleting the results lock. False Source code in simple_uam/workspace/manager.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True )","title":"delete_locks()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.init_dirs","text":"Creates the various workspace directories and subdirs that this class manages. Source code in simple_uam/workspace/manager.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True )","title":"init_dirs()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.init_ref_dir","text":"This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by Source code in simple_uam/workspace/manager.py 255 256 257 258 259 260 261 262 263 264 def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" )","title":"init_ref_dir()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.prune_results","text":"Deletes the oldest files in the results dir if there are too many. Source code in simple_uam/workspace/manager.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass","title":"prune_results()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.reference_lock","text":"Lockfile for reference directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 31 32 33 34 35 36 37 38 39 def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile )","title":"reference_lock()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.results_lock","text":"Lockfile for the results storage directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 41 42 43 44 45 46 47 48 def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile )","title":"results_lock()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.setup_reference_dir","text":"Wraps init_ref_dir with appropriate locks and file deletion. Parameters: Name Type Description Default **kwargs Passed to the init_ref_dir call. {} Source code in simple_uam/workspace/manager.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err","title":"setup_reference_dir()"},{"location":"reference/simple_uam/workspace/#simple_uam.workspace.manager.WorkspaceManager.workspace_lock","text":"Lockfile for single workspace directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 50 51 52 53 54 55 56 57 def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num ))","title":"workspace_lock()"},{"location":"reference/simple_uam/workspace/manager/","text":"WorkspaceManager \u00b6 Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. Source code in simple_uam/workspace/manager.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @frozen class WorkspaceManager (): \"\"\" Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. \"\"\" config : WorkspaceConfig = field () \"\"\" The workspace config object that collects all the relevant paths. \"\"\" def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile ) def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile ) def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num )) def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True ) def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True ) def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" ) config : WorkspaceConfig = field () class-attribute \u00b6 The workspace config object that collects all the relevant paths. acquire_workspace_lock ( num = None ) \u00b6 Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. Source code in simple_uam/workspace/manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None add_result ( archive , prefix = None , suffix = None , copy = True ) \u00b6 Adds an archive to the result directory. Parameters: Name Type Description Default archive Union [ str , Path ] The file to be moved into the result dir. required prefix Optional [ str ] The prefix to the final file name. None suffix Optional [ str ] The file extension to use. None copy bool copy the file if true, otherwise move. True Returns: Type Description Optional [ Path ] The path to the resulting file in results dir, none if Optional [ Path ] no result was saved. Source code in simple_uam/workspace/manager.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path delete_locks ( skip_reference = False , skip_results = False ) \u00b6 Deletes all the locks that this WorkspaceManager can interact with. Parameters: Name Type Description Default skip_reference If true, skip deleting the reference lockfile. False skip_results If true, skip deleting the results lock. False Source code in simple_uam/workspace/manager.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True ) init_dirs () \u00b6 Creates the various workspace directories and subdirs that this class manages. Source code in simple_uam/workspace/manager.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True ) init_ref_dir ( reference_dir , assets_dir ) \u00b6 This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by Source code in simple_uam/workspace/manager.py 255 256 257 258 259 260 261 262 263 264 def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" ) prune_results () \u00b6 Deletes the oldest files in the results dir if there are too many. Source code in simple_uam/workspace/manager.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass reference_lock () \u00b6 Lockfile for reference directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 31 32 33 34 35 36 37 38 39 def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile ) results_lock () \u00b6 Lockfile for the results storage directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 41 42 43 44 45 46 47 48 def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile ) setup_reference_dir ( ** kwargs ) \u00b6 Wraps init_ref_dir with appropriate locks and file deletion. Parameters: Name Type Description Default **kwargs Passed to the init_ref_dir call. {} Source code in simple_uam/workspace/manager.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err workspace_lock ( num ) \u00b6 Lockfile for single workspace directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 50 51 52 53 54 55 56 57 def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num ))","title":"manager"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager","text":"Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. Source code in simple_uam/workspace/manager.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @frozen class WorkspaceManager (): \"\"\" Object that provides locks for a particular workspace and holds some abstract functions for initializing the reference workspace. \"\"\" config : WorkspaceConfig = field () \"\"\" The workspace config object that collects all the relevant paths. \"\"\" def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile ) def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile ) def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num )) def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True ) def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True ) def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" )","title":"WorkspaceManager"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.config","text":"The workspace config object that collects all the relevant paths.","title":"config"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.acquire_workspace_lock","text":"Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. Source code in simple_uam/workspace/manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def acquire_workspace_lock ( self , num : Optional [ int ] = None ) -> Optional [ Tuple [ int , FileLock ]]: \"\"\" Will attempt to acquire a lock for a specific workspace, returns None on failure. If successful will return a tuple of workspace number and ALREADY ACQUIRED lock. The caller MUST ensure that the lock is released. \"\"\" workspace_queue = [ num ] if num == None : workspace_queue = self . config . workspace_nums for workspace_num in workspace_queue : lock = self . workspace_lock ( workspace_num ) try : lock . acquire ( blocking = False ) return tuple ([ workspace_num , lock ]) except Timeout : # Will only fail in lock.acquire and that cleans up after itself. pass # No lock free return None","title":"acquire_workspace_lock()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.add_result","text":"Adds an archive to the result directory. Parameters: Name Type Description Default archive Union [ str , Path ] The file to be moved into the result dir. required prefix Optional [ str ] The prefix to the final file name. None suffix Optional [ str ] The file extension to use. None copy bool copy the file if true, otherwise move. True Returns: Type Description Optional [ Path ] The path to the resulting file in results dir, none if Optional [ Path ] no result was saved. Source code in simple_uam/workspace/manager.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def add_result ( self , archive : Union [ str , Path ], prefix : Optional [ str ] = None , suffix : Optional [ str ] = None , copy : bool = True , ) -> Optional [ Path ]: \"\"\" Adds an archive to the result directory. Arguments: archive: The file to be moved into the result dir. prefix: The prefix to the final file name. suffix: The file extension to use. copy: copy the file if true, otherwise move. Returns: The path to the resulting file in results dir, none if no result was saved. \"\"\" # No results to save, don't bother. if self . config . results . max_count == 0 : return None # Normalize and validate input archive = Path ( archive ) if not archive . exists () or not archive . is_file (): raise RuntimeError ( f \"Expected { archive } to be file, cannot proceed.\" ) # Get archive file name setup time_str = datetime . now () . strftime ( \"%Y-%m- %d \" ) uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) if not prefix : prefix = archive . stem if not suffix : suffix = archive . suffix if not suffix . startswith ( '.' ): suffix = \".\" + suffix out_file = f \" { prefix } - { time_str } - { uniq_str }{ suffix } \" with tempfile . TemporaryDirectory () as tmp_dir : # Create a temorary copy if needed. if copy : target = Path ( tmp_dir ) / out_file shutil . copy2 ( archive , target ) archive = target # Move temp_file to results dir (no lock needed) out_path = self . config . results_path / out_file shutil . move ( archive , out_path ) # Return the resulting filepath return out_path","title":"add_result()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.delete_locks","text":"Deletes all the locks that this WorkspaceManager can interact with. Parameters: Name Type Description Default skip_reference If true, skip deleting the reference lockfile. False skip_results If true, skip deleting the results lock. False Source code in simple_uam/workspace/manager.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def delete_locks ( self , skip_reference = False , skip_results = False ): \"\"\" Deletes all the locks that this WorkspaceManager can interact with. Arguments: skip_reference: If true, skip deleting the reference lockfile. skip_results: If true, skip deleting the results lock. \"\"\" lockfiles = [ * self . config . workspace_lockfiles ] if not skip_reference : lockfiles . append ( self . config . reference_lockfile ) if not skip_results : lockfiles . append ( self . config . results_lockfile ) for lf in lockfiles : lf . unlink ( missing_ok = True )","title":"delete_locks()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.init_dirs","text":"Creates the various workspace directories and subdirs that this class manages. Source code in simple_uam/workspace/manager.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def init_dirs ( self ): \"\"\" Creates the various workspace directories and subdirs that this class manages. \"\"\" managed_dirs = [ self . config . workspaces_path , self . config . locks_path , self . config . reference_path , self . config . assets_path , self . config . results_path , self . config . results_lockdir , * self . config . workspace_paths , ] for d in managed_dirs : d . mkdir ( parents = True , exist_ok = True )","title":"init_dirs()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.init_ref_dir","text":"This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by Source code in simple_uam/workspace/manager.py 255 256 257 258 259 260 261 262 263 264 def init_ref_dir ( self , reference_dir : Path , assets_dir : Path ): \"\"\" This function should be overloaded by a child class with a task specific setup operation. Sets up the reference directory that workspaces are copied from. Should only be called by \"\"\" raise NotImplementedError ( \"Overload in child class. See docstring.\" )","title":"init_ref_dir()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.prune_results","text":"Deletes the oldest files in the results dir if there are too many. Source code in simple_uam/workspace/manager.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def prune_results ( self ): \"\"\" Deletes the oldest files in the results dir if there are too many. \"\"\" # Never pruning results, don't bother. if self . config . results . max_count < 0 : return # Gather results that are old enough to prune. now = datetime . now () results = list () total_results = 0 for result in self . config . results_path . iterdir (): if result . is_file (): total_results += 1 access_time = datime . from_timestamp ( result . stat () . st_atime ) staletime = ( now - access_time ) . total_seconds () if staletime > self . config . results . min_staletime : results . append ( tuple ( staletime , result )) # Short circuit if there's nothing to prune surplus_results = total_results - self . config . results . max_count if surplus_results <= 0 : return # Gather the oldest. results = heapq . nlargest ( surplus_results , results , key = lambda t : t [ 0 ], ) try : # Presumably someone else is also pruning if there's a lock, # Just let them do the work, and move on. with self . results_lock () . acquire ( blocking = False ): for result in results : # If a result got deleted before this point, that's fine. results . unlink ( missing_ok = True ) except Timeout : pass","title":"prune_results()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.reference_lock","text":"Lockfile for reference directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 31 32 33 34 35 36 37 38 39 def reference_lock ( self ) -> FileLock : \"\"\" Lockfile for reference directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . reference_lockfile )","title":"reference_lock()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.results_lock","text":"Lockfile for the results storage directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 41 42 43 44 45 46 47 48 def results_lock ( self ) -> FileLock : \"\"\" Lockfile for the results storage directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . results_lockfile )","title":"results_lock()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.setup_reference_dir","text":"Wraps init_ref_dir with appropriate locks and file deletion. Parameters: Name Type Description Default **kwargs Passed to the init_ref_dir call. {} Source code in simple_uam/workspace/manager.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def setup_reference_dir ( self , ** kwargs ): \"\"\" Wraps init_ref_dir with appropriate locks and file deletion. Arguments: **kwargs: Passed to the init_ref_dir call. \"\"\" ref_lock = self . reference_lock () try : with ref_lock : self . init_ref_dir ( self . config . reference_path , self . config . assets_path , ** kwargs , ) except Timeout as err : log . exception ( \"Could not acquire reference directory lock.\" , err = err , ) raise err","title":"setup_reference_dir()"},{"location":"reference/simple_uam/workspace/manager/#simple_uam.workspace.manager.WorkspaceManager.workspace_lock","text":"Lockfile for single workspace directory. We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. Source code in simple_uam/workspace/manager.py 50 51 52 53 54 55 56 57 def workspace_lock ( self , num : int ) -> FileLock : \"\"\" Lockfile for single workspace directory. Note: We provide new locks on every request due to reentrancy issues. Multiple threads sharing the same lock isn't the behavior we want. \"\"\" return FileLock ( self . config . workspace_lockfile ( num ))","title":"workspace_lock()"},{"location":"reference/simple_uam/workspace/session/","text":"Session \u00b6 An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. Source code in simple_uam/workspace/session.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 @define class Session (): \"\"\" An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. \"\"\" number : int = field ( kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" reference_dir : Path = field ( kw_only = True , ) \"\"\" The reference directory that the work_dir is copied from. \"\"\" @reference_dir . validator def _ref_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) work_dir : Path = field ( kw_only = True , ) \"\"\" The directory in which the session runs. \"\"\" @work_dir . validator def _work_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) _result_archive : Path = field ( kw_only = True , ) \"\"\" Where the result archive should be written to, after session completion the workspace should update this to the final archive location. \"\"\" @_result_archive . validator def _result_archive_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) @property def result_archive ( self ): return self . _result_archive @result_archive . setter def result_archive ( self , val ): self . metadata [ 'result_archive' ] = str ( val ) self . _result_archive = val name : str = field ( default = \"generic-session\" , kw_only = True , ) \"\"\" A name for the type of session being performed. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace. \"\"\" metadata_file : Path = field ( default = Path ( \"metadata.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores the current metadata. \"\"\" log_file : Path = field ( default = Path ( \"log.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores logs. \"\"\" @metadata_file . validator def _meta_file_valid ( self , attr , val ): if val . is_absolute (): raise RuntimeError ( \"The metadata file must be specified relative to the workdir.\" ) init_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace initialization. \"\"\" @init_exclude_patterns . default def _init_exclude_pats_def ( self ): return [ \".git\" ] result_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace result archive. \"\"\" @result_exclude_patterns . default def _result_exclude_pats_def ( self ): return [ \".git\" ] old_work_dir : Optional [ Path ] = field ( default = None , init = False , ) \"\"\" The working directory before the start of the session, to be restored after the session. \"\"\" start_time : datetime = field ( factory = datetime . now , init = False , ) \"\"\" The time when the session object was created/started. \"\"\" def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data ) @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs ) @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir ) @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" ) @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args ) @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" ) init_exclude_patterns : List [ str ] = field ( kw_only = True ) class-attribute \u00b6 Patterns to exclude from workspace initialization. log_file : Path = field ( default = Path ( 'log.json' ), kw_only = True ) class-attribute \u00b6 The file in the workdir (and eventually in the results archive) that stores logs. metadata : Dict = field ( factory = dict , kw_only = True ) class-attribute \u00b6 The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace. metadata_file : Path = field ( default = Path ( 'metadata.json' ), kw_only = True ) class-attribute \u00b6 The file in the workdir (and eventually in the results archive) that stores the current metadata. name : str = field ( default = 'generic-session' , kw_only = True ) class-attribute \u00b6 A name for the type of session being performed. number : int = field ( kw_only = True ) class-attribute \u00b6 The number of the workspace this object is operating with. old_work_dir : Optional [ Path ] = field ( default = None , init = False ) class-attribute \u00b6 The working directory before the start of the session, to be restored after the session. reference_dir : Path = field ( kw_only = True ) class-attribute \u00b6 The reference directory that the work_dir is copied from. result_exclude_patterns : List [ str ] = field ( kw_only = True ) class-attribute \u00b6 Patterns to exclude from workspace result archive. start_time : datetime = field ( factory = datetime . now , init = False ) class-attribute \u00b6 The time when the session object was created/started. work_dir : Path = field ( kw_only = True ) class-attribute \u00b6 The directory in which the session runs. enter_workdir () \u00b6 Changes the current working dir of the session to the workspace. Source code in simple_uam/workspace/session.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir ) exit_workdir () \u00b6 Reset the work directory to what it was before enter_workdir. Source code in simple_uam/workspace/session.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None generate_result_archive () \u00b6 Creates the result archive from the current working directory as it is. Source code in simple_uam/workspace/session.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args ) log_exception ( * excs , exc_type = None , exc_val = None , exc_tb = None ) \u00b6 Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Parameters: Name Type Description Default *exc list of exceptions to process, mutually exclusive with following args. required exc_type The type of the exception. None exc_val The value of the exception. None exc_tb The traceback of the exception. None Source code in simple_uam/workspace/session.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data ) reset_workspace ( progress = True , verbose = False , quiet = False ) \u00b6 Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Parameters: Name Type Description Default progress bool show a progress bar. True verbose bool rsync verbose output. False quiet bool perform the copy silently. False Source code in simple_uam/workspace/session.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) run ( * vargs , cwd = None , ** kwargs ) \u00b6 Identical to subprocess.run except the working directory is set automatically to the workspace directory. Source code in simple_uam/workspace/session.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs ) session_info () \u00b6 Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. Source code in simple_uam/workspace/session.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info validate_complete () \u00b6 Runs any checks we need to ensure that we can close down the session. Source code in simple_uam/workspace/session.py 424 425 426 427 428 429 430 431 432 433 434 435 436 @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" ) write_metadata () \u00b6 Writes the current metadata to a file in the working directory. Source code in simple_uam/workspace/session.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" ) session_op ( f ) \u00b6 Decorator for functions within the workspace that must take place within a session and should be added to the log. Source code in simple_uam/workspace/session.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def session_op ( f ): \"\"\" Decorator for functions within the workspace that must take place within a session and should be added to the log. \"\"\" @wraps ( f ) def wrapper ( self , * args , ** kwargs ): # NOTE : 'self' here is a Session or child. # add start log entry result = f ( self , * args , ** kwargs ) # add end log entry return result return wrapper","title":"session"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session","text":"An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. Source code in simple_uam/workspace/session.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 @define class Session (): \"\"\" An object that represents a session within a workspace, provide various helpers for managed a bunch of operations, as well as logging and the like. Used within a context manager. Inherit from this class and add more operations that are specific to a particular context. \"\"\" number : int = field ( kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" reference_dir : Path = field ( kw_only = True , ) \"\"\" The reference directory that the work_dir is copied from. \"\"\" @reference_dir . validator def _ref_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) work_dir : Path = field ( kw_only = True , ) \"\"\" The directory in which the session runs. \"\"\" @work_dir . validator def _work_dir_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) _result_archive : Path = field ( kw_only = True , ) \"\"\" Where the result archive should be written to, after session completion the workspace should update this to the final archive location. \"\"\" @_result_archive . validator def _result_archive_valid ( self , attr , val ): if not val . is_absolute (): raise RuntimeError ( \"Session init paths must be absolute.\" ) @property def result_archive ( self ): return self . _result_archive @result_archive . setter def result_archive ( self , val ): self . metadata [ 'result_archive' ] = str ( val ) self . _result_archive = val name : str = field ( default = \"generic-session\" , kw_only = True , ) \"\"\" A name for the type of session being performed. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace. \"\"\" metadata_file : Path = field ( default = Path ( \"metadata.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores the current metadata. \"\"\" log_file : Path = field ( default = Path ( \"log.json\" ), kw_only = True , ) \"\"\" The file in the workdir (and eventually in the results archive) that stores logs. \"\"\" @metadata_file . validator def _meta_file_valid ( self , attr , val ): if val . is_absolute (): raise RuntimeError ( \"The metadata file must be specified relative to the workdir.\" ) init_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace initialization. \"\"\" @init_exclude_patterns . default def _init_exclude_pats_def ( self ): return [ \".git\" ] result_exclude_patterns : List [ str ] = field ( kw_only = True , ) \"\"\" Patterns to exclude from workspace result archive. \"\"\" @result_exclude_patterns . default def _result_exclude_pats_def ( self ): return [ \".git\" ] old_work_dir : Optional [ Path ] = field ( default = None , init = False , ) \"\"\" The working directory before the start of the session, to be restored after the session. \"\"\" start_time : datetime = field ( factory = datetime . now , init = False , ) \"\"\" The time when the session object was created/started. \"\"\" def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data ) @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs ) @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args ) @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir ) @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" ) @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args ) @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" )","title":"Session"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.init_exclude_patterns","text":"Patterns to exclude from workspace initialization.","title":"init_exclude_patterns"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.log_file","text":"The file in the workdir (and eventually in the results archive) that stores logs.","title":"log_file"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.metadata","text":"The metadata for this session, will be stored in metadata.json in the result archive. Will be initialized by the Workspace.","title":"metadata"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.metadata_file","text":"The file in the workdir (and eventually in the results archive) that stores the current metadata.","title":"metadata_file"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.name","text":"A name for the type of session being performed.","title":"name"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.number","text":"The number of the workspace this object is operating with.","title":"number"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.old_work_dir","text":"The working directory before the start of the session, to be restored after the session.","title":"old_work_dir"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.reference_dir","text":"The reference directory that the work_dir is copied from.","title":"reference_dir"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.result_exclude_patterns","text":"Patterns to exclude from workspace result archive.","title":"result_exclude_patterns"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.start_time","text":"The time when the session object was created/started.","title":"start_time"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.work_dir","text":"The directory in which the session runs.","title":"work_dir"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.enter_workdir","text":"Changes the current working dir of the session to the workspace. Source code in simple_uam/workspace/session.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 @session_op def enter_workdir ( self ): \"\"\" Changes the current working dir of the session to the workspace. \"\"\" if self . old_work_dir != None : raise RuntimeError ( \"Trying to enter session directory when already in session directory.\" ) self . old_work_dir = Path . cwd () . resolve () log . info ( \"Entering workdir within session.\" , workspace = self . number , old_cwd = str ( self . old_work_dir ), new_cwd = str ( self . work_dir ), ) os . chdir ( self . work_dir )","title":"enter_workdir()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.exit_workdir","text":"Reset the work directory to what it was before enter_workdir. Source code in simple_uam/workspace/session.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 @session_op def exit_workdir ( self ): \"\"\" Reset the work directory to what it was before enter_workdir. \"\"\" if self . old_work_dir == None : raise RuntimeError ( \"Trying to exist session dir without first entering\" ) log . info ( \"Exiting workdir within session.\" , workspace = self . number , old_cwd = str ( self . work_dir ), new_cwd = str ( self . old_work_dir ), ) os . chdir ( self . old_work_dir ) self . old_work_dir = None","title":"exit_workdir()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.generate_result_archive","text":"Creates the result archive from the current working directory as it is. Source code in simple_uam/workspace/session.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 @session_op def generate_result_archive ( self ): \"\"\" Creates the result archive from the current working directory as it is. \"\"\" rsync_args = dict ( ref = str ( self . reference_dir ), src = str ( self . work_dir ), out = str ( self . result_archive ), exclude_from = [ str ( f ) for f in self . result_exclude_files ], ) log . info ( \"Generating result archive for session.\" , workspace = self . number , ** rsync_args , ) Rsync . archive_changes ( ** rsync_args )","title":"generate_result_archive()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.log_exception","text":"Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Parameters: Name Type Description Default *exc list of exceptions to process, mutually exclusive with following args. required exc_type The type of the exception. None exc_val The value of the exception. None exc_tb The traceback of the exception. None Source code in simple_uam/workspace/session.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def log_exception ( self , * excs , exc_type = None , exc_val = None , exc_tb = None ): \"\"\" Adds the provided exception to the metadata of this session. Useful for detecting errors during the run. Arguments: *exc: list of exceptions to process, mutually exclusive with following args. exc_type: The type of the exception. exc_val: The value of the exception. exc_tb: The traceback of the exception. \"\"\" if ( exc_type or exc_val or exc_tb ) and len ( excs ) > 0 : raise RuntimeError ( \"Can only provide positional arg or kw args, not both.\" ) elif len ( excs ) > 1 : raise RuntimeError ( \"Can only log a single exception at a time.\" ) ### Organize Exceptions ### exceptions = list () current = None if len ( excs ) > 0 : current = dict ( type = type ( exc ), val = exc , tb = exc . __traceback__ , ) elif exc_type or exc_val or exc_tb : current = dict ( type = exc_type or type ( exc_val ), val = exc_val , tb = exc_tb or exc_val . __traceback__ , ) else : return # Nothing to do while current : exceptions . append ( current ) next = current [ 'val' ] . __cause__ or current [ 'val' ] . __context__ if next : current = dict ( type = type ( next ), val = next , tb = next , ) else : current = None ### Serialize Exceptions ### exception_data = dict ( time = datetime . now () . isoformat (), chain = list () ) for exception in exceptions : exception_data [ 'chain' ] . append ( dict ( type = exception [ 'type' ] . __name__ , value = traceback . format_exception_only ( exception [ 'type' ], exception [ 'val' ], ), stack = traceback . format_stack ( exception [ 'tb' ]), )) ### Add to Metadata ### if 'exceptions' not in self . metadata : self . metadata [ 'exceptions' ] = list () self . metadata [ 'exceptions' ] . append ( exception_data )","title":"log_exception()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.reset_workspace","text":"Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Parameters: Name Type Description Default progress bool show a progress bar. True verbose bool rsync verbose output. False quiet bool perform the copy silently. False Source code in simple_uam/workspace/session.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @session_op def reset_workspace ( self , progress : bool = True , verbose : bool = False , quiet : bool = False ): \"\"\" Resets the workspace from the reference_workspace, using rsync to ensure the files are in an identical state. Arguments: progress: show a progress bar. verbose: rsync verbose output. quiet: perform the copy silently. \"\"\" rsync_args = dict ( src = self . reference_dir , dst = self . work_dir , exclude = self . init_exclude_patterns , delete = True , update = False , progress = progress , verbose = verbose , quiet = quiet , ) log . info ( \"Resetting workspace with rsync.\" , workspace = self . number , ** rsync_args , ) Rsync . copy_dir ( ** rsync_args )","title":"reset_workspace()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.run","text":"Identical to subprocess.run except the working directory is set automatically to the workspace directory. Source code in simple_uam/workspace/session.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 @session_op def run ( self , * vargs , cwd : Union [ str , Path ] = None , ** kwargs ) -> subprocess . CompletedProcess : \"\"\" Identical to subprocess.run except the working directory is set automatically to the workspace directory. \"\"\" if cwd == None : cwd = self . work_dir cwd = Path ( cwd ) if not cwd . is_absolute (): cwd = self . work_dir / cwd log . info ( \"Running console command in session.\" , workspace = self . number , args = vargs , cwd = str ( cwd ), ** kwargs ) return subprocess . run ( * vargs , cwd = cwd , ** kwargs )","title":"run()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.session_info","text":"Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. Source code in simple_uam/workspace/session.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 @session_op def session_info ( self ) -> Dict : \"\"\" Returns a number of session specific metadata fields that are then added to the metadata file by write_metadata. \"\"\" info = dict () info [ 'name' ] = self . name info [ 'start_time' ] = self . start_time . isoformat () info [ 'end_time' ] = datetime . now () . isoformat () info [ 'reference_dir' ] = str ( self . reference_dir ) info [ 'workspace_num' ] = self . number info [ 'workspace' ] = str ( self . work_dir ) info [ 'platform' ] = platform . system () info [ 'platform_release' ] = platform . release () info [ 'platform_version' ] = platform . version () info [ 'architecture' ] = platform . machine () info [ 'hostname' ] = socket . gethostname () info [ 'ip_address' ] = socket . gethostbyname ( socket . gethostname ()) info [ 'mac_address' ] = ':' . join ( re . findall ( '..' , ' %012x ' % uuid . getnode ())) info [ 'processor' ] = platform . processor () return info","title":"session_info()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.validate_complete","text":"Runs any checks we need to ensure that we can close down the session. Source code in simple_uam/workspace/session.py 424 425 426 427 428 429 430 431 432 433 434 435 436 @session_op def validate_complete ( self ): \"\"\" Runs any checks we need to ensure that we can close down the session. \"\"\" log . info ( \"Running session close validation checks.\" , workspace = self . number , ) if self . old_work_dir != None : raise RuntimeError ( \"Still in work_dir on closing.\" )","title":"validate_complete()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.Session.write_metadata","text":"Writes the current metadata to a file in the working directory. Source code in simple_uam/workspace/session.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 @session_op def write_metadata ( self ): \"\"\" Writes the current metadata to a file in the working directory. \"\"\" self . metadata [ 'session_info' ] = self . session_info () meta_path = self . work_dir / self . metadata_file log . info ( \"Writing session metadata to file.\" , metadata_file = str ( meta_path ), metadata = self . metadata , ) with meta_path . open ( 'w' ) as out_file : json . dump ( self . metadata , out_file , indent = \" \" )","title":"write_metadata()"},{"location":"reference/simple_uam/workspace/session/#simple_uam.workspace.session.session_op","text":"Decorator for functions within the workspace that must take place within a session and should be added to the log. Source code in simple_uam/workspace/session.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def session_op ( f ): \"\"\" Decorator for functions within the workspace that must take place within a session and should be added to the log. \"\"\" @wraps ( f ) def wrapper ( self , * args , ** kwargs ): # NOTE : 'self' here is a Session or child. # add start log entry result = f ( self , * args , ** kwargs ) # add end log entry return result return wrapper","title":"session_op()"},{"location":"reference/simple_uam/workspace/workspace/","text":"Workspace \u00b6 A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. Source code in simple_uam/workspace/workspace.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 @define class Workspace (): \"\"\" A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. \"\"\" name : str = field ( default = \"generic-session\" , ) \"\"\" A name for the type of session being performed. \"\"\" number : Optional [ int ] = field ( default = None , converter = converters . optional ( int ), kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions. \"\"\" manager : WorkspaceManager = field ( on_setattr = setters . frozen , kw_only = True , ) \"\"\" The manager object that holds all the locks. \"\"\" config : WorkspaceConfig = field ( init = False , on_setattr = setters . frozen , ) \"\"\" The workspace config object that collects all the relevant paths. \"\"\" @config . default def _config_def ( self ): return self . manager . config session_class : Type [ Session ] = field ( default = Session , kw_only = True , on_setattr = setters . frozen , ) \"\"\" The class we're using to generate the active session context. \"\"\" active_session : Optional [ Session ] = field ( default = None , init = False , ) \"\"\" The current session context. Should be non-None after session_started. \"\"\" active_workspace : Optional [ int ] = field ( default = None , init = False , ) \"\"\" The current active workspace within a session. \"\"\" active_lock : Optional [ FileLock ] = field ( default = None , init = False , ) \"\"\" The FileLock for the current workspace. \"\"\" active_temp_dir : Optional [ tempfile . TemporaryDirectory ] = field ( default = None , init = False , ) \"\"\" The temporary directory the result archive is written to before it's moved into the results directory. \"\"\" def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session def __exit__ ( self , exp_type , exp_val , exp_traceback ): self . active_session . log_exception ( exc_type = exc_type , exc_val = exc_val , exc_tb = exc_traceback , ) self . active_session . exit_workdir () self . finish () return None active_lock : Optional [ FileLock ] = field ( default = None , init = False ) class-attribute \u00b6 The FileLock for the current workspace. active_session : Optional [ Session ] = field ( default = None , init = False ) class-attribute \u00b6 The current session context. Should be non-None after session_started. active_temp_dir : Optional [ tempfile . TemporaryDirectory ] = field ( default = None , init = False ) class-attribute \u00b6 The temporary directory the result archive is written to before it's moved into the results directory. active_workspace : Optional [ int ] = field ( default = None , init = False ) class-attribute \u00b6 The current active workspace within a session. config : WorkspaceConfig = field ( init = False , on_setattr = setters . frozen ) class-attribute \u00b6 The workspace config object that collects all the relevant paths. manager : WorkspaceManager = field ( on_setattr = setters . frozen , kw_only = True ) class-attribute \u00b6 The manager object that holds all the locks. metadata : Dict = field ( factory = dict , kw_only = True ) class-attribute \u00b6 The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions. name : str = field ( default = 'generic-session' ) class-attribute \u00b6 A name for the type of session being performed. number : Optional [ int ] = field ( default = None , converter = converters . optional ( int ), kw_only = True ) class-attribute \u00b6 The number of the workspace this object is operating with. session_class : Type [ Session ] = field ( default = Session , kw_only = True , on_setattr = setters . frozen ) class-attribute \u00b6 The class we're using to generate the active session context. __enter__ () \u00b6 Provides a context manager you can use to do various tasks within the Workspace. workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) Source code in simple_uam/workspace/workspace.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session finish () \u00b6 Finishes an active session. Source code in simple_uam/workspace/workspace.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None start () \u00b6 Starts the session, meant to be used in a try-finally style. workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() Note that this won't change the working directory, unlike when a workspace is used as a context managed. Source code in simple_uam/workspace/workspace.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session","title":"workspace"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace","text":"A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. Source code in simple_uam/workspace/workspace.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 @define class Workspace (): \"\"\" A temporary wrapper which represents a single session within a workspace. Holds the current lock and some other stuff for the workspace and whatever return values ought to be accesible after the workspace has closed. Acts as a context manager. Have another class inherit from it to change the default session_class and set a default manager. \"\"\" name : str = field ( default = \"generic-session\" , ) \"\"\" A name for the type of session being performed. \"\"\" number : Optional [ int ] = field ( default = None , converter = converters . optional ( int ), kw_only = True , ) \"\"\" The number of the workspace this object is operating with. \"\"\" metadata : Dict = field ( factory = dict , kw_only = True , ) \"\"\" The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions. \"\"\" manager : WorkspaceManager = field ( on_setattr = setters . frozen , kw_only = True , ) \"\"\" The manager object that holds all the locks. \"\"\" config : WorkspaceConfig = field ( init = False , on_setattr = setters . frozen , ) \"\"\" The workspace config object that collects all the relevant paths. \"\"\" @config . default def _config_def ( self ): return self . manager . config session_class : Type [ Session ] = field ( default = Session , kw_only = True , on_setattr = setters . frozen , ) \"\"\" The class we're using to generate the active session context. \"\"\" active_session : Optional [ Session ] = field ( default = None , init = False , ) \"\"\" The current session context. Should be non-None after session_started. \"\"\" active_workspace : Optional [ int ] = field ( default = None , init = False , ) \"\"\" The current active workspace within a session. \"\"\" active_lock : Optional [ FileLock ] = field ( default = None , init = False , ) \"\"\" The FileLock for the current workspace. \"\"\" active_temp_dir : Optional [ tempfile . TemporaryDirectory ] = field ( default = None , init = False , ) \"\"\" The temporary directory the result archive is written to before it's moved into the results directory. \"\"\" def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session def __exit__ ( self , exp_type , exp_val , exp_traceback ): self . active_session . log_exception ( exc_type = exc_type , exc_val = exc_val , exc_tb = exc_traceback , ) self . active_session . exit_workdir () self . finish () return None","title":"Workspace"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.active_lock","text":"The FileLock for the current workspace.","title":"active_lock"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.active_session","text":"The current session context. Should be non-None after session_started.","title":"active_session"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.active_temp_dir","text":"The temporary directory the result archive is written to before it's moved into the results directory.","title":"active_temp_dir"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.active_workspace","text":"The current active workspace within a session.","title":"active_workspace"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.config","text":"The workspace config object that collects all the relevant paths.","title":"config"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.manager","text":"The manager object that holds all the locks.","title":"manager"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.metadata","text":"The metadata for this session, will be stored in metadata.json in the result archive. Can be set at init, or modified before session start. Changes to this persist between sessions.","title":"metadata"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.name","text":"A name for the type of session being performed.","title":"name"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.number","text":"The number of the workspace this object is operating with.","title":"number"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.session_class","text":"The class we're using to generate the active session context.","title":"session_class"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.__enter__","text":"Provides a context manager you can use to do various tasks within the Workspace. workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) Source code in simple_uam/workspace/workspace.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def __enter__ ( self ): \"\"\" Provides a context manager you can use to do various tasks within the Workspace. ``` workspace = Workspace(...) with workspace as session: # current working directory is now the workspace do_stuff_here() session.run(...) ``` \"\"\" session = self . start () session . enter_workdir () return session","title":"__enter__()"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.finish","text":"Finishes an active session. Source code in simple_uam/workspace/workspace.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def finish ( self ): \"\"\" Finishes an active session. \"\"\" if not self . active_session : raise RuntimeError ( \"Trying to finish session without an active session.\" ) try : if self . config . results . max_count != 0 : # Generate the results archive self . active_session . write_metadata () self . active_session . generate_result_archive () # Move it to the manager's results directory self . active_session . result_archive = self . manager . add_result ( archive = self . active_session . result_archive , prefix = self . name , copy = False , ) # Ensure we can close session self . active_session . validate_complete () finally : # release lock self . active_lock . release () self . active_temp_dir . cleanup () # reset workspace state self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None","title":"finish()"},{"location":"reference/simple_uam/workspace/workspace/#simple_uam.workspace.workspace.Workspace.start","text":"Starts the session, meant to be used in a try-finally style. workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() Note that this won't change the working directory, unlike when a workspace is used as a context managed. Source code in simple_uam/workspace/workspace.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def start ( self ) -> Session : \"\"\" Starts the session, meant to be used in a try-finally style. ``` workspace = Workspace(...) session = workspace.start() try: do_stuff_here() session.run(...) catch Exception as err: session.log_exception(err) finally: workspace.finish() ``` Note that this won't change the working directory, unlike when a workspace is used as a context managed. \"\"\" # Checks if self . active_session : raise RuntimeError ( \"Workspace currently in session, can't start a new one.\" ) # Get lock if possible, fail otherwise. lock_tuple = self . manager . acquire_workspace_lock ( self . number ) if lock_tuple == None : raise RuntimeError ( \"Could not acquire Workspace lock.\" ) try : # mark session_start self . active_workspace = lock_tuple [ 0 ] self . active_lock = lock_tuple [ 1 ] # create temp_dir & temp_results dir name self . active_temp_dir = tempfile . TemporaryDirectory () uniq_str = '' . join ( random . choices ( string . ascii_lowercase + string . digits , k = 10 )) temp_archive = Path ( self . active_temp_dir . name ) / f \" { self . name } - { uniq_str } .zip\" # setup metadata (more stuff can go here I guess) metadata = deepcopy ( self . metadata ) # create active session self . active_session = self . session_class ( reference_dir = self . config . reference_path , number = self . active_workspace , work_dir = self . config . workspace_path ( self . active_workspace ), init_exclude_patterns = self . config . exclude , result_exclude_patterns = self . config . result_exclude , result_archive = temp_archive , metadata = metadata , name = self . name , metadata_file = Path ( self . config . results . metadata_file ), ) self . active_session . reset_workspace ( progress = True , ) except Exception : # Perform Cleanup if self . active_temp_dir : self . active_temp_dir . cleanup () if lock_tuple : lock_tuple [ 1 ] . release () # Reset Internal State self . active_session = None self . active_workspace = None self . active_lock = None self . active_temp_dir = None # Re-raise exception raise return self . active_session","title":"start()"},{"location":"setup/aws-instance/","text":"Amazon Web Services (AWS) Instance Setup \u00b6 This section of the guide will cover: Creating the various EC2 instances needed for each component or combination of component. Provisioning an Instance \u00b6 Setup an EC2 instance for running various components. This only covers the various windows nodes that SimpleUAM currently supports and needs to be repeated for each node. Under the EC2 Console click \"Launch an Instance\". Creo License Server Minimum Requirements The license server is simple enough we only need a free tier windows instance. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t1.micro 1x vCPU 0.6gb Memory Key Pair: <ec2-keypair> Network Settings: VPC: <aws-vpc> Subnet: <aws-private-subnet> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-security-group> Advanced Network Configuration: None Configure Storage: 1x 30gb gp2 Worker Node Minimum Requirements The worker node needs an elastic graphics interface for Creo's UI to work. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t2.large 2x vCPU 8gb Memory Key Pair: <ec2-keypair> Network Settings: VPC: <aws-vpc> Subnet: <aws-private-subnet> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-security-group> Advanced Network Configuration: None Configure Storage: 1x 1000gb gp2 Advanced Details: Elastic GPU: eg1.medium This can get CPU bound, so it might make sense to bump it up. Corpus DB Minimum Requirements Option 1: If you need a read-only corpus DB itermittently for generating a static corpus, we reccomend just running the stub DB on a worker temporarily. Option 2: If you need a corpus DB for an alternate analysis pipeline and are fine with a non-persistent corpus then use a stub DB on a worker. Option 3: If you need a corpus that is persistent, shared between multiple workers, or needs to be performant then you should set up janus-graph on a linux machine. This is outside the scope of this guide and you should try to follow SWRI's instructions. Message Broker Minimum Requirements Option 1: If you want a non-backendless broker or one that's not wasting your money set up a linux instance and skip to the section on message broker setup . I'm not sure what minimum requirements make sense. Option 2: If you want a backendless message broker for intermittent testing or development work then install it sidecar with a worker node. Option 3: If you want a backendless broker and are fine with lower performance the specifications for a worker node on a windows machine be fine. Find the instance you just created with \"Name\" <instance.name> . Save the \"Instance ID\" as: <instance.id> Connect to an Instance \u00b6 Use the remote desktop protocol to connect to the instance. Get RDP Connection Information \u00b6 Select the instance on your EC2 Dashboard Hit the Connect Button Connect to Instance -> RDP Client Keep note of: Private IP: <instance.ip> Username: <instance.user> RDP File: Click \"Download remote desktop file\" Save to <instance.rdp-file> Password: Click \"Get Password\" Upload <ec2-keypair-pem> Click \"Decrypt Password\" Save to <instance.rdp-pass> Connect Via RDP \u00b6 Make sure your local machine is connected to the VPN set up previously, otherwise none of the provided IPs will correctly resolve. Via Preferred RDP client (e.g. Remmina ): Import: <instance.rdp-file> IP: <instance.ip> Username: <instance.user> Password: <instance.rdp-pass> Mount Shared FSx Drive \u00b6 The AWS specific instructions are largely the same as the standard windows NFS ones, but they need certain server features to be enabled first. Add NFS client: Instructions Setup The Automount of drive: Instructions Open File Explorer and click \"Network\" and \"Map Network Drive\": Drive : <fsx-drive-letter> Folder : \\\\<aws-fsx-ip>\\fsx\\ Reconnect At Login : Yes Continue to General Setup ...","title":"AWS (Instance)"},{"location":"setup/aws-instance/#amazon-web-services-aws-instance-setup","text":"This section of the guide will cover: Creating the various EC2 instances needed for each component or combination of component.","title":"Amazon Web Services (AWS) Instance Setup"},{"location":"setup/aws-instance/#provisioning-an-instance","text":"Setup an EC2 instance for running various components. This only covers the various windows nodes that SimpleUAM currently supports and needs to be repeated for each node. Under the EC2 Console click \"Launch an Instance\". Creo License Server Minimum Requirements The license server is simple enough we only need a free tier windows instance. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t1.micro 1x vCPU 0.6gb Memory Key Pair: <ec2-keypair> Network Settings: VPC: <aws-vpc> Subnet: <aws-private-subnet> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-security-group> Advanced Network Configuration: None Configure Storage: 1x 30gb gp2 Worker Node Minimum Requirements The worker node needs an elastic graphics interface for Creo's UI to work. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t2.large 2x vCPU 8gb Memory Key Pair: <ec2-keypair> Network Settings: VPC: <aws-vpc> Subnet: <aws-private-subnet> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-security-group> Advanced Network Configuration: None Configure Storage: 1x 1000gb gp2 Advanced Details: Elastic GPU: eg1.medium This can get CPU bound, so it might make sense to bump it up. Corpus DB Minimum Requirements Option 1: If you need a read-only corpus DB itermittently for generating a static corpus, we reccomend just running the stub DB on a worker temporarily. Option 2: If you need a corpus DB for an alternate analysis pipeline and are fine with a non-persistent corpus then use a stub DB on a worker. Option 3: If you need a corpus that is persistent, shared between multiple workers, or needs to be performant then you should set up janus-graph on a linux machine. This is outside the scope of this guide and you should try to follow SWRI's instructions. Message Broker Minimum Requirements Option 1: If you want a non-backendless broker or one that's not wasting your money set up a linux instance and skip to the section on message broker setup . I'm not sure what minimum requirements make sense. Option 2: If you want a backendless message broker for intermittent testing or development work then install it sidecar with a worker node. Option 3: If you want a backendless broker and are fine with lower performance the specifications for a worker node on a windows machine be fine. Find the instance you just created with \"Name\" <instance.name> . Save the \"Instance ID\" as: <instance.id>","title":"Provisioning an Instance"},{"location":"setup/aws-instance/#connect-to-an-instance","text":"Use the remote desktop protocol to connect to the instance.","title":"Connect to an Instance"},{"location":"setup/aws-instance/#get-rdp-connection-information","text":"Select the instance on your EC2 Dashboard Hit the Connect Button Connect to Instance -> RDP Client Keep note of: Private IP: <instance.ip> Username: <instance.user> RDP File: Click \"Download remote desktop file\" Save to <instance.rdp-file> Password: Click \"Get Password\" Upload <ec2-keypair-pem> Click \"Decrypt Password\" Save to <instance.rdp-pass>","title":"Get RDP Connection Information"},{"location":"setup/aws-instance/#connect-via-rdp","text":"Make sure your local machine is connected to the VPN set up previously, otherwise none of the provided IPs will correctly resolve. Via Preferred RDP client (e.g. Remmina ): Import: <instance.rdp-file> IP: <instance.ip> Username: <instance.user> Password: <instance.rdp-pass>","title":"Connect Via RDP"},{"location":"setup/aws-instance/#mount-shared-fsx-drive","text":"The AWS specific instructions are largely the same as the standard windows NFS ones, but they need certain server features to be enabled first. Add NFS client: Instructions Setup The Automount of drive: Instructions Open File Explorer and click \"Network\" and \"Map Network Drive\": Drive : <fsx-drive-letter> Folder : \\\\<aws-fsx-ip>\\fsx\\ Reconnect At Login : Yes Continue to General Setup ...","title":"Mount Shared FSx Drive"},{"location":"setup/aws-network/","text":"Amazon Web Services (AWS) Network Setup \u00b6 This section of the guide will cover: Setting up a virtual private cloud (VPC) to contain SUAM nodes. Creating a VPN connection into that cloud for home access. Creating a shared network drive for hosting code and analysis results. Initial Steps \u00b6 Most operations will require access to the AWS Console so ensure you have a valid amazon AWS account. Info This guide assumes that you have full access to AWS. In principle nothing should prevent an IAM user from performing all these steps except knowing the set of neccesary permissions. If you do use an IAM user for this install process, please tell us what policies you used so we can include them in this guide. Virtual Private Cloud \u00b6 This creates a private network where your servers can live isolated from the internet yet able to communicate with each other. Create public and private subnets \u00b6 Open the VPC console's Create VPC page . VPC Settings: Resources to Create : VPC and more Name tag auto-generation: Auto-generate : Yes (check box) Name : <aws-vpc> (pick your own and save it) IPv4 CIDR block : 10.0.0.0/16 IPv6 CIDR block : No IPv6 CIDR block Number of Availability Zones : 1 Number of Public Subnets : 1 Number of Private Subnets : 1 Customize subnets CIDR blocks: Public subnet CIDR block : 10.0.0.0/20 Private subnet CIDR block : 10.0.128.0/20 NAT gateways : in 1 AZ VPC endpoints : None (unless you want S3 access) DNS options: Enable DNS Hostnames : Yes (check box) Enable DNS Resolution : Yes (check box) Save useful information \u00b6 Click \"Create VPC\". Once the process bar is done click \"View VPC\". Keep track of the VPC ID as: <aws-vpc-id> Open the \"Elastic IPs\" page on the VPC console: Find <aws-vpc> 's external IP address. It should be named \" <aws-vpc> -eip-...\" . Keep track of the \"Allocated IPv4 address\" under <aws-elastic-ip> Open the \"Subnets\" page on the VPC console. Find <aws-vpc> 's public subnet. It should be named \" <aws-vpc> -subnet-public1-...\" . Keep track of \"Subnet ID\" as: <aws-public-subnet> Find <aws-vpc> 's private subnet. It should be named \" <aws-vpc> -subnet-private1-...\" . Keep track of \"Subnet ID\" as: <aws-private-subnet> Open up the internal firewall \u00b6 Open the \"Security Groups\" page of the VPC console. Find the subnet whose \"VPC ID\" is <aws-vpc-id> . Keep track of \"Security group ID\" as: <aws-default-sg> Select it, go to \"Inbound Rules\" on the bottom pane, and click \"Edit Inbound Rules\". Click \"Add Rule\": Type : Elastic Graphics Source : <aws-default-sg> Click \"Add Rule\": Type : All TCP Source : 10.0.0.0/8 Click \"Save rules\" AWS VPN Connection \u00b6 Sets up keys and a VPN connection so your local computer can directly communicate with instances and services on the private subnet. Create keys for VPN access \u00b6 Create sever and client certs: Follow the instructions here . Keep track of: server.crt as: <aws-server-crt> server.key as: <aws-server-key> client1.domain.tld.crt as: <aws-client-crt> client1.domain.tld.key as: <aws-client-key> Import the certs to ACM: Follow the instructions here . Open the AWS Certificate Manager's \"List Certificates\" page. For the server cert: Keep track of \"Identifier\" as: <aws-server-cert-id> Keep track of \"ARN\" as: <aws-server-cert-arn> For the client cert: Keep track of \"Identifier\" as: <aws-client-cert-id> Keep track of \"ARN\" as: <aws-client-cert-arn> Create the VPN interface \u00b6 Follow the instructions here . Step 2: Name : <aws-cvpn> Client IPV4 CIDR : 10.10.0.0/22 Server Cert ARN : <aws-server-cert-arn> Authentication Option : Mutual Client Cert ARN : <aws-client-cert-arn> Log client Details? : No Enable client Connect : no Optional Params: Transport : UDP Enable Split Tunnel : Yes VPC ID : <aws-vpc-id> VPN Port : 443 Enable Self-Service : yes Session Timeout : 24h Enable Client Logic Banner : No Save the Client VPN ID as <aws-cvpn-id> Step 3: Client VPN Assoc VPC : <aws-vpc-id> Subnet : <aws-private-subnet> Step 4: Add Auth Rule: Dest Network : 10.0.0.0/20 Grant Access : all users Step 5: Route Dest : 0.0.0.0/0 Target Subnet : <aws-private-subnet> Step 6: No changes Step 7: Skip this step Step 8: Skip this step Create Client VPN Config File \u00b6 This is the file users will import in order to connect to the above VPN interface. Instructions taken from here and here . Open the Amazon VPC console at https://console.aws.amazon.com/vpc/ In the navigation pane, choose \"Client VPN Endpoints\". Choose <aws-cvpn-id> and click \"Download Client Configuration\". Save this as <aws-cvpn-config> . Open <aws-cvpn-config> in a text editor. Prepend a random subdomain to the ClientVPN DNS entry The line should start with remote cvpn-endpoint- . When finished it should begin remote awrogr.cvpn-endpoint- with the rest remaining the same and awrogr replaced with some random string of your own. After the line that begins verb 3 insert the following: <ca> </ca> <cert> </cert> <key> </key> Take the ceritificate from <aws-server-crt> and place it between the <ca> tags. This is the section of the file between the BEGIN CERTIFICATE and END CERTIFICATE bars including the bars themselves . Take the certificate from <aws-client-crt> and place it between the <cert> tags. Take the private key from <aws-client-key> and place it between the <key> tags. This is the section of the file between the BEGIN PRIVATE KEY and END PRIVATE KEY bars including the bars themselves . Save the modified file. Distribute the <aws-cvpn-config> to your intended users. Sample <aws-cvpn-config> after above modifications. The actual contents of the certificates have been replaced with ... . client dev tun proto udp remote asdf.cvpn-endpoint-0011abcabcabcabc1.prod.clientvpn.eu-west-2.amazonaws.com 443 remote-random-hostname resolv-retry infinite nobind remote-cert-tls server cipher AES-256-GCM verb 3 <ca> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- </key> reneg-sec 0 Connect to the VPN \u00b6 These instructions apply to your users as well as long as you provide them access to the <aws-cvpn-config> file. Linux: Instructions MacOS: Instructions Windows: Instructions FSx Shared Drive \u00b6 Create a shared drive that you can mount on both your local machine and worker nodes. This drive is both a convenient shared mount for development work, using your local setup to edit server code, and a place for multiple workers to stash results. Go to the FSx File System console . Click \"Create File System\": Select file system type : OpenZFS Creation Method : Standard create File System Details: File system name : <aws-fsx-name> SSD Storage capacity : more than 50gb Network and Security: VPC : <aws-vpc-id> VPC Security Groups : <vpc-default-sg> Subnet : <aws-private-subnet> Root Volume Configuration: Data compression type : none NFS Exports: Client Address: * NFS Options: rw,no_auth_nlm,all_squash,anonuid=0,anongid=0,crossmnt Click \"Next\" then \"Create File System\" Open the \"file systems\" page on the FSx console and select <aws-fsx-name> : Keep track of \"File System ID\" as: <aws-fsx-id> Open its \"Network Interface\" and save its \"IPv4 address\" as: <aws-fsx-ip> Unix Mount Instructions Ensure you're connected to the VPN. Choose and create a mount directory at <local-mount> . Make sure you have nfs utilities installed. For ubuntu: sudo apt-get -y install nfs-common . Run: sudo mount -t nfs -o nfsvers=4.1 <aws-fsx-ip>:/fsx/ <local-mount> Consider using nfs caching and async. (google for details) Windows Mount Instructions Ensure you're connected to the VPN. Chose a <drive-letter> to be the mount point. (e.g. 'Z:') Open cmd.exe as admin (Not powershell) Run: mount \\\\<aws-fsx-ip>\\fsx\\ <drive-letter> Create an EC2 Keypair \u00b6 This keypair is needed to connect to various EC2 instances in the VPC. Create a keypair for connecting to AWS instances. \u00b6 Open the EC2 console to the \"Key pairs\" page and click \"Create key pair\". Name : <ec2-keypair> Key Pair Type : RSA Private Key Format : .pem Click \"Create Key Pair\" When prompted save the \" <ec2-keypair> .pem\" file as: <ec2-keypair-pem> Continue to AWS Instance Setup ...","title":"AWS (Network)"},{"location":"setup/aws-network/#amazon-web-services-aws-network-setup","text":"This section of the guide will cover: Setting up a virtual private cloud (VPC) to contain SUAM nodes. Creating a VPN connection into that cloud for home access. Creating a shared network drive for hosting code and analysis results.","title":"Amazon Web Services (AWS) Network Setup"},{"location":"setup/aws-network/#initial-steps","text":"Most operations will require access to the AWS Console so ensure you have a valid amazon AWS account. Info This guide assumes that you have full access to AWS. In principle nothing should prevent an IAM user from performing all these steps except knowing the set of neccesary permissions. If you do use an IAM user for this install process, please tell us what policies you used so we can include them in this guide.","title":"Initial Steps"},{"location":"setup/aws-network/#virtual-private-cloud","text":"This creates a private network where your servers can live isolated from the internet yet able to communicate with each other.","title":"Virtual Private Cloud"},{"location":"setup/aws-network/#create-public-and-private-subnets","text":"Open the VPC console's Create VPC page . VPC Settings: Resources to Create : VPC and more Name tag auto-generation: Auto-generate : Yes (check box) Name : <aws-vpc> (pick your own and save it) IPv4 CIDR block : 10.0.0.0/16 IPv6 CIDR block : No IPv6 CIDR block Number of Availability Zones : 1 Number of Public Subnets : 1 Number of Private Subnets : 1 Customize subnets CIDR blocks: Public subnet CIDR block : 10.0.0.0/20 Private subnet CIDR block : 10.0.128.0/20 NAT gateways : in 1 AZ VPC endpoints : None (unless you want S3 access) DNS options: Enable DNS Hostnames : Yes (check box) Enable DNS Resolution : Yes (check box)","title":"Create public and private subnets"},{"location":"setup/aws-network/#save-useful-information","text":"Click \"Create VPC\". Once the process bar is done click \"View VPC\". Keep track of the VPC ID as: <aws-vpc-id> Open the \"Elastic IPs\" page on the VPC console: Find <aws-vpc> 's external IP address. It should be named \" <aws-vpc> -eip-...\" . Keep track of the \"Allocated IPv4 address\" under <aws-elastic-ip> Open the \"Subnets\" page on the VPC console. Find <aws-vpc> 's public subnet. It should be named \" <aws-vpc> -subnet-public1-...\" . Keep track of \"Subnet ID\" as: <aws-public-subnet> Find <aws-vpc> 's private subnet. It should be named \" <aws-vpc> -subnet-private1-...\" . Keep track of \"Subnet ID\" as: <aws-private-subnet>","title":"Save useful information"},{"location":"setup/aws-network/#open-up-the-internal-firewall","text":"Open the \"Security Groups\" page of the VPC console. Find the subnet whose \"VPC ID\" is <aws-vpc-id> . Keep track of \"Security group ID\" as: <aws-default-sg> Select it, go to \"Inbound Rules\" on the bottom pane, and click \"Edit Inbound Rules\". Click \"Add Rule\": Type : Elastic Graphics Source : <aws-default-sg> Click \"Add Rule\": Type : All TCP Source : 10.0.0.0/8 Click \"Save rules\"","title":"Open up the internal firewall"},{"location":"setup/aws-network/#aws-vpn-connection","text":"Sets up keys and a VPN connection so your local computer can directly communicate with instances and services on the private subnet.","title":"AWS VPN Connection"},{"location":"setup/aws-network/#create-keys-for-vpn-access","text":"Create sever and client certs: Follow the instructions here . Keep track of: server.crt as: <aws-server-crt> server.key as: <aws-server-key> client1.domain.tld.crt as: <aws-client-crt> client1.domain.tld.key as: <aws-client-key> Import the certs to ACM: Follow the instructions here . Open the AWS Certificate Manager's \"List Certificates\" page. For the server cert: Keep track of \"Identifier\" as: <aws-server-cert-id> Keep track of \"ARN\" as: <aws-server-cert-arn> For the client cert: Keep track of \"Identifier\" as: <aws-client-cert-id> Keep track of \"ARN\" as: <aws-client-cert-arn>","title":"Create keys for VPN access"},{"location":"setup/aws-network/#create-the-vpn-interface","text":"Follow the instructions here . Step 2: Name : <aws-cvpn> Client IPV4 CIDR : 10.10.0.0/22 Server Cert ARN : <aws-server-cert-arn> Authentication Option : Mutual Client Cert ARN : <aws-client-cert-arn> Log client Details? : No Enable client Connect : no Optional Params: Transport : UDP Enable Split Tunnel : Yes VPC ID : <aws-vpc-id> VPN Port : 443 Enable Self-Service : yes Session Timeout : 24h Enable Client Logic Banner : No Save the Client VPN ID as <aws-cvpn-id> Step 3: Client VPN Assoc VPC : <aws-vpc-id> Subnet : <aws-private-subnet> Step 4: Add Auth Rule: Dest Network : 10.0.0.0/20 Grant Access : all users Step 5: Route Dest : 0.0.0.0/0 Target Subnet : <aws-private-subnet> Step 6: No changes Step 7: Skip this step Step 8: Skip this step","title":"Create the VPN interface"},{"location":"setup/aws-network/#create-client-vpn-config-file","text":"This is the file users will import in order to connect to the above VPN interface. Instructions taken from here and here . Open the Amazon VPC console at https://console.aws.amazon.com/vpc/ In the navigation pane, choose \"Client VPN Endpoints\". Choose <aws-cvpn-id> and click \"Download Client Configuration\". Save this as <aws-cvpn-config> . Open <aws-cvpn-config> in a text editor. Prepend a random subdomain to the ClientVPN DNS entry The line should start with remote cvpn-endpoint- . When finished it should begin remote awrogr.cvpn-endpoint- with the rest remaining the same and awrogr replaced with some random string of your own. After the line that begins verb 3 insert the following: <ca> </ca> <cert> </cert> <key> </key> Take the ceritificate from <aws-server-crt> and place it between the <ca> tags. This is the section of the file between the BEGIN CERTIFICATE and END CERTIFICATE bars including the bars themselves . Take the certificate from <aws-client-crt> and place it between the <cert> tags. Take the private key from <aws-client-key> and place it between the <key> tags. This is the section of the file between the BEGIN PRIVATE KEY and END PRIVATE KEY bars including the bars themselves . Save the modified file. Distribute the <aws-cvpn-config> to your intended users. Sample <aws-cvpn-config> after above modifications. The actual contents of the certificates have been replaced with ... . client dev tun proto udp remote asdf.cvpn-endpoint-0011abcabcabcabc1.prod.clientvpn.eu-west-2.amazonaws.com 443 remote-random-hostname resolv-retry infinite nobind remote-cert-tls server cipher AES-256-GCM verb 3 <ca> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- </key> reneg-sec 0","title":"Create Client VPN Config File"},{"location":"setup/aws-network/#connect-to-the-vpn","text":"These instructions apply to your users as well as long as you provide them access to the <aws-cvpn-config> file. Linux: Instructions MacOS: Instructions Windows: Instructions","title":"Connect to the VPN"},{"location":"setup/aws-network/#fsx-shared-drive","text":"Create a shared drive that you can mount on both your local machine and worker nodes. This drive is both a convenient shared mount for development work, using your local setup to edit server code, and a place for multiple workers to stash results. Go to the FSx File System console . Click \"Create File System\": Select file system type : OpenZFS Creation Method : Standard create File System Details: File system name : <aws-fsx-name> SSD Storage capacity : more than 50gb Network and Security: VPC : <aws-vpc-id> VPC Security Groups : <vpc-default-sg> Subnet : <aws-private-subnet> Root Volume Configuration: Data compression type : none NFS Exports: Client Address: * NFS Options: rw,no_auth_nlm,all_squash,anonuid=0,anongid=0,crossmnt Click \"Next\" then \"Create File System\" Open the \"file systems\" page on the FSx console and select <aws-fsx-name> : Keep track of \"File System ID\" as: <aws-fsx-id> Open its \"Network Interface\" and save its \"IPv4 address\" as: <aws-fsx-ip> Unix Mount Instructions Ensure you're connected to the VPN. Choose and create a mount directory at <local-mount> . Make sure you have nfs utilities installed. For ubuntu: sudo apt-get -y install nfs-common . Run: sudo mount -t nfs -o nfsvers=4.1 <aws-fsx-ip>:/fsx/ <local-mount> Consider using nfs caching and async. (google for details) Windows Mount Instructions Ensure you're connected to the VPN. Chose a <drive-letter> to be the mount point. (e.g. 'Z:') Open cmd.exe as admin (Not powershell) Run: mount \\\\<aws-fsx-ip>\\fsx\\ <drive-letter>","title":"FSx Shared Drive"},{"location":"setup/aws-network/#create-an-ec2-keypair","text":"This keypair is needed to connect to various EC2 instances in the VPC.","title":"Create an EC2 Keypair"},{"location":"setup/aws-network/#create-a-keypair-for-connecting-to-aws-instances","text":"Open the EC2 console to the \"Key pairs\" page and click \"Create key pair\". Name : <ec2-keypair> Key Pair Type : RSA Private Key Format : .pem Click \"Create Key Pair\" When prompted save the \" <ec2-keypair> .pem\" file as: <ec2-keypair-pem> Continue to AWS Instance Setup ...","title":"Create a keypair for connecting to AWS instances."},{"location":"setup/broker/","text":"Message Broker Setup \u00b6 A message broker allows worker nodes to pull analysis requests from a shared queue. The standard message broker is RabbitMQ with the default settings. Option 1: Run RabbitMQ on a Linux Machine (Recommended) \u00b6 RabbitMQ is a backendless message broker that's sufficient if you're okay with clients not being notified when the analysis result is complete. (It will just appear in the results directory.) Follow the instructions here to set up RabbitMQ. Save this machine's IP as: <broker-ip> Save rabbitmq's open port as: <broker-port> (default: 5672) Option 2: Run Redis on a Linux Machine \u00b6 Redis can be configured as a response backend as well as a message broker. This means that clients can get notified when an analysis is complete. We have not tried to get this running so don't have install instructions. Save this machine's IP as: <broker-ip> Save redis's open port as: <broker-port> (default: 6379) Save the redis database you're using as: <broker-db> (default: \"0\") Option 3: Run RabbitMQ as a Windows Service \u00b6 This runs RabbitMQ as a windows service and is fine for development or production setups. Prerequisites \u00b6 General Setup has been completed. Install Dependencies \u00b6 Install utilities and RabbitMQ Open an admin powershell to <repo-root> . Install dependency packages: pdm run setup-win install.broker-deps Open Required Ports \u00b6 Open the relevant ports up so worker and client nodes can connect to the broker. Note If you only intend to use the broker with workers and clients on the same machine then you don't need to open ports. Option 1: Open only <broker-port> . (Default: 5672) \u00b6 The instructions for this are too configuration specific for us to provide. Option 2: Disable broker firewalls entirely. \u00b6 We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Warning This is not secure at all. Do not do this if the license server is at all accessible from public or untrusted computers. Disable the Windows Server 2019 firewall: pdm run setup-win license-server.disable-firewall Note: This might work with other versions of Windows but that hasn't been tested. Preserve Settings \u00b6 Keep this machine's IP as: <broker-ip> Keep rabbitmq's open port as: <broker-port> (default: 5672) Start RabbitMQ Service \u00b6 Start the RabbitMQ service: Start Menu -> RabbitMQ Server -> RabbitMQ Service Start The server should automatically start on boot.","title":"Message Broker"},{"location":"setup/broker/#message-broker-setup","text":"A message broker allows worker nodes to pull analysis requests from a shared queue. The standard message broker is RabbitMQ with the default settings.","title":"Message Broker Setup"},{"location":"setup/broker/#option-1-run-rabbitmq-on-a-linux-machine-recommended","text":"RabbitMQ is a backendless message broker that's sufficient if you're okay with clients not being notified when the analysis result is complete. (It will just appear in the results directory.) Follow the instructions here to set up RabbitMQ. Save this machine's IP as: <broker-ip> Save rabbitmq's open port as: <broker-port> (default: 5672)","title":"Option 1: Run RabbitMQ on a Linux Machine (Recommended)"},{"location":"setup/broker/#option-2-run-redis-on-a-linux-machine","text":"Redis can be configured as a response backend as well as a message broker. This means that clients can get notified when an analysis is complete. We have not tried to get this running so don't have install instructions. Save this machine's IP as: <broker-ip> Save redis's open port as: <broker-port> (default: 6379) Save the redis database you're using as: <broker-db> (default: \"0\")","title":"Option 2: Run Redis on a Linux Machine"},{"location":"setup/broker/#option-3-run-rabbitmq-as-a-windows-service","text":"This runs RabbitMQ as a windows service and is fine for development or production setups.","title":"Option 3: Run RabbitMQ as a Windows Service"},{"location":"setup/broker/#prerequisites","text":"General Setup has been completed.","title":"Prerequisites"},{"location":"setup/broker/#install-dependencies","text":"Install utilities and RabbitMQ Open an admin powershell to <repo-root> . Install dependency packages: pdm run setup-win install.broker-deps","title":"Install Dependencies"},{"location":"setup/broker/#open-required-ports","text":"Open the relevant ports up so worker and client nodes can connect to the broker. Note If you only intend to use the broker with workers and clients on the same machine then you don't need to open ports.","title":"Open Required Ports"},{"location":"setup/broker/#option-1-open-only-broker-port-default-5672","text":"The instructions for this are too configuration specific for us to provide.","title":"Option 1: Open only &lt;broker-port&gt;. (Default: 5672)"},{"location":"setup/broker/#option-2-disable-broker-firewalls-entirely","text":"We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Warning This is not secure at all. Do not do this if the license server is at all accessible from public or untrusted computers. Disable the Windows Server 2019 firewall: pdm run setup-win license-server.disable-firewall Note: This might work with other versions of Windows but that hasn't been tested.","title":"Option 2: Disable broker firewalls entirely."},{"location":"setup/broker/#preserve-settings","text":"Keep this machine's IP as: <broker-ip> Keep rabbitmq's open port as: <broker-port> (default: 5672)","title":"Preserve Settings"},{"location":"setup/broker/#start-rabbitmq-service","text":"Start the RabbitMQ service: Start Menu -> RabbitMQ Server -> RabbitMQ Service Start The server should automatically start on boot.","title":"Start RabbitMQ Service"},{"location":"setup/client/","text":"Client Node Setup \u00b6 Clients send design analysis requests to a message broker so that worker nodes can process them. Unlike the other nodes clients can easily be of any platform as long as they can access the same brokers and results directory as the workers. Prerequisites \u00b6 SSH keys or credentials for git.isis.vanderbilt.edu . A broker running at <broker-ip> and <broker-port> . The following installed software: Git Python (>=3.9, <3.11) An environment with the following Python packages: PDM Setup File Sharing (Optional) \u00b6 If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> . Download SimpleUAM \u00b6 Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as <repo-root> . Option 1: Clone from Github (HTTP): \u00b6 git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Option 2: Clone from Github (SSH): \u00b6 git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Initialize SimpleUAM Package \u00b6 Initialize pdm and packages for client use. Navigate a shell to <repo_root> . Setup PDM environment for this repo: pdm install Test whether setup script was installed: pdm run d2c-client --help Result should be a help message showing all of d2c-client 's flags and subcommands. Get Configuration Directory \u00b6 The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate a shell to <repo_root> . Print config directory: pdm run suam-config dir Save result as <config-dir> . Configure Broker Settings \u00b6 The client process needs to be configured with how to connect to a message broker. These options should be identical to any worker nodes which is why they use the same config file. Open a shell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Update the config at <config-dir>/d2c_worker.conf.yaml : Set broker.protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set broker.host to <broker-ip> . Set broker.port to <broker-port> . If using Redis, set broker.db to <broker-db> See the config file guide for more detailed instructions and information. Further details on configuring the client's broker and backend are here ... Test the Client Node (Optional) \u00b6 Run a simple test task, generating the info files for a design, in order to test the client node's configuration. Have a valid design file (usually design_swri.json ) at <design-file> . Have a broker running at the configured location. Have at least one worker running and connected to the broker. Open a shell to <repo-root> . Test generating design info files: pdm run d2c-client gen-info-files --input = <design-file> A worker should pick up this task and run it, eventually placing an archive in the <results-dir> with the generated info files. Information on how to run a fill analysis pipeline, through either a CLI or Python code, can be found here ...","title":"Client Node"},{"location":"setup/client/#client-node-setup","text":"Clients send design analysis requests to a message broker so that worker nodes can process them. Unlike the other nodes clients can easily be of any platform as long as they can access the same brokers and results directory as the workers.","title":"Client Node Setup"},{"location":"setup/client/#prerequisites","text":"SSH keys or credentials for git.isis.vanderbilt.edu . A broker running at <broker-ip> and <broker-port> . The following installed software: Git Python (>=3.9, <3.11) An environment with the following Python packages: PDM","title":"Prerequisites"},{"location":"setup/client/#setup-file-sharing-optional","text":"If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> .","title":"Setup File Sharing (Optional)"},{"location":"setup/client/#download-simpleuam","text":"Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as <repo-root> .","title":"Download SimpleUAM"},{"location":"setup/client/#option-1-clone-from-github-http","text":"git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 1: Clone from Github (HTTP):"},{"location":"setup/client/#option-2-clone-from-github-ssh","text":"git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 2: Clone from Github (SSH):"},{"location":"setup/client/#initialize-simpleuam-package","text":"Initialize pdm and packages for client use. Navigate a shell to <repo_root> . Setup PDM environment for this repo: pdm install Test whether setup script was installed: pdm run d2c-client --help Result should be a help message showing all of d2c-client 's flags and subcommands.","title":"Initialize SimpleUAM Package"},{"location":"setup/client/#get-configuration-directory","text":"The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate a shell to <repo_root> . Print config directory: pdm run suam-config dir Save result as <config-dir> .","title":"Get Configuration Directory"},{"location":"setup/client/#configure-broker-settings","text":"The client process needs to be configured with how to connect to a message broker. These options should be identical to any worker nodes which is why they use the same config file. Open a shell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Update the config at <config-dir>/d2c_worker.conf.yaml : Set broker.protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set broker.host to <broker-ip> . Set broker.port to <broker-port> . If using Redis, set broker.db to <broker-db> See the config file guide for more detailed instructions and information. Further details on configuring the client's broker and backend are here ...","title":"Configure Broker Settings"},{"location":"setup/client/#test-the-client-node-optional","text":"Run a simple test task, generating the info files for a design, in order to test the client node's configuration. Have a valid design file (usually design_swri.json ) at <design-file> . Have a broker running at the configured location. Have at least one worker running and connected to the broker. Open a shell to <repo-root> . Test generating design info files: pdm run d2c-client gen-info-files --input = <design-file> A worker should pick up this task and run it, eventually placing an archive in the <results-dir> with the generated info files. Information on how to run a fill analysis pipeline, through either a CLI or Python code, can be found here ...","title":"Test the Client Node (Optional)"},{"location":"setup/corpus/","text":"Static Corpus Setup \u00b6 The graph database is far too bloated for practical use, instead we can export the relevant information to a small json file and use that directly. This file is needed for many of the tasks that worker nodes will do and needs to be retrieved or generated. Option 1: Use static corpus provided in repo (Recommended) \u00b6 This is the corpus for all_schema_uam.graphml generated in June '22. Prerequisites \u00b6 General Setup has been completed. Install Static Corpus \u00b6 Once configured open admin powershell at <repo-root> . Install the corpus provided with this repo: pdm run craidl static-corpus.copy Option 2: User a user provided static corpus \u00b6 This will just load the user provided file into the install location. Prerequisites \u00b6 General Setup has been completed. The corpus we're installing is at <static-corpus-loc> . Install Static Corpus \u00b6 Once configured open admin powershell at <repo-root> . Install a user provided corpus from <static-corpus-loc> pdm run craidl static-corpus.copy --input = <static-corpus-loc> Option 3: Generate Static Corpus from a Corpus Database \u00b6 This will generate a static corpus from a running graph server, using whatever corpus it was configured with. Prerequisites \u00b6 General Setup has been completed. Configure Server Settings \u00b6 The config file at <config-dir>/craidl.conf.yaml stores information for connecting to a corpus DB server. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r The fields under server_host and server_port determine which corpus database this tool will connect to when generating a static corpus. The default options connect to a stub server running on the same machine. (Optional) If you're connecting to a corpus database not running on the current machine then update <config-dir>/craidl.conf.yaml . Set server_host to <corpus-db-ip> . Set server_port to <corpus-db-port> . Generate Static Corpus \u00b6 Ensure the server at <corpus-db-ip> is currently running. Generate the static corpus from that server. pdm run craidl static-corpus.generate With default settings the corpus generation and the stub server can hang intermittently, so the generation command periodically saves its progress. Just rerun the command (with the same settings) and it will resume generating the corpus from the last saved cluster of components. See the section on using Craidl for information on how to use the generated static corpus...","title":"Static Corpus"},{"location":"setup/corpus/#static-corpus-setup","text":"The graph database is far too bloated for practical use, instead we can export the relevant information to a small json file and use that directly. This file is needed for many of the tasks that worker nodes will do and needs to be retrieved or generated.","title":"Static Corpus Setup"},{"location":"setup/corpus/#option-1-use-static-corpus-provided-in-repo-recommended","text":"This is the corpus for all_schema_uam.graphml generated in June '22.","title":"Option 1: Use static corpus provided in repo (Recommended)"},{"location":"setup/corpus/#prerequisites","text":"General Setup has been completed.","title":"Prerequisites"},{"location":"setup/corpus/#install-static-corpus","text":"Once configured open admin powershell at <repo-root> . Install the corpus provided with this repo: pdm run craidl static-corpus.copy","title":"Install Static Corpus"},{"location":"setup/corpus/#option-2-user-a-user-provided-static-corpus","text":"This will just load the user provided file into the install location.","title":"Option 2: User a user provided static corpus"},{"location":"setup/corpus/#prerequisites_1","text":"General Setup has been completed. The corpus we're installing is at <static-corpus-loc> .","title":"Prerequisites"},{"location":"setup/corpus/#install-static-corpus_1","text":"Once configured open admin powershell at <repo-root> . Install a user provided corpus from <static-corpus-loc> pdm run craidl static-corpus.copy --input = <static-corpus-loc>","title":"Install Static Corpus"},{"location":"setup/corpus/#option-3-generate-static-corpus-from-a-corpus-database","text":"This will generate a static corpus from a running graph server, using whatever corpus it was configured with.","title":"Option 3: Generate Static Corpus from a Corpus Database"},{"location":"setup/corpus/#prerequisites_2","text":"General Setup has been completed.","title":"Prerequisites"},{"location":"setup/corpus/#configure-server-settings","text":"The config file at <config-dir>/craidl.conf.yaml stores information for connecting to a corpus DB server. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r The fields under server_host and server_port determine which corpus database this tool will connect to when generating a static corpus. The default options connect to a stub server running on the same machine. (Optional) If you're connecting to a corpus database not running on the current machine then update <config-dir>/craidl.conf.yaml . Set server_host to <corpus-db-ip> . Set server_port to <corpus-db-port> .","title":"Configure Server Settings"},{"location":"setup/corpus/#generate-static-corpus","text":"Ensure the server at <corpus-db-ip> is currently running. Generate the static corpus from that server. pdm run craidl static-corpus.generate With default settings the corpus generation and the stub server can hang intermittently, so the generation command periodically saves its progress. Just rerun the command (with the same settings) and it will resume generating the corpus from the last saved cluster of components. See the section on using Craidl for information on how to use the generated static corpus...","title":"Generate Static Corpus"},{"location":"setup/general/","text":"General Machine Setup \u00b6 This is setup that needs to be done for multiple types of machines. Prerequisites \u00b6 This machine must be running Windows. If you're using a Linux machine skip to the setup page of whatever component type it's meant to host. You must be okay using chocolatey as your package manager. Install Chocolatey and Minimal Deps \u00b6 This downloads a minimal install script for chocolatey, git, and python. This step is idempotent and will just do nothing if these are all installed. Open admin powershell and run: iwr -Uri https://raw.githubusercontent.com/LOGiCS-Project/swri-simple-uam-pipeline/main/data/setup/bootstrap_win.ps1 | iex Close this powershell terminal and open new ones for future steps. Download SimpleUAM \u00b6 Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as <repo-root> . Option 1: Clone from Github (HTTP): \u00b6 git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Option 2: Clone from Github (SSH): \u00b6 git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Initialize Setup Package \u00b6 Initialize pdm and packages for worker setup. Navigate an admin powershell to <repo_root> . Setup PDM environment for this repo: pdm install -d Test whether setup script was installed: pdm run setup-win --help Result should be a help message showing all of setup-win 's flags and subcommands. Get Configuration Directory \u00b6 The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate an admin powershell to <repo_root> . Print config directory: pdm run suam-config dir Save result as <config-dir> . Install Quality of Life Packages (Optional) \u00b6 This installs Firefox, Notepad++, Tess, and other applications that make working on a new windows install more bearable. Navigate an admin powershell to <repo_root> . Install the packages: pdm run setup-win install.qol-deps Setup File Sharing (Optional) \u00b6 If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> . Install SSH keys for git.isis.vanderbilt.edu (Optional) \u00b6 SSH access to Isis, while not strictly necessary, will make future install steps easier and more secure. Follow the instructions here to set up ssh key based access to the isis server. Further Setup \u00b6 Once this setup is complete you can continue to one or more of the following steps. All of the following nodes can coexist on a single windows instance. Continue to Creo License Server Setup ... Or to Message Broker Setup ... Or to Corpus DB Setup ... Or to Static Corpus Setup ... Or to Worker Node Setup ...","title":"General Setup"},{"location":"setup/general/#general-machine-setup","text":"This is setup that needs to be done for multiple types of machines.","title":"General Machine Setup"},{"location":"setup/general/#prerequisites","text":"This machine must be running Windows. If you're using a Linux machine skip to the setup page of whatever component type it's meant to host. You must be okay using chocolatey as your package manager.","title":"Prerequisites"},{"location":"setup/general/#install-chocolatey-and-minimal-deps","text":"This downloads a minimal install script for chocolatey, git, and python. This step is idempotent and will just do nothing if these are all installed. Open admin powershell and run: iwr -Uri https://raw.githubusercontent.com/LOGiCS-Project/swri-simple-uam-pipeline/main/data/setup/bootstrap_win.ps1 | iex Close this powershell terminal and open new ones for future steps.","title":"Install Chocolatey and Minimal Deps"},{"location":"setup/general/#download-simpleuam","text":"Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as <repo-root> .","title":"Download SimpleUAM"},{"location":"setup/general/#option-1-clone-from-github-http","text":"git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 1: Clone from Github (HTTP):"},{"location":"setup/general/#option-2-clone-from-github-ssh","text":"git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 2: Clone from Github (SSH):"},{"location":"setup/general/#initialize-setup-package","text":"Initialize pdm and packages for worker setup. Navigate an admin powershell to <repo_root> . Setup PDM environment for this repo: pdm install -d Test whether setup script was installed: pdm run setup-win --help Result should be a help message showing all of setup-win 's flags and subcommands.","title":"Initialize Setup Package"},{"location":"setup/general/#get-configuration-directory","text":"The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate an admin powershell to <repo_root> . Print config directory: pdm run suam-config dir Save result as <config-dir> .","title":"Get Configuration Directory"},{"location":"setup/general/#install-quality-of-life-packages-optional","text":"This installs Firefox, Notepad++, Tess, and other applications that make working on a new windows install more bearable. Navigate an admin powershell to <repo_root> . Install the packages: pdm run setup-win install.qol-deps","title":"Install Quality of Life Packages (Optional)"},{"location":"setup/general/#setup-file-sharing-optional","text":"If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> .","title":"Setup File Sharing (Optional)"},{"location":"setup/general/#install-ssh-keys-for-gitisisvanderbiltedu-optional","text":"SSH access to Isis, while not strictly necessary, will make future install steps easier and more secure. Follow the instructions here to set up ssh key based access to the isis server.","title":"Install SSH keys for git.isis.vanderbilt.edu (Optional)"},{"location":"setup/general/#further-setup","text":"Once this setup is complete you can continue to one or more of the following steps. All of the following nodes can coexist on a single windows instance. Continue to Creo License Server Setup ... Or to Message Broker Setup ... Or to Corpus DB Setup ... Or to Static Corpus Setup ... Or to Worker Node Setup ...","title":"Further Setup"},{"location":"setup/graph/","text":"Corpus Database Setup \u00b6 SWRi distributes their corpus of design data as a graph database that needs to be queried for various tasks. Option 1: Running a Local Stub Server (Recommended) \u00b6 This is a local, minimal version of a corpus DB that uses TinkerPop 's reference graph db implementation. It is not persistent so no changes will be preserved between restarts. Prerequisites \u00b6 General Setup has been completed. If not using the default corpus have the .graphml corpus ready at <corpus-loc> . If using the default corpus have SSH keys or credentials for git.isis.vanderbilt.edu set up. Install Dependencies \u00b6 Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.graph-deps Reboot the machine. Install GraphML Corpus \u00b6 We need to install the GraphML corpus to a default location so future setup steps can find it. Option 1 : Download default from athens-uav-workflows \u00b6 Open an admin powershell at <repo-root> . Automatically download athens-uav-workflows repo and get all_schema_uam.graphml : pdm run craidl corpus.download Follow prompts, entering in git.isis.vanderbilt.edu credentials if needed. Install corpus to default location: pdm run craidl corpus.install Option 2 : Install corpus from user provided file \u00b6 Open admin powershell to <repo-root> . Place your corpus on the machine at <corpus-loc> . Install user provided corpus from <corpus-loc> : pdm run craidl corpus.install --corpus = <corpus-loc> Configure Corpus DB Server \u00b6 The config file at <config-dir>/craidl.conf.yaml stores information for running SimpleUAM's stub corpus database. Those settings are then used to configure the stub server. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r The fields under stub_server.host and stub_server.port determine how the corpus is served. (Optional) If serving the graph to other machines then update the stub_server.host config property. Set stub_server.host to 0.0.0.0 . See the config file guide for more detailed instructions and information. Install and configure corpus DB: pdm run craidl stub-server.configure In the absence of any arguments this uses the configured host, port, and graphml corpus from <corpus-dir>/craid.conf.yaml . Open Required Ports (Optional) \u00b6 Open the relevant ports up so that non-local worker nodes can connect to the stub database. We do not recommend non-local connections to a stub corpus database. Note If you only intend to connect to a local worker node then you don't need to open any ports up. Option 1: Open only port 8182. \u00b6 The instructions for this are too configuration specific for us to provide. Option 2: Disable license server firewalls entirely. \u00b6 We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Warning This is not secure at all. Do not do this if the license server is at all accessible from public or untrusted computers. Disable the Windows Server 2019 firewall: pdm run setup-win license-server.disable-firewall Note: This might work with other versions of Windows but that hasn't been tested. Preserve Settings \u00b6 If you're connecting from a different machine. Keep this machine's IP as: <corpus-db-ip> Keep the database's open port as: <corpus-db-port> (default: 8182) Next Steps \u00b6 See the section on using Craidl for information on how to run and use this server... Option 2: Run a full corpus database. \u00b6 We try to avoid using the SWRi provided graph database and haven't explored alternate options, so this is up to you. Preserve Settings \u00b6 If you're connecting from a different machine. Keep this machine's IP as: <corpus-db-ip> Keep the database's open port as: <corpus-db-port> (default: 8182)","title":"Corpus Database"},{"location":"setup/graph/#corpus-database-setup","text":"SWRi distributes their corpus of design data as a graph database that needs to be queried for various tasks.","title":"Corpus Database Setup"},{"location":"setup/graph/#option-1-running-a-local-stub-server-recommended","text":"This is a local, minimal version of a corpus DB that uses TinkerPop 's reference graph db implementation. It is not persistent so no changes will be preserved between restarts.","title":"Option 1: Running a Local Stub Server (Recommended)"},{"location":"setup/graph/#prerequisites","text":"General Setup has been completed. If not using the default corpus have the .graphml corpus ready at <corpus-loc> . If using the default corpus have SSH keys or credentials for git.isis.vanderbilt.edu set up.","title":"Prerequisites"},{"location":"setup/graph/#install-dependencies","text":"Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.graph-deps Reboot the machine.","title":"Install Dependencies"},{"location":"setup/graph/#install-graphml-corpus","text":"We need to install the GraphML corpus to a default location so future setup steps can find it.","title":"Install GraphML Corpus"},{"location":"setup/graph/#option-1-download-default-from-athens-uav-workflows","text":"Open an admin powershell at <repo-root> . Automatically download athens-uav-workflows repo and get all_schema_uam.graphml : pdm run craidl corpus.download Follow prompts, entering in git.isis.vanderbilt.edu credentials if needed. Install corpus to default location: pdm run craidl corpus.install","title":"Option 1: Download default from athens-uav-workflows"},{"location":"setup/graph/#option-2-install-corpus-from-user-provided-file","text":"Open admin powershell to <repo-root> . Place your corpus on the machine at <corpus-loc> . Install user provided corpus from <corpus-loc> : pdm run craidl corpus.install --corpus = <corpus-loc>","title":"Option 2: Install corpus from user provided file"},{"location":"setup/graph/#configure-corpus-db-server","text":"The config file at <config-dir>/craidl.conf.yaml stores information for running SimpleUAM's stub corpus database. Those settings are then used to configure the stub server. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r The fields under stub_server.host and stub_server.port determine how the corpus is served. (Optional) If serving the graph to other machines then update the stub_server.host config property. Set stub_server.host to 0.0.0.0 . See the config file guide for more detailed instructions and information. Install and configure corpus DB: pdm run craidl stub-server.configure In the absence of any arguments this uses the configured host, port, and graphml corpus from <corpus-dir>/craid.conf.yaml .","title":"Configure Corpus DB Server"},{"location":"setup/graph/#open-required-ports-optional","text":"Open the relevant ports up so that non-local worker nodes can connect to the stub database. We do not recommend non-local connections to a stub corpus database. Note If you only intend to connect to a local worker node then you don't need to open any ports up.","title":"Open Required Ports (Optional)"},{"location":"setup/graph/#option-1-open-only-port-8182","text":"The instructions for this are too configuration specific for us to provide.","title":"Option 1: Open only port 8182."},{"location":"setup/graph/#option-2-disable-license-server-firewalls-entirely","text":"We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Warning This is not secure at all. Do not do this if the license server is at all accessible from public or untrusted computers. Disable the Windows Server 2019 firewall: pdm run setup-win license-server.disable-firewall Note: This might work with other versions of Windows but that hasn't been tested.","title":"Option 2: Disable license server firewalls entirely."},{"location":"setup/graph/#preserve-settings","text":"If you're connecting from a different machine. Keep this machine's IP as: <corpus-db-ip> Keep the database's open port as: <corpus-db-port> (default: 8182)","title":"Preserve Settings"},{"location":"setup/graph/#next-steps","text":"See the section on using Craidl for information on how to run and use this server...","title":"Next Steps"},{"location":"setup/graph/#option-2-run-a-full-corpus-database","text":"We try to avoid using the SWRi provided graph database and haven't explored alternate options, so this is up to you.","title":"Option 2: Run a full corpus database."},{"location":"setup/graph/#preserve-settings_1","text":"If you're connecting from a different machine. Keep this machine's IP as: <corpus-db-ip> Keep the database's open port as: <corpus-db-port> (default: 8182)","title":"Preserve Settings"},{"location":"setup/intro/","text":"SimpleUAM Installation Guide \u00b6 Info We would appreciate assistance in making this guide better. Any notes on issues with the install process, lack of clarity in wording, or other improvements would be appreciated. SimpleUAM Component Structure The core goal of SimpleUAM is allow users to set up a service for processing requests to analyze UAM and UAV designs. Client nodes, such as optimizers or search tools, should be able to queue requests for distribution to worker nodes as they become available. The results of those analyses, packaged as zip files, should then be made available to the clients as they're completed. In order to form a complete SimpleUAM service some core requirements need to be met: There needs to be a configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to an engineering corpus, either through a graph database holding all the data or through a static corpus file which has the same information. Each worker has to have access to results storage, a folder on the local file system or a network drive, where zip files with results from each analysis will be placed. There must be a message broker which workers can connect to in order to retrieve open analysis requests. Client nodes should be able to access the results store to get the archives with the analysis results. With those requirements met, a client node can then send requests to the message broker and watch the results store for their completed analyses. Choosing a Service Topology \u00b6 It's possible to distribute SimpleUAM components between multiple machines in numerous ways that meet the given requirements. Picking a topology, specifically the components that go on each individual machine, tells you which installation steps are needed for that machine. We'll look at two example topologies, one I use for (semi)local development work and one for a potential production system. Development SimpleUAM System This development setup has a local machine and a single worker. The local machine is set up so it can run a SimpleUAM client and so that any code shared with a worker node can be edited in whatever manner the user is comfortable with. The worker node then has all the other necessary components of the service, including broker, license, and corpus. The structure is a broad guideline and can be tweaked as needed. For instance, if you're running windows you can just run all the components on your local machine and use a stub message broker that will run analysis requests as blocking calls. Production SimpleUAM System The production service has significantly more going on. There are one or more clients producing analysis requests, multiple workers processing them, a Creo license server, a message broker, and a results server. Other topologies are also viable, for instance running a central graph database for all the workers to share instead of relying on a local, static corpus. The important part is knowing what components you want on each machine. Command Line Interfaces \u00b6 All the command line scripts SimpleUAM provides are made using Invoke and evaluated within a PDM administered python environment. This means that all the SimpleUAM provided commands must be run from <repo-root> and have this format: pdm run <command> All the core SimpleUAM commands suam-config , setup-win , craidl , d2c-workspace , and d2c-client will print a help message when run without arguments. In their base form these commands are safe and will never make change to your system. The help messages also provide a list of subcommands that do perform various tasks. These subcommands are run with: pdm run <command> <sub-command> [ ARGS ] All of these subcommands come with detailed help information that can be accessed with: pdm run <command> <sub-command> --help These help messages are worth checking for available options and notes. Configuration \u00b6 SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. While you can find a more detailed breakdown of the system here , this is a quick overview. Once the SimpleUAM project is installed (in General Setup ) you can run the following command to find the config file directory: pdm run suam-config dir Files placed there will be loaded when most SimpleUAM code is started up. The configuration is immutable for the runtime of a program and changes will require a restart to register. You can get a printout of the current configuration state with the following: pdm run suam-config print --all Sample Output of pdm run suam-config print --all ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - rsync worker_pip_packages : - psutil - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : ${path:data_directory}/craidl_examples stub_server : cache_dir : ${path:cache_directory}/corpus_stub_cache server_dir : ${path:data_directory}/corpus_stub_server graphml_corpus : ${path:data_directory}/corpus_stub.graphml host : localhost port : 8182 read_only : false server_host : ${stub_server.host} server_port : ${stub_server.port} static_corpus : ${path:data_directory}/corpus_static_dump.json static_corpus_cache : ${path:cache_directory}/static_corpus_cache use_static_corpus : true ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : ${workspaces_dir}/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json max_workspaces : 4 workspaces_dir : ${path:work_directory}/d2c_workspaces cache_dir : ${path:cache_directory}/d2c_workspaces exclude : - .git exclude_from : [] result_exclude : - .git result_exclude_from : [] ### d2c_worker.conf.yaml ### broker : protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : ${.protocol}://${.host}:${.port}${.db} backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : ${.protocol}://${.host}:${.port}/${.db} max_processes : ${d2c_workspace:max_workspaces} max_threads : 1 shutdown_timeout : 600000 skip_logging : false If you want to see the full expanded version of the configs, with all the interpolations resolved, add the --resolved flag. Sample Output of pdm run suam-config print --all --resolved ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - rsync worker_pip_packages : - psutil - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : /usr/share/budgie-desktop/SimpleUAM/data/craidl_examples stub_server : cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/corpus_stub_cache server_dir : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub_server graphml_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub.graphml host : localhost port : 8182 read_only : false server_host : localhost server_port : 8182 static_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_static_dump.json static_corpus_cache : /usr/share/budgie-desktop/SimpleUAM/cache/static_corpus_cache use_static_corpus : true ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json max_workspaces : 4 workspaces_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/d2c_workspaces exclude : - .git exclude_from : [] result_exclude : - .git result_exclude_from : [] ### d2c_worker.conf.yaml ### broker : protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : amqp://127.0.0.1:5672 backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : redis://127.0.0.1:6379/0 max_processes : 4 max_threads : 1 shutdown_timeout : 600000 skip_logging : false You can also use the write subcommand to write sample config files out to the appropriate locations. Run the following for more info: pdm run suam-config write --help Config files can be partial and do not need to define every possible key. Keys that are missing will just use their default values. Overriding Configuration Fields. Consider the following defaults for example.conf.yaml : ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'default' field-1 : 'default' field-2 : 'default' With the following example.conf.yaml actually on disk: ### example.conf.yaml defaults ### subsection : subfield-2 : 'modified' field-1 : 'modifed' The final loaded values for example.conf.yaml as seen by the application would be: ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'modified' field-1 : 'modified' field-2 : 'default' When describing keys in a config file, we'll use dot notation. Config File Dot Notation Consider the following config file: subsection : subfield-1 : 'sub-1' subfield-2 : 'sub-2' field-1 : 'fld-1' field-2 : 'fld-2' Then field-1 would have value 'fld-1' and subsection.subfield-1 would have value 'sub-1' Likewise, setting foo to 3 and bar.buzz to 4 would leave you with the following file: foo : 3 bar : buzz : 4 Further details are here ... Placeholder Conventions \u00b6 Throughout these install instructions, but especially in the AWS setup, we use placeholders like <this-one> to represent user provided information or things that might be needed later. This guide tries to proactive about asking you to save potentially useful information. We recommend keeping a file open for this. Placeholder file from partway through AWS setup. aws-vpc : suam-project1 aws-vpc-id : vpc-0c2ca2caaf403057f aws-elastic-ip : 44.206.27.84 aws-public-subnet : subnet-0cfe362170f73830d aws-private-subnet : subnet-0a94695b5b8b8c9c2 aws-default-sg : sg-03e11656feb78ca14 aws-cert-id : 856062a4-b146-4c58-893c-72205a6c47ee aws-cert-arn : arn:aws:acm:us-east-1:689242578769:certificate/856062a4-b146-4c58-893c-72205a6c47ee We never use this information programmatically, so use whatever format you want, but it does make it easier to keep track of what you're doing during install. This is particularly important if you are setting up multiple machines and don't want to waste time. AWS Network Setup \u00b6 If you are using AWS you can start with our instructions for setting up a virtual private cloud (VPC). It sets up a private subnet for non-client machines and a VPN and Network drive for access to that private subnet. AWS (Network) Setup Machine Setup \u00b6 Installation for each machine requires following the other pages in this section in order, skipping any that aren't relevant and always including general setup. Try to setup machines with centralized functions, like the license server and message broker, before the worker nodes. AWS (Instance) Setup General Setup (Required) Creo License Server Message Broker Engineering Corpus Worker Node Client nodes are less directly dependent on the SimpleUAM start and their setup can skip directly to the corresponding section: Client Node","title":"Introduction"},{"location":"setup/intro/#simpleuam-installation-guide","text":"Info We would appreciate assistance in making this guide better. Any notes on issues with the install process, lack of clarity in wording, or other improvements would be appreciated. SimpleUAM Component Structure The core goal of SimpleUAM is allow users to set up a service for processing requests to analyze UAM and UAV designs. Client nodes, such as optimizers or search tools, should be able to queue requests for distribution to worker nodes as they become available. The results of those analyses, packaged as zip files, should then be made available to the clients as they're completed. In order to form a complete SimpleUAM service some core requirements need to be met: There needs to be a configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to an engineering corpus, either through a graph database holding all the data or through a static corpus file which has the same information. Each worker has to have access to results storage, a folder on the local file system or a network drive, where zip files with results from each analysis will be placed. There must be a message broker which workers can connect to in order to retrieve open analysis requests. Client nodes should be able to access the results store to get the archives with the analysis results. With those requirements met, a client node can then send requests to the message broker and watch the results store for their completed analyses.","title":"SimpleUAM Installation Guide"},{"location":"setup/intro/#choosing-a-service-topology","text":"It's possible to distribute SimpleUAM components between multiple machines in numerous ways that meet the given requirements. Picking a topology, specifically the components that go on each individual machine, tells you which installation steps are needed for that machine. We'll look at two example topologies, one I use for (semi)local development work and one for a potential production system. Development SimpleUAM System This development setup has a local machine and a single worker. The local machine is set up so it can run a SimpleUAM client and so that any code shared with a worker node can be edited in whatever manner the user is comfortable with. The worker node then has all the other necessary components of the service, including broker, license, and corpus. The structure is a broad guideline and can be tweaked as needed. For instance, if you're running windows you can just run all the components on your local machine and use a stub message broker that will run analysis requests as blocking calls. Production SimpleUAM System The production service has significantly more going on. There are one or more clients producing analysis requests, multiple workers processing them, a Creo license server, a message broker, and a results server. Other topologies are also viable, for instance running a central graph database for all the workers to share instead of relying on a local, static corpus. The important part is knowing what components you want on each machine.","title":"Choosing a Service Topology"},{"location":"setup/intro/#command-line-interfaces","text":"All the command line scripts SimpleUAM provides are made using Invoke and evaluated within a PDM administered python environment. This means that all the SimpleUAM provided commands must be run from <repo-root> and have this format: pdm run <command> All the core SimpleUAM commands suam-config , setup-win , craidl , d2c-workspace , and d2c-client will print a help message when run without arguments. In their base form these commands are safe and will never make change to your system. The help messages also provide a list of subcommands that do perform various tasks. These subcommands are run with: pdm run <command> <sub-command> [ ARGS ] All of these subcommands come with detailed help information that can be accessed with: pdm run <command> <sub-command> --help These help messages are worth checking for available options and notes.","title":"Command Line Interfaces"},{"location":"setup/intro/#configuration","text":"SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. While you can find a more detailed breakdown of the system here , this is a quick overview. Once the SimpleUAM project is installed (in General Setup ) you can run the following command to find the config file directory: pdm run suam-config dir Files placed there will be loaded when most SimpleUAM code is started up. The configuration is immutable for the runtime of a program and changes will require a restart to register. You can get a printout of the current configuration state with the following: pdm run suam-config print --all Sample Output of pdm run suam-config print --all ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - rsync worker_pip_packages : - psutil - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : ${path:data_directory}/craidl_examples stub_server : cache_dir : ${path:cache_directory}/corpus_stub_cache server_dir : ${path:data_directory}/corpus_stub_server graphml_corpus : ${path:data_directory}/corpus_stub.graphml host : localhost port : 8182 read_only : false server_host : ${stub_server.host} server_port : ${stub_server.port} static_corpus : ${path:data_directory}/corpus_static_dump.json static_corpus_cache : ${path:cache_directory}/static_corpus_cache use_static_corpus : true ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : ${workspaces_dir}/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json max_workspaces : 4 workspaces_dir : ${path:work_directory}/d2c_workspaces cache_dir : ${path:cache_directory}/d2c_workspaces exclude : - .git exclude_from : [] result_exclude : - .git result_exclude_from : [] ### d2c_worker.conf.yaml ### broker : protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : ${.protocol}://${.host}:${.port}${.db} backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : ${.protocol}://${.host}:${.port}/${.db} max_processes : ${d2c_workspace:max_workspaces} max_threads : 1 shutdown_timeout : 600000 skip_logging : false If you want to see the full expanded version of the configs, with all the interpolations resolved, add the --resolved flag. Sample Output of pdm run suam-config print --all --resolved ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - rsync worker_pip_packages : - psutil - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : /usr/share/budgie-desktop/SimpleUAM/data/craidl_examples stub_server : cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/corpus_stub_cache server_dir : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub_server graphml_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub.graphml host : localhost port : 8182 read_only : false server_host : localhost server_port : 8182 static_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_static_dump.json static_corpus_cache : /usr/share/budgie-desktop/SimpleUAM/cache/static_corpus_cache use_static_corpus : true ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json max_workspaces : 4 workspaces_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/d2c_workspaces exclude : - .git exclude_from : [] result_exclude : - .git result_exclude_from : [] ### d2c_worker.conf.yaml ### broker : protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : amqp://127.0.0.1:5672 backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : redis://127.0.0.1:6379/0 max_processes : 4 max_threads : 1 shutdown_timeout : 600000 skip_logging : false You can also use the write subcommand to write sample config files out to the appropriate locations. Run the following for more info: pdm run suam-config write --help Config files can be partial and do not need to define every possible key. Keys that are missing will just use their default values. Overriding Configuration Fields. Consider the following defaults for example.conf.yaml : ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'default' field-1 : 'default' field-2 : 'default' With the following example.conf.yaml actually on disk: ### example.conf.yaml defaults ### subsection : subfield-2 : 'modified' field-1 : 'modifed' The final loaded values for example.conf.yaml as seen by the application would be: ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'modified' field-1 : 'modified' field-2 : 'default' When describing keys in a config file, we'll use dot notation. Config File Dot Notation Consider the following config file: subsection : subfield-1 : 'sub-1' subfield-2 : 'sub-2' field-1 : 'fld-1' field-2 : 'fld-2' Then field-1 would have value 'fld-1' and subsection.subfield-1 would have value 'sub-1' Likewise, setting foo to 3 and bar.buzz to 4 would leave you with the following file: foo : 3 bar : buzz : 4 Further details are here ...","title":"Configuration"},{"location":"setup/intro/#placeholder-conventions","text":"Throughout these install instructions, but especially in the AWS setup, we use placeholders like <this-one> to represent user provided information or things that might be needed later. This guide tries to proactive about asking you to save potentially useful information. We recommend keeping a file open for this. Placeholder file from partway through AWS setup. aws-vpc : suam-project1 aws-vpc-id : vpc-0c2ca2caaf403057f aws-elastic-ip : 44.206.27.84 aws-public-subnet : subnet-0cfe362170f73830d aws-private-subnet : subnet-0a94695b5b8b8c9c2 aws-default-sg : sg-03e11656feb78ca14 aws-cert-id : 856062a4-b146-4c58-893c-72205a6c47ee aws-cert-arn : arn:aws:acm:us-east-1:689242578769:certificate/856062a4-b146-4c58-893c-72205a6c47ee We never use this information programmatically, so use whatever format you want, but it does make it easier to keep track of what you're doing during install. This is particularly important if you are setting up multiple machines and don't want to waste time.","title":"Placeholder Conventions"},{"location":"setup/intro/#aws-network-setup","text":"If you are using AWS you can start with our instructions for setting up a virtual private cloud (VPC). It sets up a private subnet for non-client machines and a VPN and Network drive for access to that private subnet. AWS (Network) Setup","title":"AWS Network Setup"},{"location":"setup/intro/#machine-setup","text":"Installation for each machine requires following the other pages in this section in order, skipping any that aren't relevant and always including general setup. Try to setup machines with centralized functions, like the license server and message broker, before the worker nodes. AWS (Instance) Setup General Setup (Required) Creo License Server Message Broker Engineering Corpus Worker Node Client nodes are less directly dependent on the SimpleUAM start and their setup can skip directly to the corresponding section: Client Node","title":"Machine Setup"},{"location":"setup/license_server/","text":"Creo License Server Setup \u00b6 This sets up a floating Creo license that worker nodes can use. A machine or VM with direct access can use a node-locked Creo license instead. However if you need to access the VM through RDP a floating license is required. Prerequisites \u00b6 General Setup has been completed. The IP of this machine is saved as: <license-server-ip> Get License File \u00b6 Get the Host ID and License File. Open an admin powershell to <repo-root> . Get the Host ID / mac address for this machine: pdm run setup-win mac-address Save the result as <license-host-id> . Get the Creo floating license for <license-host-id> . Place the license file at <license-file> , somewhere on this machine. Install Dependencies \u00b6 Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install necessary dependencies: pdm run setup-win install.license-deps Install Flexnet Server \u00b6 Flexnet server is one of the options for hosting the license server and has been reasonably easy to use. Open an admin powershell to <repo-root> . Download and run the installer: pdm run setup-win license-server.flexnet Wait for installer to download and hit enter when prompted. Follow GUI installer's prompts until done, including providing <license-file> when asked for it. Open Required Ports \u00b6 Open the relevant ports up so instances of Creo on the worker nodes can connect. Note If you only intend to use Creo locally, on the same machine as the license server then you don't need to open any ports up. Option 1: Open only port 7788. \u00b6 The instructions for this are too configuration specific for us to provide. Option 2: Disable license server firewalls entirely. \u00b6 We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Warning This is not secure at all. Do not do this if the license server is at all accessible from public or untrusted computers. Disable the Windows Server 2019 firewall: pdm run setup-win license-server.disable-firewall Note: This might work with other versions of Windows but that hasn't been tested. Configure and Start Server \u00b6 Configure the server with a new admin password and start it. Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the administration page: Default User: admin Default Pass: admin Change the password: Old Pass: admin New Pass: <flexnet-password> Start the server: Administration -> Server Configuration -> Start Server The server should now automatically start on boot.","title":"License Server"},{"location":"setup/license_server/#creo-license-server-setup","text":"This sets up a floating Creo license that worker nodes can use. A machine or VM with direct access can use a node-locked Creo license instead. However if you need to access the VM through RDP a floating license is required.","title":"Creo License Server Setup"},{"location":"setup/license_server/#prerequisites","text":"General Setup has been completed. The IP of this machine is saved as: <license-server-ip>","title":"Prerequisites"},{"location":"setup/license_server/#get-license-file","text":"Get the Host ID and License File. Open an admin powershell to <repo-root> . Get the Host ID / mac address for this machine: pdm run setup-win mac-address Save the result as <license-host-id> . Get the Creo floating license for <license-host-id> . Place the license file at <license-file> , somewhere on this machine.","title":"Get License File"},{"location":"setup/license_server/#install-dependencies","text":"Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install necessary dependencies: pdm run setup-win install.license-deps","title":"Install Dependencies"},{"location":"setup/license_server/#install-flexnet-server","text":"Flexnet server is one of the options for hosting the license server and has been reasonably easy to use. Open an admin powershell to <repo-root> . Download and run the installer: pdm run setup-win license-server.flexnet Wait for installer to download and hit enter when prompted. Follow GUI installer's prompts until done, including providing <license-file> when asked for it.","title":"Install Flexnet Server"},{"location":"setup/license_server/#open-required-ports","text":"Open the relevant ports up so instances of Creo on the worker nodes can connect. Note If you only intend to use Creo locally, on the same machine as the license server then you don't need to open any ports up.","title":"Open Required Ports"},{"location":"setup/license_server/#option-1-open-only-port-7788","text":"The instructions for this are too configuration specific for us to provide.","title":"Option 1: Open only port 7788."},{"location":"setup/license_server/#option-2-disable-license-server-firewalls-entirely","text":"We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Warning This is not secure at all. Do not do this if the license server is at all accessible from public or untrusted computers. Disable the Windows Server 2019 firewall: pdm run setup-win license-server.disable-firewall Note: This might work with other versions of Windows but that hasn't been tested.","title":"Option 2: Disable license server firewalls entirely."},{"location":"setup/license_server/#configure-and-start-server","text":"Configure the server with a new admin password and start it. Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the administration page: Default User: admin Default Pass: admin Change the password: Old Pass: admin New Pass: <flexnet-password> Start the server: Administration -> Server Configuration -> Start Server The server should now automatically start on boot.","title":"Configure and Start Server"},{"location":"setup/worker/","text":"Worker Node Setup \u00b6 A worker can analyze designs on behalf of clients and requires access to a license server and a broker for most tasks. Prerequisites \u00b6 General Setup has been completed. SSH keys or credentials for git.isis.vanderbilt.edu A broker running at <broker-ip> and <broker-port> Access to a corpus: Either: Via a corpus DB at <corpus-db-ip> on port <corpus-db-port> Or: A static corpus installed as described here . Have a results directory set up at <results-dir> . Install Dependencies \u00b6 Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.worker-deps Get License Information \u00b6 Option 1: License Server Have a license server running at <license-server-ip> on port <license-server-port> . Option 2: Static Creo License Open an admin powershell to <repo-root> . Get your mac address (for generating licenses): pdm run setup-win mac-address If using a local license, have the license file for the above mac address downloaded somewhere convenient. Install Creo \u00b6 Downloads and installs PTC Creo 5.6. Open an admin powershell to <repo-root> . Download and run installer: pdm run setup-win worker.creo Read instructions in terminal and hit enter when ready. Follow installer prompts until done. Fix some minor usability issues. If on Windows Server 2019 you can disable the IE Enhanced Security popups that open whenever Creo starts. pdm run setup-win worker.disable-ieesc Install Creopyson \u00b6 Creopyson provides a python interface to Creo. Prepare to connect to git.isis.vanderbilt.edu . Either: Install SSH keys for git.isis.vanderbilt.edu . Or: have credentials ready for prompt. Open an admin powershell to <repo-root> . Download creopyson repository and install via pip: pdm run setup-win worker.creopyson Follow prompts. Configure Corpus Settings \u00b6 Performing various analysis tasks requires access to either a Corpus DB or a local static corpus. The config file at <config-dir>/craidl.conf.yaml specifies which of these use and how the connection is configured. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r Option 1: Using a static corpus. \u00b6 No changes should be needed to config files. Option 2: Using a remote corpus DB. \u00b6 Update <config-dir>/craidl.md : Set use_static_corpus to False . Set server_host to <corpus-db-host . Set server_port to <corpus-db-port> . See the config file guide for more detailed instructions and information. Further details on using a static or database corpus can be found in this section ... Configure Results Dir \u00b6 The results directory needs to be configured to point at the appropriate directory. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_workspace -r Update the config at <config-dir>/d2c_workspace.conf.yaml : Set results_dir to <results-dir> . See the config file guide for more detailed instructions and information. Further details on results storage and local worker node operations are in this page ... Initialize Reference Workspace \u00b6 Each worker needs a pristine copy of the workspace in which designs can be analyzed. This reference workspace will be copied, via rsync for efficiency, whenever the worker performs a new analysis. The results archives are also constructed by gathering all the files which have been changed after an analysis when compared to the reference directory. Open admin powershell at <repo-root> . Set up the reference workspace: pdm run d2c-workspace setup.reference-workspace For further details on workspace configuration, operation, and running analyses locally see this page ... Configure Broker Settings \u00b6 The worker process itself needs to be configured with how to connect to a message broker. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Update the config at <config-dir>/d2c_worker.conf.yaml : Set broker.protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set broker.host to <broker-ip> . Set broker.port to <broker-port> . If using Redis, set broker.db to <broker-db> See the config file guide for more detailed instructions and information. Further details on configuring the worker's backend are here ... Run the Worker Node as a Process (Optional) \u00b6 You can run the worker node as a process to verify all the above settings function as intended. Open admin powershell to <repo-root> . Run the SimpleUAM worker node: pdm run d2c-worker run Configure the Worker Node to Auto-Start on Boot (Optional) \u00b6 The worker node service needs to configured, in particular whether it should automatically start on boot. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Edit the config at <config-dir>/d2c_worker.conf.yaml : Set service.auto_start to True See the config file guide for more detailed instructions and information. Further details on configuring the worker as a service are here ... Set up the Worker Node Service \u00b6 We use Non-Sucking Service Manager to manage the worker node service, and that requires some setup. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run d2c-worker service.install Start the Worker Node Service (Optional) \u00b6 The worker node service can start immediately but we recommend holding off until you've verified configurations while running the node as a process. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run d2c-worker service.start Further details on running the worker as a service are here ... Next Steps \u00b6 View information on corpus use here ... View information on local task processing here ... View information on remote task processing here ...","title":"Worker Node"},{"location":"setup/worker/#worker-node-setup","text":"A worker can analyze designs on behalf of clients and requires access to a license server and a broker for most tasks.","title":"Worker Node Setup"},{"location":"setup/worker/#prerequisites","text":"General Setup has been completed. SSH keys or credentials for git.isis.vanderbilt.edu A broker running at <broker-ip> and <broker-port> Access to a corpus: Either: Via a corpus DB at <corpus-db-ip> on port <corpus-db-port> Or: A static corpus installed as described here . Have a results directory set up at <results-dir> .","title":"Prerequisites"},{"location":"setup/worker/#install-dependencies","text":"Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.worker-deps","title":"Install Dependencies"},{"location":"setup/worker/#get-license-information","text":"Option 1: License Server Have a license server running at <license-server-ip> on port <license-server-port> . Option 2: Static Creo License Open an admin powershell to <repo-root> . Get your mac address (for generating licenses): pdm run setup-win mac-address If using a local license, have the license file for the above mac address downloaded somewhere convenient.","title":"Get License Information"},{"location":"setup/worker/#install-creo","text":"Downloads and installs PTC Creo 5.6. Open an admin powershell to <repo-root> . Download and run installer: pdm run setup-win worker.creo Read instructions in terminal and hit enter when ready. Follow installer prompts until done. Fix some minor usability issues. If on Windows Server 2019 you can disable the IE Enhanced Security popups that open whenever Creo starts. pdm run setup-win worker.disable-ieesc","title":"Install Creo"},{"location":"setup/worker/#install-creopyson","text":"Creopyson provides a python interface to Creo. Prepare to connect to git.isis.vanderbilt.edu . Either: Install SSH keys for git.isis.vanderbilt.edu . Or: have credentials ready for prompt. Open an admin powershell to <repo-root> . Download creopyson repository and install via pip: pdm run setup-win worker.creopyson Follow prompts.","title":"Install Creopyson"},{"location":"setup/worker/#configure-corpus-settings","text":"Performing various analysis tasks requires access to either a Corpus DB or a local static corpus. The config file at <config-dir>/craidl.conf.yaml specifies which of these use and how the connection is configured. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r","title":"Configure Corpus Settings"},{"location":"setup/worker/#option-1-using-a-static-corpus","text":"No changes should be needed to config files.","title":"Option 1: Using a static corpus."},{"location":"setup/worker/#option-2-using-a-remote-corpus-db","text":"Update <config-dir>/craidl.md : Set use_static_corpus to False . Set server_host to <corpus-db-host . Set server_port to <corpus-db-port> . See the config file guide for more detailed instructions and information. Further details on using a static or database corpus can be found in this section ...","title":"Option 2: Using a remote corpus DB."},{"location":"setup/worker/#configure-results-dir","text":"The results directory needs to be configured to point at the appropriate directory. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_workspace -r Update the config at <config-dir>/d2c_workspace.conf.yaml : Set results_dir to <results-dir> . See the config file guide for more detailed instructions and information. Further details on results storage and local worker node operations are in this page ...","title":"Configure Results Dir"},{"location":"setup/worker/#initialize-reference-workspace","text":"Each worker needs a pristine copy of the workspace in which designs can be analyzed. This reference workspace will be copied, via rsync for efficiency, whenever the worker performs a new analysis. The results archives are also constructed by gathering all the files which have been changed after an analysis when compared to the reference directory. Open admin powershell at <repo-root> . Set up the reference workspace: pdm run d2c-workspace setup.reference-workspace For further details on workspace configuration, operation, and running analyses locally see this page ...","title":"Initialize Reference Workspace"},{"location":"setup/worker/#configure-broker-settings","text":"The worker process itself needs to be configured with how to connect to a message broker. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Update the config at <config-dir>/d2c_worker.conf.yaml : Set broker.protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set broker.host to <broker-ip> . Set broker.port to <broker-port> . If using Redis, set broker.db to <broker-db> See the config file guide for more detailed instructions and information. Further details on configuring the worker's backend are here ...","title":"Configure Broker Settings"},{"location":"setup/worker/#run-the-worker-node-as-a-process-optional","text":"You can run the worker node as a process to verify all the above settings function as intended. Open admin powershell to <repo-root> . Run the SimpleUAM worker node: pdm run d2c-worker run","title":"Run the Worker Node as a Process (Optional)"},{"location":"setup/worker/#configure-the-worker-node-to-auto-start-on-boot-optional","text":"The worker node service needs to configured, in particular whether it should automatically start on boot. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Edit the config at <config-dir>/d2c_worker.conf.yaml : Set service.auto_start to True See the config file guide for more detailed instructions and information. Further details on configuring the worker as a service are here ...","title":"Configure the Worker Node to Auto-Start on Boot (Optional)"},{"location":"setup/worker/#set-up-the-worker-node-service","text":"We use Non-Sucking Service Manager to manage the worker node service, and that requires some setup. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run d2c-worker service.install","title":"Set up the Worker Node Service"},{"location":"setup/worker/#start-the-worker-node-service-optional","text":"The worker node service can start immediately but we recommend holding off until you've verified configurations while running the node as a process. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run d2c-worker service.start Further details on running the worker as a service are here ...","title":"Start the Worker Node Service (Optional)"},{"location":"setup/worker/#next-steps","text":"View information on corpus use here ... View information on local task processing here ... View information on remote task processing here ...","title":"Next Steps"},{"location":"usage/clients/","text":"SimpleUAM Client Nodes \u00b6 Client sends out requests to message broker for analysis Watches shared file directory for results. If backend set up then maybe gets notified. Must be connected to same broker as the worker nodes. Two sessions: gen_info_files : Same as craidl and workspace versions, except ships out to worker for evaluation. Command Line: pdm run d2c-client gen-info-files -i design_swri.json Python: from simple_uam import direct2cad msg = direct2cad . gen_info_files . send ( design ) process design : same ad d2c workspace version except ships out to worker for eval. Command Line: pdm run d2c-client process-design -i design_swri.json Python: from simple_uam import direct2cad msg = direct2cad . process_design . send ( design ) Talk about config file d2c_worker.conf.yaml and fields: Broker config Backend config reqs Clients: Config broker and backend Run pipeline vie CLI Run pipeline via python Making a Client Request via Command Line \u00b6 A client needs to have a broker configured in its <config-dir>/d2c_worker.conf.yaml just like the worker node. By default it can only send requests to workers and it won't receive any response back. If both the worker and client have an enabled and configured backend then the client will receive metadata which includes the name of the archive file generated in response to the original request. The client doesn't need any of the other setup of a worker node past having this repo available and the proper configuration. All of this works on Linux and OS X, not just Windows. Send a gen_info_files request using the command line interface: PS D: \\s imple-uam> pdm run d2c-worker gen-info-files --help Usage: pdm run d2c-worker [ --core-opts ] gen-info-files [ --options ] [ other tasks here ... ] Docstring: Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Arguments: input: The design file to read in . metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. Options: -i STRING, --input = STRING -m STRING, --metadata = STRING -o STRING, --output = STRING Send a process_design request using the command line interface: PS D: \\s imple-uam> pdm run d2c-worker process-design --help Usage: pdm run d2c-worker [ --core-opts ] process-design [ --options ] [ other tasks here ... ] Docstring: Runs the direct2cad pipeline on the input design files, producing output metadata and a records archive with all the generated files. Arguments: input: The design file to read in . metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. Options: -i STRING, --input = STRING -m STRING, --metadata = STRING -o STRING, --output = STRING Making a Client Request in Python \u00b6 The configuration requirements are identical to the command-line case. Sending a request requires: SimpleUAM and dramatiq as dependencies. design as a JSON-serializable python object. Optional metadata a JSON-serializable python dictionary. A basic example: from simple_uam import direct2cad from simple_uam.util.config import Config , D2CWorkerConfig design = your_code_here () metadata = your_code_here () msg = direct2cad . process_design . send ( design , metadata = metadata ) # Only works if we have a configured and enabled backend to cache results. if Config [ D2CWorkerConfig ] . backend . enabled : result = msg . get_result ( block = True , timeout = 600000 ) print ( result ) The core task processing is handled by dramatiq , and their docs can help with any advanced use.","title":"Clients"},{"location":"usage/clients/#simpleuam-client-nodes","text":"Client sends out requests to message broker for analysis Watches shared file directory for results. If backend set up then maybe gets notified. Must be connected to same broker as the worker nodes. Two sessions: gen_info_files : Same as craidl and workspace versions, except ships out to worker for evaluation. Command Line: pdm run d2c-client gen-info-files -i design_swri.json Python: from simple_uam import direct2cad msg = direct2cad . gen_info_files . send ( design ) process design : same ad d2c workspace version except ships out to worker for eval. Command Line: pdm run d2c-client process-design -i design_swri.json Python: from simple_uam import direct2cad msg = direct2cad . process_design . send ( design ) Talk about config file d2c_worker.conf.yaml and fields: Broker config Backend config reqs Clients: Config broker and backend Run pipeline vie CLI Run pipeline via python","title":"SimpleUAM Client Nodes"},{"location":"usage/clients/#making-a-client-request-via-command-line","text":"A client needs to have a broker configured in its <config-dir>/d2c_worker.conf.yaml just like the worker node. By default it can only send requests to workers and it won't receive any response back. If both the worker and client have an enabled and configured backend then the client will receive metadata which includes the name of the archive file generated in response to the original request. The client doesn't need any of the other setup of a worker node past having this repo available and the proper configuration. All of this works on Linux and OS X, not just Windows. Send a gen_info_files request using the command line interface: PS D: \\s imple-uam> pdm run d2c-worker gen-info-files --help Usage: pdm run d2c-worker [ --core-opts ] gen-info-files [ --options ] [ other tasks here ... ] Docstring: Will write the design info files in the specified workspace, and create a new result archive with only the newly written data. The workspace will be reset on the next run. Arguments: input: The design file to read in . metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. Options: -i STRING, --input = STRING -m STRING, --metadata = STRING -o STRING, --output = STRING Send a process_design request using the command line interface: PS D: \\s imple-uam> pdm run d2c-worker process-design --help Usage: pdm run d2c-worker [ --core-opts ] process-design [ --options ] [ other tasks here ... ] Docstring: Runs the direct2cad pipeline on the input design files, producing output metadata and a records archive with all the generated files. Arguments: input: The design file to read in . metadata: The json-format metadata file to include with the query. Should be a dictionary. output: File to write output session metadata to, prints to stdout if not specified. Options: -i STRING, --input = STRING -m STRING, --metadata = STRING -o STRING, --output = STRING","title":"Making a Client Request via Command Line"},{"location":"usage/clients/#making-a-client-request-in-python","text":"The configuration requirements are identical to the command-line case. Sending a request requires: SimpleUAM and dramatiq as dependencies. design as a JSON-serializable python object. Optional metadata a JSON-serializable python dictionary. A basic example: from simple_uam import direct2cad from simple_uam.util.config import Config , D2CWorkerConfig design = your_code_here () metadata = your_code_here () msg = direct2cad . process_design . send ( design , metadata = metadata ) # Only works if we have a configured and enabled backend to cache results. if Config [ D2CWorkerConfig ] . backend . enabled : result = msg . get_result ( block = True , timeout = 600000 ) print ( result ) The core task processing is handled by dramatiq , and their docs can help with any advanced use.","title":"Making a Client Request in Python"},{"location":"usage/config/","text":"SimpleUAM Configuration System \u00b6 SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. In addition there is a utility, suam-config , provided with this repo to help manage configuration information. All commands are run in <repo-root> . Configuration Files \u00b6 All SimpleUAM configuration files are in YAML . We support OmegaConf's variable interpolation to allow referencing values in one config file from another. Configuration Semantics \u00b6 There is a single opinionated config directory where config files should be located. All config files overload keys from a default configuration, so you can leave keys (or the entire file) out if you don't want to specify them. Default configs are defined as attrs dataclasses in simple_uam.util.config submodules. Interpolation Keys for each config file are specified at init, and can be used to retrieve values. Using suam-config \u00b6 pdm run suam-config dir : Print config directory pdm run suam-config list-files : Lists valid config files in dir pdm run suam-config list-keys : Lists valid interpolation keys for configs. pdm run suam-config print : Will print current configuration to stdout Options for printing all files and resolving all interpolations. pdm run suam-config write : Can write configs out to file. Will default to placing fully commented out configs in the default configuration directory. User can uncomment and edit as needed. Existing Configuration Files \u00b6 paths.conf.yaml : Defaults paths for cache, logs, data, etc.. Used by other config defaults a lot. Most people shouldn't make any changes. Changes basically require reinstallation, since locations of files and applications will be different. win_setup.conf.yaml : Lists packages needed for windows installs of various components. Exists to make custom automation easier. Most people shouldn't make any changes. craidl.conf.yaml : Settings for working with a corpus DB or static corpus. stub_server section controls the local stub server, defaults should be fine if you're only using it to generate a static corpus. Other fields control whether a corpus DB or local static corpus is used for generating direct2cad input files. The defaults should be fine when using a static corpus. d2c_workspace.conf.yaml : Controls how the local processing of analysis jobs is done. Most important field is results_dir which controls where the analysis results are placed when completed. Most people will need to make this point to their shared directory. d2c_worker.conf.yaml : Controls connections to the message broker and how the worker nodes run. The broker and backend sections tell both the worker and client how to connect to the message broker. Most people will need to edit these. The service section determines how the worker node will run as a service. The auto_start subfield controls whether the service is configured resume on reboot.","title":"Configuration"},{"location":"usage/config/#simpleuam-configuration-system","text":"SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. In addition there is a utility, suam-config , provided with this repo to help manage configuration information. All commands are run in <repo-root> .","title":"SimpleUAM Configuration System"},{"location":"usage/config/#configuration-files","text":"All SimpleUAM configuration files are in YAML . We support OmegaConf's variable interpolation to allow referencing values in one config file from another.","title":"Configuration Files"},{"location":"usage/config/#configuration-semantics","text":"There is a single opinionated config directory where config files should be located. All config files overload keys from a default configuration, so you can leave keys (or the entire file) out if you don't want to specify them. Default configs are defined as attrs dataclasses in simple_uam.util.config submodules. Interpolation Keys for each config file are specified at init, and can be used to retrieve values.","title":"Configuration Semantics"},{"location":"usage/config/#using-suam-config","text":"pdm run suam-config dir : Print config directory pdm run suam-config list-files : Lists valid config files in dir pdm run suam-config list-keys : Lists valid interpolation keys for configs. pdm run suam-config print : Will print current configuration to stdout Options for printing all files and resolving all interpolations. pdm run suam-config write : Can write configs out to file. Will default to placing fully commented out configs in the default configuration directory. User can uncomment and edit as needed.","title":"Using suam-config"},{"location":"usage/config/#existing-configuration-files","text":"paths.conf.yaml : Defaults paths for cache, logs, data, etc.. Used by other config defaults a lot. Most people shouldn't make any changes. Changes basically require reinstallation, since locations of files and applications will be different. win_setup.conf.yaml : Lists packages needed for windows installs of various components. Exists to make custom automation easier. Most people shouldn't make any changes. craidl.conf.yaml : Settings for working with a corpus DB or static corpus. stub_server section controls the local stub server, defaults should be fine if you're only using it to generate a static corpus. Other fields control whether a corpus DB or local static corpus is used for generating direct2cad input files. The defaults should be fine when using a static corpus. d2c_workspace.conf.yaml : Controls how the local processing of analysis jobs is done. Most important field is results_dir which controls where the analysis results are placed when completed. Most people will need to make this point to their shared directory. d2c_worker.conf.yaml : Controls connections to the message broker and how the worker nodes run. The broker and backend sections tell both the worker and client how to connect to the message broker. Most people will need to edit these. The service section determines how the worker node will run as a service. The auto_start subfield controls whether the service is configured resume on reboot.","title":"Existing Configuration Files"},{"location":"usage/craidl/","text":"Craidl Tools \u00b6 Modified from code privded by sri. Exists to support a single step needed by direct2cad analysis pipeline gen_info_file has infra for downloading and installing a graphml corpus keeping track of a component information corpus keeping track of an example design corpus setup, sri download, and cleanup generating a static corpus from a running corpus db using either a corpus db to a static corpus to generate the info files for a design. command line args Also config file breakdown? reqs Craidl: Run and use CorpusDB stub server Use the generated static corpus Using / generating either type of corpus Tools can use graph server corpus or static file for 'gen_info_files' Specify which through config files ( craidl.conf.yaml ) Server: Needs .graphml corpus from swri. Can download UAM corpus from repo or install user provided Static Corpus: Can copy from this repo's data Can generate from running graph server Examples: Keeps a directory of example designs Can download examples from trinity-craidl repo Can let user add as needed. Install Engineering Corpus \u00b6 Option 1 : Download default from athens-uav-workflows In admin powershell at <repo-root> . Download athens-uav-workflows repo and get all_schema_uam.graphml Run: pdm run craidl corpus.download Follow prompts, enter git.isis.vanderbilt.edu credentials if needed. Install corpus to default location. Run: pdm run craidl corpus.install Default parameters will use downloaded corpus. Option 2 : Install from provided file In admin powershell at <repo-root> . Install user provided corpus from <corpus-loc> . Run: pdm run craidl corpus.install --corpus=<corpus-loc> Configure Corpus Server \u00b6 In admin powershell at <repo-root> . Craidl settings file: Examine loaded config with pdm run suam-config print --config=craidl -r Server will use stub_server.host and stub_server.port by default. Install and configure graph server Run: pdm run craidl stub-server.configure Will use configured host, port, and graphml corpus. Run Corpus Server \u00b6 Once configured open admin powershell at <repo-root> . Run the graph server as a process. Run: pdm run craidl stub-server.run Note that this isn't a service, the server will stop if you close the terminal. Install Static Corpus \u00b6 Generating the info files for a design is very slow when using a server so we provide an option to use a static JSON corpus. Option 1 : Use the provided static corpus from this repo. This is the corpus for all_schema_uam.graphml generated in June '22. Once configured open admin powershell at <repo-root> . Install the corpus provided with this repo. Run: pdm run craidl static-corpus.copy Option 2 : Use a user provided static corpus. Once configured open admin powershell at <repo-root> . Install a user provided corpus from <static-corpus-loc> Run: pdm run craidl static-corpus.copy --input=<static-corpus-loc> Option 3 : Generate from the graph server. This will generate a static corpus from a running graph server, using whatever corpus it was configured with. Make sure a graph server is running at the configured location. Examine loaded config with pdm run suam-config print --config=craidl -r Server should be running at server_host on port server_port Defaults would use a local stub server. Generate the static corpus from that server. Run: pdm run craidl static-corpus.generate With default settings the corpus generation and the stub server can hang intermittently, so the generation command periodically saves its progress. Just rerun the command (with the same settings) and it will resume generating the corpus from the last saved cluster of components. Generate Design Info Files \u00b6 The gen-info-files command will let you generate the files the direct2cad pipeline uses. PS D :\\ simple-uam > pdm run craidl gen-info-files - -help Usage : craidl . EXE [- -core-opts ] gen-info-files [- -options ] [other tasks here ...] Docstring : Generates the info files for a given design . Arguments : design : '.json' with design information . Default : 'design_swri.json' output : The output ** directory ** in which to place the files . Default : cwd copy_design : Do we copy the input design to the output directory ? Default : False static : The static '.json' corpus to use when generating the files . Mutually exclusive with host and port . Default : As configured host : The hostname of the corpus server to use when generating the files . Mutually exclusive with static . Default : As configured port : The port of the corpus server to use when generating the files . Mutually exclusice with static . Default : As configured Options : -c , - -copy-design -d STRING , - -design = STRING -h STRING , - -host = STRING -o STRING , - -output = STRING -p STRING , - -port = STRING -s STRING , - -static = STRING The default settings for static , host , and port are based on the values in craidl.conf.yaml .","title":"Craidl"},{"location":"usage/craidl/#craidl-tools","text":"Modified from code privded by sri. Exists to support a single step needed by direct2cad analysis pipeline gen_info_file has infra for downloading and installing a graphml corpus keeping track of a component information corpus keeping track of an example design corpus setup, sri download, and cleanup generating a static corpus from a running corpus db using either a corpus db to a static corpus to generate the info files for a design. command line args Also config file breakdown? reqs Craidl: Run and use CorpusDB stub server Use the generated static corpus Using / generating either type of corpus Tools can use graph server corpus or static file for 'gen_info_files' Specify which through config files ( craidl.conf.yaml ) Server: Needs .graphml corpus from swri. Can download UAM corpus from repo or install user provided Static Corpus: Can copy from this repo's data Can generate from running graph server Examples: Keeps a directory of example designs Can download examples from trinity-craidl repo Can let user add as needed.","title":"Craidl Tools"},{"location":"usage/craidl/#install-engineering-corpus","text":"Option 1 : Download default from athens-uav-workflows In admin powershell at <repo-root> . Download athens-uav-workflows repo and get all_schema_uam.graphml Run: pdm run craidl corpus.download Follow prompts, enter git.isis.vanderbilt.edu credentials if needed. Install corpus to default location. Run: pdm run craidl corpus.install Default parameters will use downloaded corpus. Option 2 : Install from provided file In admin powershell at <repo-root> . Install user provided corpus from <corpus-loc> . Run: pdm run craidl corpus.install --corpus=<corpus-loc>","title":"Install Engineering Corpus"},{"location":"usage/craidl/#configure-corpus-server","text":"In admin powershell at <repo-root> . Craidl settings file: Examine loaded config with pdm run suam-config print --config=craidl -r Server will use stub_server.host and stub_server.port by default. Install and configure graph server Run: pdm run craidl stub-server.configure Will use configured host, port, and graphml corpus.","title":"Configure Corpus Server"},{"location":"usage/craidl/#run-corpus-server","text":"Once configured open admin powershell at <repo-root> . Run the graph server as a process. Run: pdm run craidl stub-server.run Note that this isn't a service, the server will stop if you close the terminal.","title":"Run Corpus Server"},{"location":"usage/craidl/#install-static-corpus","text":"Generating the info files for a design is very slow when using a server so we provide an option to use a static JSON corpus. Option 1 : Use the provided static corpus from this repo. This is the corpus for all_schema_uam.graphml generated in June '22. Once configured open admin powershell at <repo-root> . Install the corpus provided with this repo. Run: pdm run craidl static-corpus.copy Option 2 : Use a user provided static corpus. Once configured open admin powershell at <repo-root> . Install a user provided corpus from <static-corpus-loc> Run: pdm run craidl static-corpus.copy --input=<static-corpus-loc> Option 3 : Generate from the graph server. This will generate a static corpus from a running graph server, using whatever corpus it was configured with. Make sure a graph server is running at the configured location. Examine loaded config with pdm run suam-config print --config=craidl -r Server should be running at server_host on port server_port Defaults would use a local stub server. Generate the static corpus from that server. Run: pdm run craidl static-corpus.generate With default settings the corpus generation and the stub server can hang intermittently, so the generation command periodically saves its progress. Just rerun the command (with the same settings) and it will resume generating the corpus from the last saved cluster of components.","title":"Install Static Corpus"},{"location":"usage/craidl/#generate-design-info-files","text":"The gen-info-files command will let you generate the files the direct2cad pipeline uses. PS D :\\ simple-uam > pdm run craidl gen-info-files - -help Usage : craidl . EXE [- -core-opts ] gen-info-files [- -options ] [other tasks here ...] Docstring : Generates the info files for a given design . Arguments : design : '.json' with design information . Default : 'design_swri.json' output : The output ** directory ** in which to place the files . Default : cwd copy_design : Do we copy the input design to the output directory ? Default : False static : The static '.json' corpus to use when generating the files . Mutually exclusive with host and port . Default : As configured host : The hostname of the corpus server to use when generating the files . Mutually exclusive with static . Default : As configured port : The port of the corpus server to use when generating the files . Mutually exclusice with static . Default : As configured Options : -c , - -copy-design -d STRING , - -design = STRING -h STRING , - -host = STRING -o STRING , - -output = STRING -p STRING , - -port = STRING -s STRING , - -static = STRING The default settings for static , host , and port are based on the values in craidl.conf.yaml .","title":"Generate Design Info Files"},{"location":"usage/workers/","text":"Direct2Cad Worker \u00b6 Worker runs as a service and listens for tasks from msg broker. When task arrives uses the D2C Workspaces code to run the analysis and produce a result archive. Talk about broker vs backend. Broker gets message from client to worker. Backend gets notification of completion from worker to client. (Really just tells you the name of the zip file from a particular request.) RabbitMQ: Broker only Redis: Both Broker and Backend Supports running worker node as normal process: Normal Process: pdm run d2c-worker run Supports running worker node as a service managed by NSSM. The commands do the obvious things. pdm run d2c-worker service.install pdm run d2c-worker service.uninstall pdm run d2c-worker service.configure : Converts config file settings to NSSM settings. pdm run d2c-worker service.start pdm run d2c-worker service.stop pdm run d2c-worker service.restart pdm run d2c-worker service.status Talk about config file d2c_worker.conf.yaml and fields: Broker config Backend config Worker Node Opts Service config reqs Workers: Configuring broker and backend Configure use as service Remote task processing A worker will listen for tasks from a message broker and then run those tasks. Configure Worker Settings \u00b6 Configure the message broker and other settings in <config-dir>/d2c_worker.conf.yaml . Examine loaded config with pdm run suam-config print --config=d2c_worker -r Run Worker Process \u00b6 A worker node, which listens to a broker and performs tasks as requests come in must have been set up as here . Make sure that direct2cad workspaces have been properly set up on this node as described here . That code is what the worker process uses to evaluate a task, so if it doesn't function locally it won't function when a remote client asks. Make sure that a message broker is running at the configured host and port. Actually run the worker process. In admin powershell at <repo-root> . Run: pdm run d2c-worker worker.run Note that this is a process not a service. If it shuts down it needs to be restarted.","title":"Workers"},{"location":"usage/workers/#direct2cad-worker","text":"Worker runs as a service and listens for tasks from msg broker. When task arrives uses the D2C Workspaces code to run the analysis and produce a result archive. Talk about broker vs backend. Broker gets message from client to worker. Backend gets notification of completion from worker to client. (Really just tells you the name of the zip file from a particular request.) RabbitMQ: Broker only Redis: Both Broker and Backend Supports running worker node as normal process: Normal Process: pdm run d2c-worker run Supports running worker node as a service managed by NSSM. The commands do the obvious things. pdm run d2c-worker service.install pdm run d2c-worker service.uninstall pdm run d2c-worker service.configure : Converts config file settings to NSSM settings. pdm run d2c-worker service.start pdm run d2c-worker service.stop pdm run d2c-worker service.restart pdm run d2c-worker service.status Talk about config file d2c_worker.conf.yaml and fields: Broker config Backend config Worker Node Opts Service config reqs Workers: Configuring broker and backend Configure use as service Remote task processing A worker will listen for tasks from a message broker and then run those tasks.","title":"Direct2Cad Worker"},{"location":"usage/workers/#configure-worker-settings","text":"Configure the message broker and other settings in <config-dir>/d2c_worker.conf.yaml . Examine loaded config with pdm run suam-config print --config=d2c_worker -r","title":"Configure Worker Settings"},{"location":"usage/workers/#run-worker-process","text":"A worker node, which listens to a broker and performs tasks as requests come in must have been set up as here . Make sure that direct2cad workspaces have been properly set up on this node as described here . That code is what the worker process uses to evaluate a task, so if it doesn't function locally it won't function when a remote client asks. Make sure that a message broker is running at the configured host and port. Actually run the worker process. In admin powershell at <repo-root> . Run: pdm run d2c-worker worker.run Note that this is a process not a service. If it shuts down it needs to be restarted.","title":"Run Worker Process"},{"location":"usage/workspaces/","text":"Direct2Cad Workspace \u00b6 Workspace is wrapper around d2c that provides a consistent environment for SWRi's scripts to run in. Consistent env is defined in terms of a reference workspace which is static and defines the initial condition for any operation. You can have a \"session\" within a workspace with this broad flow: Workspace is reset (restored to same file state as reference workspace) Arbitrary python code, console commands, etc.. are run Cad pipeline, writing info to file, etc... Session Metadata, Logs, Errors are written to files in the workspace directory. We compare this workspace to the reference and pack any files that changed (incl. the metadata and logs) into a zip file. The zip file contains the results of the session and is placed in a shared folder. We provide 3 sessions: tasks.start-creo : Will start Creo on the worker. Exists for debug purposes. Other sessions that need Creo running can start it themselves. tasks.gen-info-files : Will generate info files using the configs in d2c_workspace.conf.yaml and package them in a records archive. This uses the same core operation as craidl and requires that be set up. tasks.process-design : Runs the direct2cad pipeline on the provided design and generates a record archive with the result. This uses the same core operation as craidl and requires that be set up. The reference workspace: setup w/ pdm run d2c-workspace setup.reference-workspace . Mostly takes care of itself. General Workspace Management w/ pdm run d2c-workspace <subcommand> : manage.cache-dir : Prints cache dir manage.workspaces-dir : Print the root workspaces directory. By default contains the reference workspaces, lock files, and live workspaces. manage.results-dir : Print the directory where results will be stored given current config. manage.delete-locks : Deletes all extant locks for workspace management, used to clear stale ones. manage.prune-results : Deletes any results the are to old and over the configured limit. (default is no limit) Go over config file? A workspace is an environment (i.e. folder) set up so that SWRi's direct2cad pipeline can run within it. Each workspace has session where commands are run and any changes from a clean workspace (like output cad or data files) are saved to a zipfile and moved to a storage directory. You can run these sessions locally through code or console commands. There are 3 provided sessions: These tasks only function on a worker node set up as here . In addition before running any sessions, a reference workspace must be set up to serve as the basis for any 'live' workspaces. Setup Reference Workspace \u00b6 In admin powershell at <repo-root> . Run: pdm run d2c-workspace setup.reference-workspace Manage Configuration Files \u00b6 The configs in <config-dir>/d2c_workspace.conf.yaml control workspace locations, the settings for craidl's gen_info_files , and where records are stored. Examine loaded config with pdm run suam-config print --config=d2c_workspace -r See here for more information on config files. Important Make sure the records_dir is set how you want. That is where all the results archives will go. If you don't want it to be a sub-directory of workspaces_dir make sure records_dir is an absolute path. Manage Workspaces \u00b6 Get the cache directory for these workspaces. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.cache-dir Get the root directory of the reference workspace and the live workspaces. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.cache-dir Get the records directory for these workspaces. This is where the zip files with data from each session will go. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.records-dir Prune the records directory by deleting old records until only the configured maximum are present. It will not delete records that are newer than the configured min_staletime . In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.prune-records Delete any file system locks that might have gotten left behind. These usually prevent multiple processes from taking control of the same live workspace. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.delete-locks Run gen_info_files Session \u00b6 This requires craidl to be properly set up including having a static corpus or graph server set up as needed. PS D :\\ simple-uam > pdm run d2c-workspace tasks . gen-info-files - -help Usage : pdm run d2c-workspace [- -core-opts ] tasks . gen-info-files [- -options ] Docstring : Will write the design info files in the specified workspace , and create a new result archive with only the newly written data . The workspace will be reset on the next run . Arguments : input : The design file to read in . workspace : The workspace to run this operation in . output : File to write output session metadata to , prints to stdout if not specified . Options : -i STRING , - -input = STRING -o STRING , - -output = STRING -w STRING , - -workspace = STRING Run start_creo Session \u00b6 PS D :\\ simple-uam > pdm run d2c-workspace tasks . start-creo - -help Usage : pdm run d2c-workspace [- -core-opts ] tasks . start-creo [- -options ] Docstring : Start creo within the specified workspace , whichever ' s available if none . Arguments : workspace : The workspace to run this operation in . output : File to write output session metadata to , prints to stdout if not specified . Options : -o STRING , - -output = STRING -w STRING , - -workspace = STRING Run process_design Session \u00b6 This requires craidl to be properly set up including having a static corpus or graph server set up as needed. PS D :\\ simple-uam > pdm run d2c-workspace tasks . process -design - -help Usage : pdm run d2c-workspace [- -core-opts ] tasks . process -design [- -options ] Docstring : Runs the direct2cad pipeline on the input design files , producing output metadata and a records archive with all the generated files . Arguments : design : The design file to read in . workspace : The workspace to run this operation in . output : File to write output session metadata to , prints to stdout if not specified . Options : -i STRING , - -input = STRING -o STRING , - -output = STRING -w STRING , - -workspace = STRING","title":"Workspaces"},{"location":"usage/workspaces/#direct2cad-workspace","text":"Workspace is wrapper around d2c that provides a consistent environment for SWRi's scripts to run in. Consistent env is defined in terms of a reference workspace which is static and defines the initial condition for any operation. You can have a \"session\" within a workspace with this broad flow: Workspace is reset (restored to same file state as reference workspace) Arbitrary python code, console commands, etc.. are run Cad pipeline, writing info to file, etc... Session Metadata, Logs, Errors are written to files in the workspace directory. We compare this workspace to the reference and pack any files that changed (incl. the metadata and logs) into a zip file. The zip file contains the results of the session and is placed in a shared folder. We provide 3 sessions: tasks.start-creo : Will start Creo on the worker. Exists for debug purposes. Other sessions that need Creo running can start it themselves. tasks.gen-info-files : Will generate info files using the configs in d2c_workspace.conf.yaml and package them in a records archive. This uses the same core operation as craidl and requires that be set up. tasks.process-design : Runs the direct2cad pipeline on the provided design and generates a record archive with the result. This uses the same core operation as craidl and requires that be set up. The reference workspace: setup w/ pdm run d2c-workspace setup.reference-workspace . Mostly takes care of itself. General Workspace Management w/ pdm run d2c-workspace <subcommand> : manage.cache-dir : Prints cache dir manage.workspaces-dir : Print the root workspaces directory. By default contains the reference workspaces, lock files, and live workspaces. manage.results-dir : Print the directory where results will be stored given current config. manage.delete-locks : Deletes all extant locks for workspace management, used to clear stale ones. manage.prune-results : Deletes any results the are to old and over the configured limit. (default is no limit) Go over config file? A workspace is an environment (i.e. folder) set up so that SWRi's direct2cad pipeline can run within it. Each workspace has session where commands are run and any changes from a clean workspace (like output cad or data files) are saved to a zipfile and moved to a storage directory. You can run these sessions locally through code or console commands. There are 3 provided sessions: These tasks only function on a worker node set up as here . In addition before running any sessions, a reference workspace must be set up to serve as the basis for any 'live' workspaces.","title":"Direct2Cad Workspace"},{"location":"usage/workspaces/#setup-reference-workspace","text":"In admin powershell at <repo-root> . Run: pdm run d2c-workspace setup.reference-workspace","title":"Setup Reference Workspace"},{"location":"usage/workspaces/#manage-configuration-files","text":"The configs in <config-dir>/d2c_workspace.conf.yaml control workspace locations, the settings for craidl's gen_info_files , and where records are stored. Examine loaded config with pdm run suam-config print --config=d2c_workspace -r See here for more information on config files. Important Make sure the records_dir is set how you want. That is where all the results archives will go. If you don't want it to be a sub-directory of workspaces_dir make sure records_dir is an absolute path.","title":"Manage Configuration Files"},{"location":"usage/workspaces/#manage-workspaces","text":"Get the cache directory for these workspaces. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.cache-dir Get the root directory of the reference workspace and the live workspaces. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.cache-dir Get the records directory for these workspaces. This is where the zip files with data from each session will go. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.records-dir Prune the records directory by deleting old records until only the configured maximum are present. It will not delete records that are newer than the configured min_staletime . In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.prune-records Delete any file system locks that might have gotten left behind. These usually prevent multiple processes from taking control of the same live workspace. In admin powershell at <repo-root> . Run: pdm run d2c-workspace manage.delete-locks","title":"Manage Workspaces"},{"location":"usage/workspaces/#run-gen_info_files-session","text":"This requires craidl to be properly set up including having a static corpus or graph server set up as needed. PS D :\\ simple-uam > pdm run d2c-workspace tasks . gen-info-files - -help Usage : pdm run d2c-workspace [- -core-opts ] tasks . gen-info-files [- -options ] Docstring : Will write the design info files in the specified workspace , and create a new result archive with only the newly written data . The workspace will be reset on the next run . Arguments : input : The design file to read in . workspace : The workspace to run this operation in . output : File to write output session metadata to , prints to stdout if not specified . Options : -i STRING , - -input = STRING -o STRING , - -output = STRING -w STRING , - -workspace = STRING","title":"Run gen_info_files Session"},{"location":"usage/workspaces/#run-start_creo-session","text":"PS D :\\ simple-uam > pdm run d2c-workspace tasks . start-creo - -help Usage : pdm run d2c-workspace [- -core-opts ] tasks . start-creo [- -options ] Docstring : Start creo within the specified workspace , whichever ' s available if none . Arguments : workspace : The workspace to run this operation in . output : File to write output session metadata to , prints to stdout if not specified . Options : -o STRING , - -output = STRING -w STRING , - -workspace = STRING","title":"Run start_creo Session"},{"location":"usage/workspaces/#run-process_design-session","text":"This requires craidl to be properly set up including having a static corpus or graph server set up as needed. PS D :\\ simple-uam > pdm run d2c-workspace tasks . process -design - -help Usage : pdm run d2c-workspace [- -core-opts ] tasks . process -design [- -options ] Docstring : Runs the direct2cad pipeline on the input design files , producing output metadata and a records archive with all the generated files . Arguments : design : The design file to read in . workspace : The workspace to run this operation in . output : File to write output session metadata to , prints to stdout if not specified . Options : -i STRING , - -input = STRING -o STRING , - -output = STRING -w STRING , - -workspace = STRING","title":"Run process_design Session"}]}