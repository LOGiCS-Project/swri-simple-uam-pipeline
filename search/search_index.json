{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SimpleUAM : Tools for SWRi's UAV and UAM Workflow \u00b6 SimpleUAM is a set of python libraries and command-line tools for working with SWRi's pipeline for UAV development. Its main goals are to make the SWRi pipeline easy to deploy and wrap it with convenient interfaces for external tools. See Github Pages for more details... And the Github Repo for the source code, issues, etc... Organization \u00b6 SimpleUAM Component Structure SimpleUAM organizes itself into components, each of which perform some basic task needed to evaluate UAV or UAM designs: Client Nodes : Makes requests for UAV/UAM design analysis. Message Broker : Distributes analysis requests from clients to available workers. Worker Nodes : Analyzes designs with SWRi's pipelines, placing the results into storage. Engineering Corpus : Provides component and design data to the worker nodes during analysis. License Servers : Provides floating Creo licenses to workers. Results Storage : Store the results of design analyses on the file system, whether it's in a local folder or a network drive. These components aren't tied to specific machines and can all happily coexist on a single computer or support multiple servers cooperating on each task. This project provides support for versions of all of these nodes including setup scripts and python libraries for interacting with them. Links \u00b6 Install Instructions Configuration Instructions Repo Organization \u00b6 <repo-root> \u251c\u2500\u2500 LICENSE # License File \u251c\u2500\u2500 README.md # Readme File \u251c\u2500\u2500 pyproject.toml # Project Setup Info. For use with pdm, poetry, or similar. \u251c\u2500\u2500 mkdocs.yml # Documentation Configuration via mkdocs \u2502 \u251c\u2500\u2500 config/ # Config files for type checking, linting, etc... \u251c\u2500\u2500 docs/ # Source for gh-pages documentation. \u2514\u2500\u2500 src/ # Python source root","title":"Overview"},{"location":"#simpleuam-tools-for-swris-uav-and-uam-workflow","text":"SimpleUAM is a set of python libraries and command-line tools for working with SWRi's pipeline for UAV development. Its main goals are to make the SWRi pipeline easy to deploy and wrap it with convenient interfaces for external tools. See Github Pages for more details... And the Github Repo for the source code, issues, etc...","title":"SimpleUAM : Tools for SWRi's UAV and UAM Workflow"},{"location":"#organization","text":"SimpleUAM Component Structure SimpleUAM organizes itself into components, each of which perform some basic task needed to evaluate UAV or UAM designs: Client Nodes : Makes requests for UAV/UAM design analysis. Message Broker : Distributes analysis requests from clients to available workers. Worker Nodes : Analyzes designs with SWRi's pipelines, placing the results into storage. Engineering Corpus : Provides component and design data to the worker nodes during analysis. License Servers : Provides floating Creo licenses to workers. Results Storage : Store the results of design analyses on the file system, whether it's in a local folder or a network drive. These components aren't tied to specific machines and can all happily coexist on a single computer or support multiple servers cooperating on each task. This project provides support for versions of all of these nodes including setup scripts and python libraries for interacting with them.","title":"Organization"},{"location":"#links","text":"Install Instructions Configuration Instructions","title":"Links"},{"location":"#repo-organization","text":"<repo-root> \u251c\u2500\u2500 LICENSE # License File \u251c\u2500\u2500 README.md # Readme File \u251c\u2500\u2500 pyproject.toml # Project Setup Info. For use with pdm, poetry, or similar. \u251c\u2500\u2500 mkdocs.yml # Documentation Configuration via mkdocs \u2502 \u251c\u2500\u2500 config/ # Config files for type checking, linting, etc... \u251c\u2500\u2500 docs/ # Source for gh-pages documentation. \u2514\u2500\u2500 src/ # Python source root","title":"Repo Organization"},{"location":"contributing/","text":"TODO","title":"Contributing"},{"location":"license/","text":"MIT License Copyright (c) 2022 LOGiCS-Project Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"assets/deployment-components/","text":"Client Nodes : These make requests to analyze designs and retrieve analysis results once they finish. Client nodes will usually be running some optimization or search process. Message Brokers : These gather analysis requests from client nodes and distribute them to free worker nodes. Worker Nodes : These will perform analyses on designs and store the results somewhere accessible to the clients. License Management : Each worker node needs Creo to perform analysis, and a license for Creo must be made available somehow. License Server : These can provide licenses to a number of worker nodes at once if provided with a floating license. Node-Locked Creo License : This is a worker-specific, static license file that can be placed on a worker. Component Corpus : A corpus of component information that every worker needs in order to analyze designs. Corpus DB : A graph database that the worker can use to look up component information. Static Corpus : A static file containing a dump of the component corpus which is placed on each worker node. Results Storage : A file system, accessible to both worker nodes and clients, where analysis results (in individual zip files) are placed. Results Backends : These notify client nodes when their analysis requests are completed and where the output is in Results Storage.","title":"Deployment components"},{"location":"assets/deployment-rules/","text":"There must be one, and only one, message broker. The broker must be accessible over the network to all worker and client nodes. There needs to be at least one configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to a component corpus, either through an initialized corpus DB or a static corpus file. There must be a results storage accessible to all the worker and client nodes. The results storage should be a directory where workers can place files which are then made accessible to all the clients. Possible Storage Mechanisms Generally the results store will be a mounted network file share or local folder, but could also be: A network file system mounted on workers and clients. (Default) A local folder, if both worker and client are on the same machine. rsync + cron Dropbox Google Drive S3 Drive Anything that looks like a filesystem directory to both the worker and the client. In order for clients to receive analysis completion notifications there must be a single, unique results backend. This backend must be accessible over the network to all worker nodes and any client that wants to receive notifications. A results backend is optional and simply polling the results storage is perfectly viable.","title":"Deployment rules"},{"location":"reference/SUMMARY/","text":"","title":"Code Reference"},{"location":"setup/aws-instance/","text":"Amazon Web Services (AWS) Instance Setup \u00b6 This section of the guide will cover: Creating the various EC2 instances needed for each component or combination of component. Provisioning an Instance \u00b6 Setup an EC2 instance for running various components. This only covers the various windows nodes that SimpleUAM currently supports and needs to be repeated for each node. Under the EC2 Console click \"Launch an Instance\". Creo License Server Minimum Requirements The license server is simple enough we only need a free tier windows instance. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t2.micro 1x vCPU 1gb Memory Key Pair: <ec2-keypair.name> Network Settings: VPC: <aws-vpc.id> Subnet: <aws-private-subnet.id> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-default-sg.id> Advanced Network Configuration: None Configure Storage: 1x 30gb gp2 Worker Node Minimum Requirements The worker node needs an elastic graphics interface for Creo's UI to work. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t2.xlarge 4x vCPU 16gb Memory Key Pair: <ec2-keypair.name> Network Settings: VPC: <aws-vpc.id> Subnet: <aws-private-subnet.id> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-default-sg.id> Advanced Network Configuration: None Configure Storage: 1x 200gb gp2 Advanced Details: Elastic GPU: eg1.medium Corpus DB Minimum Requirements Option 1: If you need a read-only corpus DB itermittently for generating a static corpus, we reccomend just running the stub DB on a worker temporarily. Option 2: If you need a corpus DB for an alternate analysis pipeline and are fine with a non-persistent corpus then use a stub DB on a worker. This eats about 4gb of memory so ensure that the worker has at least 12 gb if the stub server will be up permanently. Option 3: If you need a corpus that is persistent, shared between multiple workers, or needs to be performant then you should set up janus-graph on a linux machine. This is outside the scope of this guide and you should try to follow SWRI's instructions. Option 4: AWS's Neptune DB seems like a viable option, though the details are up to you to sort. Message Broker Minimum Requirements Option 1: If you want a non-backendless broker or one that's not wasting your money, set up a linux instance and skip to the section on message broker setup . I'm not sure what minimum requirements make sense. Option 2: If you want a backendless message broker for intermittent testing or development work then install it sidecar with a worker node. Option 3: If you want a backendless broker and are fine with lower performance the specifications for a worker node on a windows machine be fine. Option 4: An AmazonMQ node might work well, there are tentative instructions in the section on message broker setup . Option 5: An Amazon MemoryDB for Redis node might work well, there are tentative instructions in the section on message broker setup . Find the instance you just created with \"Name\" <instance.name> . Save the \"Instance ID\" as: <instance.id> Connect to an Instance \u00b6 Use the remote desktop protocol to connect to the instance. Get RDP Connection Information \u00b6 Select the instance on your EC2 Dashboard Hit the Connect Button Connect to Instance -> RDP Client Keep note of: Private IP: <instance.ip> Username: <instance.user> RDP File: Click \"Download remote desktop file\" Save to <instance.rdp.file> Password: Click \"Get Password\" Upload <ec2-keypair.pem> Click \"Decrypt Password\" Save to <instance.rdp.pass> Connect Via RDP \u00b6 Make sure your local machine is connected to the VPN set up previously, otherwise none of the provided IPs will correctly resolve. Via Preferred RDP client (e.g. Remmina ): Import: <instance.rdp.file> IP: <instance.ip> Username: <instance.user> Password: <instance.rdp.pass> Mount Shared FSx Drive \u00b6 The AWS specific instructions are largely the same as the standard windows NFS ones, but they need certain server features to be enabled first. Add NFS client: Instructions Setup The Automount of drive: Instructions Open File Explorer and click \"Network\" and \"Map Network Drive\": Drive : <aws-fsx.drive-letter> (e.g. \" D: \" or \" F: \") Folder : \\\\<aws-fsx.ip>\\fsx\\ Reconnect At Login : Yes Continue to General Setup ...","title":"AWS (Instance)"},{"location":"setup/aws-instance/#amazon-web-services-aws-instance-setup","text":"This section of the guide will cover: Creating the various EC2 instances needed for each component or combination of component.","title":"Amazon Web Services (AWS) Instance Setup"},{"location":"setup/aws-instance/#provisioning-an-instance","text":"Setup an EC2 instance for running various components. This only covers the various windows nodes that SimpleUAM currently supports and needs to be repeated for each node. Under the EC2 Console click \"Launch an Instance\". Creo License Server Minimum Requirements The license server is simple enough we only need a free tier windows instance. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t2.micro 1x vCPU 1gb Memory Key Pair: <ec2-keypair.name> Network Settings: VPC: <aws-vpc.id> Subnet: <aws-private-subnet.id> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-default-sg.id> Advanced Network Configuration: None Configure Storage: 1x 30gb gp2 Worker Node Minimum Requirements The worker node needs an elastic graphics interface for Creo's UI to work. Name: <instance.name> Application and OS Images: Quick Start -> Windows Microsoft Windows Server 2019 Base Instance Type: t2.xlarge 4x vCPU 16gb Memory Key Pair: <ec2-keypair.name> Network Settings: VPC: <aws-vpc.id> Subnet: <aws-private-subnet.id> Auto-assign Public IP: Disable Firewall: Select existing security group Common Security Groups: <aws-default-sg.id> Advanced Network Configuration: None Configure Storage: 1x 200gb gp2 Advanced Details: Elastic GPU: eg1.medium Corpus DB Minimum Requirements Option 1: If you need a read-only corpus DB itermittently for generating a static corpus, we reccomend just running the stub DB on a worker temporarily. Option 2: If you need a corpus DB for an alternate analysis pipeline and are fine with a non-persistent corpus then use a stub DB on a worker. This eats about 4gb of memory so ensure that the worker has at least 12 gb if the stub server will be up permanently. Option 3: If you need a corpus that is persistent, shared between multiple workers, or needs to be performant then you should set up janus-graph on a linux machine. This is outside the scope of this guide and you should try to follow SWRI's instructions. Option 4: AWS's Neptune DB seems like a viable option, though the details are up to you to sort. Message Broker Minimum Requirements Option 1: If you want a non-backendless broker or one that's not wasting your money, set up a linux instance and skip to the section on message broker setup . I'm not sure what minimum requirements make sense. Option 2: If you want a backendless message broker for intermittent testing or development work then install it sidecar with a worker node. Option 3: If you want a backendless broker and are fine with lower performance the specifications for a worker node on a windows machine be fine. Option 4: An AmazonMQ node might work well, there are tentative instructions in the section on message broker setup . Option 5: An Amazon MemoryDB for Redis node might work well, there are tentative instructions in the section on message broker setup . Find the instance you just created with \"Name\" <instance.name> . Save the \"Instance ID\" as: <instance.id>","title":"Provisioning an Instance"},{"location":"setup/aws-instance/#connect-to-an-instance","text":"Use the remote desktop protocol to connect to the instance.","title":"Connect to an Instance"},{"location":"setup/aws-instance/#get-rdp-connection-information","text":"Select the instance on your EC2 Dashboard Hit the Connect Button Connect to Instance -> RDP Client Keep note of: Private IP: <instance.ip> Username: <instance.user> RDP File: Click \"Download remote desktop file\" Save to <instance.rdp.file> Password: Click \"Get Password\" Upload <ec2-keypair.pem> Click \"Decrypt Password\" Save to <instance.rdp.pass>","title":"Get RDP Connection Information"},{"location":"setup/aws-instance/#connect-via-rdp","text":"Make sure your local machine is connected to the VPN set up previously, otherwise none of the provided IPs will correctly resolve. Via Preferred RDP client (e.g. Remmina ): Import: <instance.rdp.file> IP: <instance.ip> Username: <instance.user> Password: <instance.rdp.pass>","title":"Connect Via RDP"},{"location":"setup/aws-instance/#mount-shared-fsx-drive","text":"The AWS specific instructions are largely the same as the standard windows NFS ones, but they need certain server features to be enabled first. Add NFS client: Instructions Setup The Automount of drive: Instructions Open File Explorer and click \"Network\" and \"Map Network Drive\": Drive : <aws-fsx.drive-letter> (e.g. \" D: \" or \" F: \") Folder : \\\\<aws-fsx.ip>\\fsx\\ Reconnect At Login : Yes Continue to General Setup ...","title":"Mount Shared FSx Drive"},{"location":"setup/aws-network/","text":"Amazon Web Services (AWS) Network Setup \u00b6 This section of the guide will cover: Setting up a virtual private cloud (VPC) to contain SUAM nodes. Creating a VPN connection into that cloud for home access. Creating a shared network drive for hosting code and analysis results. Initial Steps \u00b6 Most operations will require access to the AWS Console so ensure you have a valid amazon AWS account. Info This guide assumes that you have full access to AWS. In principle nothing should prevent an IAM user from performing all these steps except knowing the set of neccesary permissions. If you do use an IAM user for this install process, please tell us what policies you used so we can include them in this guide. Choose an Availability Zone \u00b6 AWS features clusters of cloud computing resources called availability zones (AZs). Different features of AWS are available on each AZ so you should choose one with access to everything you intend to use. Info This guide assumes that all actions are taking place within a single AZ, and that no bridges between zones are needed. Choose an AZ that supports all the features you intend to use. AZ Availability Lists for AWS Services Here are links to the AZ lists for a number of services you might want to use: FSx for OpenZFS : Used to provide a shared drive for workers and clients. Brokers: AmazonMQ : RabbitMQ compatible broker service. (Optional) MemoryDB for Redis : Redis compatible broker service. (Optional) ElastiCache for Redis : Redis compatible broker service. (Optional) Graph Databases: Neptune : Amazon managed graph database. (Optional) Switch AZs using the region selector as shown here . If you cannot find a single AZ with all the neccesary features then use multiple VPCs and VPC Peering to allow services on each VPC to access each other. Virtual Private Cloud \u00b6 This creates a private network where your servers can live isolated from the internet yet able to communicate with each other. Create public and private subnets \u00b6 Open the VPC console's Create VPC page . VPC Settings: Resources to Create : VPC and more Name tag auto-generation: Auto-generate : Yes (check box) Name : <aws-vpc.prefix> (pick your own and save it) IPv4 CIDR block : 10.0.0.0/16 IPv6 CIDR block : No IPv6 CIDR block Number of Availability Zones : 1 Number of Public Subnets : 1 Number of Private Subnets : 1 Customize subnets CIDR blocks: Public subnet CIDR block : 10.0.0.0/20 Private subnet CIDR block : 10.0.128.0/20 NAT gateways : in 1 AZ VPC endpoints : None (unless you want S3 access) DNS options: Enable DNS Hostnames : Yes (check box) Enable DNS Resolution : Yes (check box) Save useful information \u00b6 Click \"Create VPC\". Once the process bar is done click \"View VPC\". Keep track of: \"VPC Name\" as: <aws-vpc.name> \"VPC ID\" as: <aws-vpc.id> Open the Elastic IPs page on the VPC console: Find <aws-vpc> 's external IP address. It should be named \" <aws-vpc.prefix> -eip-...\" . Keep track of: \"Name\" as: <aws-elastic-ip.name> \"Allocated IPv4 address\" as: <aws-elastic-ip.addr> \"Allocation ID\" as: <aws-elastic-ip.id> Open the Subnets page on the VPC console. Find <aws-vpc> 's public subnet. It should be named \" <aws-vpc.prefix> -subnet-public1-...\" . Keep track of: \"Name\" as: <aws-public-subnet.name> \"Subnet ID\" as: <aws-public-subnet.id> Find <aws-vpc> 's private subnet. It should be named \" <aws-vpc.prefix> -subnet-private1-...\" . Keep track of: \"Name\" as: <aws-private-subnet.name> \"Subnet ID\" as: <aws-private-subnet.id> Open up the internal firewal \u00b6 Open the Security Groups page of the VPC console. Find the security group whose \"VPC ID\" is <aws-vpc.id> . Keep track of: \"Name\" as: <aws-default-sg.name> (Assign if needed) \"Security group ID\" as: <aws-default-sg.id> Select the SG, go to \"Inbound Rules\" on the bottom pane, and click \"Edit Inbound Rules\". Click \"Add Rule\": Type : Elastic Graphics Source : <aws-default-sg.id> Click \"Add Rule\": Type : All TCP Source : 10.0.0.0/8 Click \"Add Rule\": Type : All UDP Source : 10.0.0.0/8 Click \"Save rules\" AWS VPN Connection \u00b6 Sets up keys and a VPN connection so your local computer can directly communicate with instances and services on the private subnet. Create keys for VPN access \u00b6 Create sever and client certs: Follow the instructions here . Keep track of: ca.crt as: <aws-ca-certs.crt> ca.key as: <aws-ca-certs.key> server.crt as: <aws-server-cert.crt> server.key as: <aws-server-cert.key> client1.domain.tld.crt as: <aws-client-cert.crt> client1.domain.tld.key as: <aws-client-cert.key> Import the server and client certs using the ACM console : Follow the instructions here . Certificate Body : Content of <aws-*-cert.crt> between -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- , inclusive. Certificate Private Key : Content of <aws-*-cert.key> Certificate Chain : Content of <aws-ca-cert.crt> Open the AWS Certificate Manager's List Certificates page : For the server cert, with domain name \"server\": Keep track of \"Identifier\" as: <aws-server-cert.id> Keep track of \"ARN\" as: <aws-server-cert.arn> For the client cert, with domain name \"client1.domain.tld\": Keep track of \"Identifier\" as: <aws-client-cert.id> Keep track of \"ARN\" as: <aws-client-cert.arn> Create the VPN interface \u00b6 Follow the instructions here . Step 2: Name : <aws-cvpn> Client IPV4 CIDR : 10.10.0.0/22 Server Cert ARN : <aws-server-cert.arn> Authentication Option : Mutual Client Cert ARN : <aws-client-cert.arn> Connection Logging : No Client Connect Handler : no Optional Params: Transport : UDP Enable Split Tunnel : Yes VPC ID : <aws-vpc.id> Security Group IDs : <aws-default-sg.id> VPN Port : 443 Enable Self-Service : yes Session Timeout : 24h Enable Client Logic Banner : No Save the Client VPN ID as <aws-cvpn.id> Step 3: Client VPN Assoc VPC : <aws-vpc.id> Subnet : <aws-private-subnet.id> Step 4: Add Auth Rule: Dest Network : 10.0.0.0/8 Grant Access : all users Step 5: Add Route: Route Dest : 0.0.0.0/0 Target Subnet : <aws-private-subnet.id> Add Auth Rule: Dest Network : 0.0.0.0/0 Grant Access : all users Step 6: No changes Step 7: Skip this step Step 8: Skip this step Create Client VPN Config File \u00b6 This is the file users will import in order to connect to the above VPN interface. Instructions taken from here and here . Open the Amazon VPC console at https://console.aws.amazon.com/vpc/ In the navigation pane, choose \"Client VPN Endpoints\". Choose <aws-cvpn.id> and click \"Download Client Configuration\". Save this as <aws-cvpn.config> . Open <aws-cvpn.config> in a text editor. Prepend a random subdomain to the ClientVPN DNS entry The line should start with remote cvpn-endpoint- . When finished it should begin remote ****.cvpn-endpoint- with the rest remaining the same and **** replaced with some random string of your own. After the line that begins verb 3 insert the following: <ca> </ca> <cert> </cert> <key> </key> Place the content of <aws-ca-cert.crt> between the <ca> tags. It's fine if this is already populated in the downloaded <aws-cvpn.config> . Place the certificate from <aws-client-cert.crt> between the <cert> tags. The certificate is what's between -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- , inclusive. Place the content of <aws-client-cert.key> between the <key> tags. Save the modified file. Sample <aws-cvpn.config> after above modifications. The actual contents of the certificates have been replaced with ... . client dev tun proto udp remote asdf.cvpn-endpoint-0011abcabcabcabc1.prod.clientvpn.eu-west-2.amazonaws.com 443 remote-random-hostname resolv-retry infinite nobind remote-cert-tls server cipher AES-256-GCM verb 3 <ca> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- </key> reneg-sec 0 Distribute the <aws-cvpn.config> to your intended users. Connect to the VPN \u00b6 These instructions apply to your users as well as long as you provide them access to the <aws-cvpn.config> file. Linux: Instructions MacOS: Instructions Windows: Instructions Shared Drive \u00b6 Create a shared drive that you can mount on both your local machine and worker nodes. This drive is both a convenient shared mount for development work, using your local setup to edit server code, and a place for multiple workers to stash results. Option 1: Use Amazon FSx for OpenZFS \u00b6 Go to the FSx File System console . Click Create File System : Select file system type : OpenZFS Creation Method : Standard create File System Details: File system name : <aws-fsx.name> SSD Storage capacity : more than 50gb Provisioned SSD IOPS : Automatic Throughput Capacity : Recommended Network and Security: VPC : <aws-vpc.name> VPC Security Groups : <vpc-default-sg.name> Subnet : <aws-private-subnet.name> Encryption: Encryption Key : aws/fsx (default) Root Volume Configuration: Data compression type : No Compression NFS Exports: Client Address: * NFS Options: rw,no_auth_nlm,all_squash,anonuid=0,anongid=0,crossmnt Record Size : Default Click \"Next\" then \"Create File System\" Open the file systems page on the FSx console and select <aws-fsx.name> : Keep track of \"File System ID\" as: <aws-fsx.id> Open \"Network Interface\" and save\"IPv4 address\" as: <aws-fsx.ip> Unix Mount Instructions Ensure you're connected to the VPN. Choose and create a mount directory at <local-mount> . Make sure you have nfs utilities installed. For ubuntu: sudo apt-get -y install nfs-common . Run: sudo mount -t nfs -o nfsvers=4.1 <aws-fsx-ip>:/fsx/ <local-mount> Consider using nfs caching and async. (google for details) Windows Mount Instructions Ensure you're connected to the VPN. Chose a <drive-letter> to be the mount point. (e.g. 'Z:') Open cmd.exe as admin (Not powershell) Run: mount \\\\<aws-fsx-ip>\\fsx\\ <drive-letter> Create an EC2 Keypair \u00b6 This keypair is needed to connect to various EC2 instances in the VPC. Create a keypair for connecting to AWS instances. \u00b6 Open the EC2 console to the Key pairs page and click \" Create key pair \". Name : <ec2-keypair.name> Key Pair Type : RSA Private Key Format : .pem Click \"Create Key Pair\" When prompted save the \" <ec2-keypair.name> .pem\" file as: <ec2-keypair.pem> Continue to AWS Instance Setup ...","title":"AWS (Network)"},{"location":"setup/aws-network/#amazon-web-services-aws-network-setup","text":"This section of the guide will cover: Setting up a virtual private cloud (VPC) to contain SUAM nodes. Creating a VPN connection into that cloud for home access. Creating a shared network drive for hosting code and analysis results.","title":"Amazon Web Services (AWS) Network Setup"},{"location":"setup/aws-network/#initial","text":"Most operations will require access to the AWS Console so ensure you have a valid amazon AWS account. Info This guide assumes that you have full access to AWS. In principle nothing should prevent an IAM user from performing all these steps except knowing the set of neccesary permissions. If you do use an IAM user for this install process, please tell us what policies you used so we can include them in this guide.","title":"Initial Steps"},{"location":"setup/aws-network/#az","text":"AWS features clusters of cloud computing resources called availability zones (AZs). Different features of AWS are available on each AZ so you should choose one with access to everything you intend to use. Info This guide assumes that all actions are taking place within a single AZ, and that no bridges between zones are needed. Choose an AZ that supports all the features you intend to use. AZ Availability Lists for AWS Services Here are links to the AZ lists for a number of services you might want to use: FSx for OpenZFS : Used to provide a shared drive for workers and clients. Brokers: AmazonMQ : RabbitMQ compatible broker service. (Optional) MemoryDB for Redis : Redis compatible broker service. (Optional) ElastiCache for Redis : Redis compatible broker service. (Optional) Graph Databases: Neptune : Amazon managed graph database. (Optional) Switch AZs using the region selector as shown here . If you cannot find a single AZ with all the neccesary features then use multiple VPCs and VPC Peering to allow services on each VPC to access each other.","title":"Choose an Availability Zone"},{"location":"setup/aws-network/#virtual-private-cloud","text":"This creates a private network where your servers can live isolated from the internet yet able to communicate with each other.","title":"Virtual Private Cloud"},{"location":"setup/aws-network/#create-public-and-private-subnets","text":"Open the VPC console's Create VPC page . VPC Settings: Resources to Create : VPC and more Name tag auto-generation: Auto-generate : Yes (check box) Name : <aws-vpc.prefix> (pick your own and save it) IPv4 CIDR block : 10.0.0.0/16 IPv6 CIDR block : No IPv6 CIDR block Number of Availability Zones : 1 Number of Public Subnets : 1 Number of Private Subnets : 1 Customize subnets CIDR blocks: Public subnet CIDR block : 10.0.0.0/20 Private subnet CIDR block : 10.0.128.0/20 NAT gateways : in 1 AZ VPC endpoints : None (unless you want S3 access) DNS options: Enable DNS Hostnames : Yes (check box) Enable DNS Resolution : Yes (check box)","title":"Create public and private subnets"},{"location":"setup/aws-network/#save-useful-information","text":"Click \"Create VPC\". Once the process bar is done click \"View VPC\". Keep track of: \"VPC Name\" as: <aws-vpc.name> \"VPC ID\" as: <aws-vpc.id> Open the Elastic IPs page on the VPC console: Find <aws-vpc> 's external IP address. It should be named \" <aws-vpc.prefix> -eip-...\" . Keep track of: \"Name\" as: <aws-elastic-ip.name> \"Allocated IPv4 address\" as: <aws-elastic-ip.addr> \"Allocation ID\" as: <aws-elastic-ip.id> Open the Subnets page on the VPC console. Find <aws-vpc> 's public subnet. It should be named \" <aws-vpc.prefix> -subnet-public1-...\" . Keep track of: \"Name\" as: <aws-public-subnet.name> \"Subnet ID\" as: <aws-public-subnet.id> Find <aws-vpc> 's private subnet. It should be named \" <aws-vpc.prefix> -subnet-private1-...\" . Keep track of: \"Name\" as: <aws-private-subnet.name> \"Subnet ID\" as: <aws-private-subnet.id>","title":"Save useful information"},{"location":"setup/aws-network/#open-up-the-internal-firewal","text":"Open the Security Groups page of the VPC console. Find the security group whose \"VPC ID\" is <aws-vpc.id> . Keep track of: \"Name\" as: <aws-default-sg.name> (Assign if needed) \"Security group ID\" as: <aws-default-sg.id> Select the SG, go to \"Inbound Rules\" on the bottom pane, and click \"Edit Inbound Rules\". Click \"Add Rule\": Type : Elastic Graphics Source : <aws-default-sg.id> Click \"Add Rule\": Type : All TCP Source : 10.0.0.0/8 Click \"Add Rule\": Type : All UDP Source : 10.0.0.0/8 Click \"Save rules\"","title":"Open up the internal firewal"},{"location":"setup/aws-network/#aws-vpn-connection","text":"Sets up keys and a VPN connection so your local computer can directly communicate with instances and services on the private subnet.","title":"AWS VPN Connection"},{"location":"setup/aws-network/#create-keys-for-vpn-access","text":"Create sever and client certs: Follow the instructions here . Keep track of: ca.crt as: <aws-ca-certs.crt> ca.key as: <aws-ca-certs.key> server.crt as: <aws-server-cert.crt> server.key as: <aws-server-cert.key> client1.domain.tld.crt as: <aws-client-cert.crt> client1.domain.tld.key as: <aws-client-cert.key> Import the server and client certs using the ACM console : Follow the instructions here . Certificate Body : Content of <aws-*-cert.crt> between -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- , inclusive. Certificate Private Key : Content of <aws-*-cert.key> Certificate Chain : Content of <aws-ca-cert.crt> Open the AWS Certificate Manager's List Certificates page : For the server cert, with domain name \"server\": Keep track of \"Identifier\" as: <aws-server-cert.id> Keep track of \"ARN\" as: <aws-server-cert.arn> For the client cert, with domain name \"client1.domain.tld\": Keep track of \"Identifier\" as: <aws-client-cert.id> Keep track of \"ARN\" as: <aws-client-cert.arn>","title":"Create keys for VPN access"},{"location":"setup/aws-network/#create-the-vpn-interface","text":"Follow the instructions here . Step 2: Name : <aws-cvpn> Client IPV4 CIDR : 10.10.0.0/22 Server Cert ARN : <aws-server-cert.arn> Authentication Option : Mutual Client Cert ARN : <aws-client-cert.arn> Connection Logging : No Client Connect Handler : no Optional Params: Transport : UDP Enable Split Tunnel : Yes VPC ID : <aws-vpc.id> Security Group IDs : <aws-default-sg.id> VPN Port : 443 Enable Self-Service : yes Session Timeout : 24h Enable Client Logic Banner : No Save the Client VPN ID as <aws-cvpn.id> Step 3: Client VPN Assoc VPC : <aws-vpc.id> Subnet : <aws-private-subnet.id> Step 4: Add Auth Rule: Dest Network : 10.0.0.0/8 Grant Access : all users Step 5: Add Route: Route Dest : 0.0.0.0/0 Target Subnet : <aws-private-subnet.id> Add Auth Rule: Dest Network : 0.0.0.0/0 Grant Access : all users Step 6: No changes Step 7: Skip this step Step 8: Skip this step","title":"Create the VPN interface"},{"location":"setup/aws-network/#create-client-vpn-config-file","text":"This is the file users will import in order to connect to the above VPN interface. Instructions taken from here and here . Open the Amazon VPC console at https://console.aws.amazon.com/vpc/ In the navigation pane, choose \"Client VPN Endpoints\". Choose <aws-cvpn.id> and click \"Download Client Configuration\". Save this as <aws-cvpn.config> . Open <aws-cvpn.config> in a text editor. Prepend a random subdomain to the ClientVPN DNS entry The line should start with remote cvpn-endpoint- . When finished it should begin remote ****.cvpn-endpoint- with the rest remaining the same and **** replaced with some random string of your own. After the line that begins verb 3 insert the following: <ca> </ca> <cert> </cert> <key> </key> Place the content of <aws-ca-cert.crt> between the <ca> tags. It's fine if this is already populated in the downloaded <aws-cvpn.config> . Place the certificate from <aws-client-cert.crt> between the <cert> tags. The certificate is what's between -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- , inclusive. Place the content of <aws-client-cert.key> between the <key> tags. Save the modified file. Sample <aws-cvpn.config> after above modifications. The actual contents of the certificates have been replaced with ... . client dev tun proto udp remote asdf.cvpn-endpoint-0011abcabcabcabc1.prod.clientvpn.eu-west-2.amazonaws.com 443 remote-random-hostname resolv-retry infinite nobind remote-cert-tls server cipher AES-256-GCM verb 3 <ca> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- </key> reneg-sec 0 Distribute the <aws-cvpn.config> to your intended users.","title":"Create Client VPN Config File"},{"location":"setup/aws-network/#connect-to-the-vpn","text":"These instructions apply to your users as well as long as you provide them access to the <aws-cvpn.config> file. Linux: Instructions MacOS: Instructions Windows: Instructions","title":"Connect to the VPN"},{"location":"setup/aws-network/#shared-drive","text":"Create a shared drive that you can mount on both your local machine and worker nodes. This drive is both a convenient shared mount for development work, using your local setup to edit server code, and a place for multiple workers to stash results.","title":"Shared Drive"},{"location":"setup/aws-network/#option-1-use-amazon-fsx-for-openzfs","text":"Go to the FSx File System console . Click Create File System : Select file system type : OpenZFS Creation Method : Standard create File System Details: File system name : <aws-fsx.name> SSD Storage capacity : more than 50gb Provisioned SSD IOPS : Automatic Throughput Capacity : Recommended Network and Security: VPC : <aws-vpc.name> VPC Security Groups : <vpc-default-sg.name> Subnet : <aws-private-subnet.name> Encryption: Encryption Key : aws/fsx (default) Root Volume Configuration: Data compression type : No Compression NFS Exports: Client Address: * NFS Options: rw,no_auth_nlm,all_squash,anonuid=0,anongid=0,crossmnt Record Size : Default Click \"Next\" then \"Create File System\" Open the file systems page on the FSx console and select <aws-fsx.name> : Keep track of \"File System ID\" as: <aws-fsx.id> Open \"Network Interface\" and save\"IPv4 address\" as: <aws-fsx.ip> Unix Mount Instructions Ensure you're connected to the VPN. Choose and create a mount directory at <local-mount> . Make sure you have nfs utilities installed. For ubuntu: sudo apt-get -y install nfs-common . Run: sudo mount -t nfs -o nfsvers=4.1 <aws-fsx-ip>:/fsx/ <local-mount> Consider using nfs caching and async. (google for details) Windows Mount Instructions Ensure you're connected to the VPN. Chose a <drive-letter> to be the mount point. (e.g. 'Z:') Open cmd.exe as admin (Not powershell) Run: mount \\\\<aws-fsx-ip>\\fsx\\ <drive-letter>","title":"Option 1: Use Amazon FSx for OpenZFS"},{"location":"setup/aws-network/#create-an-ec2-keypair","text":"This keypair is needed to connect to various EC2 instances in the VPC.","title":"Create an EC2 Keypair"},{"location":"setup/aws-network/#create-a-keypair-for-connecting-to-aws-instances","text":"Open the EC2 console to the Key pairs page and click \" Create key pair \". Name : <ec2-keypair.name> Key Pair Type : RSA Private Key Format : .pem Click \"Create Key Pair\" When prompted save the \" <ec2-keypair.name> .pem\" file as: <ec2-keypair.pem> Continue to AWS Instance Setup ...","title":"Create a keypair for connecting to AWS instances."},{"location":"setup/broker/","text":"Message Broker Setup \u00b6 A message broker allows worker nodes to pull analysis requests from a shared queue. The standard message broker is RabbitMQ with the default settings. Option 1: Run RabbitMQ on a Linux Machine (Recommended) \u00b6 RabbitMQ is a backendless message broker that's sufficient if you're okay with clients not being notified when the analysis result is complete. (It will just appear in the results directory.) Follow the instructions here to set up RabbitMQ. Save this machine's IP as: <broker.ip> Save rabbitmq's open port as: <broker.port> (default: 5672) Option 2: Run Redis on a Linux Machine \u00b6 Redis can be configured as a response backend as well as a message broker. This means that clients can get notified when an analysis is complete. We have not tried to get this running so don't have install instructions. Save this machine's IP as: <broker.ip> and <backend.ip> Save redis's open port as: <broker.port> and <backend.port> (default: 6379) Save the redis database you're using as: <broker.db> and <backend.db> (default: \"/0\") Option 3: Run RabbitMQ as a Windows Service \u00b6 This runs RabbitMQ as a windows service and is fine for development or production setups. Prerequisites \u00b6 General Setup has been completed. Install Dependencies \u00b6 Install utilities and RabbitMQ Open an admin powershell to <repo-root> . Install dependency packages: pdm run setup-win install.broker-deps Open Required Ports \u00b6 Open the relevant ports up so worker and client nodes can connect to the broker. If you only intend to use the broker with workers and clients on the same machine then you don't need to open ports. Option 1: Open only <broker.port> . (Default: 5672) \u00b6 The instructions for this are too configuration specific for us to provide. Option 2: Disable broker firewalls entirely. \u00b6 We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Do not do this on public or untrusted machines Disable the Windows Server 2019 firewall: pdm run setup-win disable-firewall Note: This might work with other versions of Windows but that hasn't been tested. Preserve Settings \u00b6 Keep this machine's IP as: <broker.ip> Keep rabbitmq's open port as: <broker.port> (default: 5672) Start RabbitMQ Service \u00b6 Start the RabbitMQ service: Start Menu -> RabbitMQ Server -> RabbitMQ Service Start The server should automatically start on boot. Option 4: Use AmazonMQ on AWS as a broker \u00b6 Amazon MQ seems fine for use with SimpleUAM, though we've only performed cursory testing. Prerequisites \u00b6 An AWS VPC at <aws-vpc> with a private subnet at <aws-private-subnet> . Create the Amazon MQ Broker \u00b6 Go to the Amazon MQ console on AWS. Click \"Create brokers\": Step 1: Broker engine types: RabbitMQ Step 2: Select deployment mode: Single Instance Broker Step 3: Broker Name: <aws-broker.name> Broker Instance Type: mq.t3.micro (or bigger, though you probably won't need it) RabbitMQ access: Username: <aws-broker.user> Password: <aws-broker.pass> Additional Settings: Access Type: Private Access VPC and Subnets: Select Existing VPC VPC: <aws-vpc> Subnet: <aws-private-subnet> Security Groups: Select Existing SG w/ <aws-default-sg> Step 4: Check settings Click \"Create Broker\" Wait ~20 minutes for the broker to be created. Preserve Settings \u00b6 Go to the Amazon MQ console on AWS. Under \"Brokers\" find <aws-broker.name> and click. Under \"Connections\" find \"Endpoints\". Save the URL under \"AMQP\" as <broker.url> . Example: amqps://b-8f2b68ab-3d0f-4a64-a2bf-24418ebf52d5.mq.us-east-1.amazonaws.com:5671 Option 5: Use Amazon MemoryDB on AWS as a broker \u00b6 Amazon MemoryDB seems fine for use with SimpleUAM, though we've only performed cursory testing. This should allow for more advanced features like return messages when compared to RabbitMQ based brokers. Prerequisites \u00b6 An AWS VPC at <aws-vpc> with a private subnet at <aws-private-subnet> . Create Amazon MemoryDB Broker \u00b6 Go to the MemoryDB dashboard . Click \" Create Cluster \": Step 1: Cluster Settings Choose Cluster Creation Method: Configuration type : Create new cluster Cluster Info: Name : <aws-broker.name> Subnet Groups: Subnet Groups : Create new subnet group Name : <aws-private-subnet.group.name> VPC ID : <aws-vpc.id> Selected Subnets : <aws-private-subnet.id> Cluster Settings: Node Type : db.t4g.small or db.t4g.medium Larger won't be useful unless you have 50+ worker nodes running simultaneously. Number of Shards : 1 Replicas per Shard : 0 Step 2: Advanced Settings Security: Selected Security Groups : <aws-default-sg.id> Encryption Key : Default key Encryption in transit : No encryption Click \"Create\" Go to the MemoryDB Clusters List . Click the entry for <aws-broker.name> . Wait ~20m for the cluster to finish being created. Save \"Cluster endpoint\" as: <aws-broker.endpoint> Prepend \" redis:// \" to <aws-broker.endpoint> and save as: <broker.url> and <backend.url> This should look like \" redis://*.*.clustercfg.memorydb.*.amazonaws.com:6379 \" with \" * \" taken as wildcards.","title":"Broker & Backend"},{"location":"setup/broker/#message-broker-setup","text":"A message broker allows worker nodes to pull analysis requests from a shared queue. The standard message broker is RabbitMQ with the default settings.","title":"Message Broker Setup"},{"location":"setup/broker/#rabbitmq-linux","text":"RabbitMQ is a backendless message broker that's sufficient if you're okay with clients not being notified when the analysis result is complete. (It will just appear in the results directory.) Follow the instructions here to set up RabbitMQ. Save this machine's IP as: <broker.ip> Save rabbitmq's open port as: <broker.port> (default: 5672)","title":"Option 1: Run RabbitMQ on a Linux Machine (Recommended)"},{"location":"setup/broker/#redis-linux","text":"Redis can be configured as a response backend as well as a message broker. This means that clients can get notified when an analysis is complete. We have not tried to get this running so don't have install instructions. Save this machine's IP as: <broker.ip> and <backend.ip> Save redis's open port as: <broker.port> and <backend.port> (default: 6379) Save the redis database you're using as: <broker.db> and <backend.db> (default: \"/0\")","title":"Option 2: Run Redis on a Linux Machine"},{"location":"setup/broker/#rabbitmq-win","text":"This runs RabbitMQ as a windows service and is fine for development or production setups.","title":"Option 3: Run RabbitMQ as a Windows Service"},{"location":"setup/broker/#rabbitmq-win-rereqs","text":"General Setup has been completed.","title":"Prerequisites"},{"location":"setup/broker/#rabbitmq-win-deps","text":"Install utilities and RabbitMQ Open an admin powershell to <repo-root> . Install dependency packages: pdm run setup-win install.broker-deps","title":"Install Dependencies"},{"location":"setup/broker/#rabbitmq-win-ports","text":"Open the relevant ports up so worker and client nodes can connect to the broker. If you only intend to use the broker with workers and clients on the same machine then you don't need to open ports.","title":"Open Required Ports"},{"location":"setup/broker/#rabbitmq-win-ports-single","text":"The instructions for this are too configuration specific for us to provide.","title":"Option 1: Open only &lt;broker.port&gt;. (Default: 5672)"},{"location":"setup/broker/#rabbitmq-win-ports-clobber","text":"We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Do not do this on public or untrusted machines Disable the Windows Server 2019 firewall: pdm run setup-win disable-firewall Note: This might work with other versions of Windows but that hasn't been tested.","title":"Option 2: Disable broker firewalls entirely."},{"location":"setup/broker/#rabbitmq-win-settings","text":"Keep this machine's IP as: <broker.ip> Keep rabbitmq's open port as: <broker.port> (default: 5672)","title":"Preserve Settings"},{"location":"setup/broker/#start-rabbitmq-service","text":"Start the RabbitMQ service: Start Menu -> RabbitMQ Server -> RabbitMQ Service Start The server should automatically start on boot.","title":"Start RabbitMQ Service"},{"location":"setup/broker/#amazonmq","text":"Amazon MQ seems fine for use with SimpleUAM, though we've only performed cursory testing.","title":"Option 4: Use AmazonMQ on AWS as a broker"},{"location":"setup/broker/#amazonmq-prereqs","text":"An AWS VPC at <aws-vpc> with a private subnet at <aws-private-subnet> .","title":"Prerequisites"},{"location":"setup/broker/#amazonmq-create","text":"Go to the Amazon MQ console on AWS. Click \"Create brokers\": Step 1: Broker engine types: RabbitMQ Step 2: Select deployment mode: Single Instance Broker Step 3: Broker Name: <aws-broker.name> Broker Instance Type: mq.t3.micro (or bigger, though you probably won't need it) RabbitMQ access: Username: <aws-broker.user> Password: <aws-broker.pass> Additional Settings: Access Type: Private Access VPC and Subnets: Select Existing VPC VPC: <aws-vpc> Subnet: <aws-private-subnet> Security Groups: Select Existing SG w/ <aws-default-sg> Step 4: Check settings Click \"Create Broker\" Wait ~20 minutes for the broker to be created.","title":"Create the Amazon MQ Broker"},{"location":"setup/broker/#amazonmq-settings","text":"Go to the Amazon MQ console on AWS. Under \"Brokers\" find <aws-broker.name> and click. Under \"Connections\" find \"Endpoints\". Save the URL under \"AMQP\" as <broker.url> . Example: amqps://b-8f2b68ab-3d0f-4a64-a2bf-24418ebf52d5.mq.us-east-1.amazonaws.com:5671","title":"Preserve Settings"},{"location":"setup/broker/#memorydb","text":"Amazon MemoryDB seems fine for use with SimpleUAM, though we've only performed cursory testing. This should allow for more advanced features like return messages when compared to RabbitMQ based brokers.","title":"Option 5: Use Amazon MemoryDB on AWS as a broker"},{"location":"setup/broker/#memorydb-prereqs","text":"An AWS VPC at <aws-vpc> with a private subnet at <aws-private-subnet> .","title":"Prerequisites"},{"location":"setup/broker/#memorydb-create","text":"Go to the MemoryDB dashboard . Click \" Create Cluster \": Step 1: Cluster Settings Choose Cluster Creation Method: Configuration type : Create new cluster Cluster Info: Name : <aws-broker.name> Subnet Groups: Subnet Groups : Create new subnet group Name : <aws-private-subnet.group.name> VPC ID : <aws-vpc.id> Selected Subnets : <aws-private-subnet.id> Cluster Settings: Node Type : db.t4g.small or db.t4g.medium Larger won't be useful unless you have 50+ worker nodes running simultaneously. Number of Shards : 1 Replicas per Shard : 0 Step 2: Advanced Settings Security: Selected Security Groups : <aws-default-sg.id> Encryption Key : Default key Encryption in transit : No encryption Click \"Create\" Go to the MemoryDB Clusters List . Click the entry for <aws-broker.name> . Wait ~20m for the cluster to finish being created. Save \"Cluster endpoint\" as: <aws-broker.endpoint> Prepend \" redis:// \" to <aws-broker.endpoint> and save as: <broker.url> and <backend.url> This should look like \" redis://*.*.clustercfg.memorydb.*.amazonaws.com:6379 \" with \" * \" taken as wildcards.","title":"Create Amazon MemoryDB Broker"},{"location":"setup/client/","text":"Client Node Setup \u00b6 Clients send design analysis requests to a message broker so that worker nodes can process them. Unlike the other nodes clients can easily be of any platform as long as they can access the same brokers and results directory as the workers. Prerequisites \u00b6 An auth token, SSH keys or credentials for git.isis.vanderbilt.edu . A broker running at <broker> . Optionally, a backend running at <backend> . The following installed software: Git Python (>=3.9, <3.11) An environment with the following Python packages: PDM Setup File Sharing (Optional) \u00b6 If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> . Download SimpleUAM \u00b6 Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as <repo-root> . Option 1: Clone from Github (HTTP): \u00b6 git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Option 2: Clone from Github (SSH): \u00b6 git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Initialize SimpleUAM Package \u00b6 Initialize pdm and packages for client use. Navigate a shell to <repo-root> . Setup PDM environment for this repo: pdm install Test whether setup script was installed: pdm run suam-client --help Result should be a help message showing all of suam-client 's flags and sub-commands. Get Configuration Directory \u00b6 The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate a shell to <repo-root> . Print config directory: pdm run suam-config dir Save result as <config-dir> . Configure Broker Settings \u00b6 The client process needs to be configured with how to connect to a message broker. These options should be identical to any worker nodes which is why they use the same config file. Option 1: Use the worker broker.conf.yaml \u00b6 As long as the IPs or DNS names of the broker and backend resolve to the same machines as the worker, then you can reuse the broker.conf.yaml directly. Have the worker's broker.conf.yaml accessible at <conf-loc> . Make a copy in the configuration directory: pdm run suam-config install --no-symlink --input = <conf-loc> See the config file usage page for other ways to set this up... Option 2 : Create a new configuration \u00b6 If visibility of the broker and backend differs from the default then a client specific broker.conf.yaml needs to be created. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = broker -r Create or update the config at <config-dir>/broker.conf.yaml : Set up the broker: If you have a <broker.url> : Set url to <broker.url> . Otherwise: Set host to <broker.ip> . Set protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set port to <broker.port> . Set db to <broker.db> if you have one. Set up the backend, if available: Set backend.enabled to true If you have a <backend.url> : Set broker.url to <broker.url> Otherwise: Set backend.host to <backend.ip> . Set backend.port to <broker.port> . Set backend.db to <broker.db> if you have one. See the config file guide for more detailed instructions and information. Further details on configuring the client's broker and backend are here ... Test the Client Node (Optional) \u00b6 Run a simple test task, generating the info files for a design, in order to test the client node's configuration. Have a valid design file (usually design_swri.json ) at <design-file> . Have the results dir available to client at <results-dir> Have a broker running at the configured location. Have at least one worker running and connected to the broker. Open a shell to <repo-root> . Test generating design info files: pdm run suam-client direct2cad.gen-info-files --design = <design-file> --results = <results-dir> A worker should pick up this task and run it, eventually placing an archive in the <results-dir> with the generated info files. Test generating processing designs: pdm run suam-client direct2cad.process-design --design = <design-file> --results = <results-dir> A worker should pick up this task and run it, eventually placing an archive in the <results-dir> with the generated results. Information on how to run a fill analysis pipeline, through either a CLI or Python code, can be found here ...","title":"Client Node"},{"location":"setup/client/#client-node-setup","text":"Clients send design analysis requests to a message broker so that worker nodes can process them. Unlike the other nodes clients can easily be of any platform as long as they can access the same brokers and results directory as the workers.","title":"Client Node Setup"},{"location":"setup/client/#prereq","text":"An auth token, SSH keys or credentials for git.isis.vanderbilt.edu . A broker running at <broker> . Optionally, a backend running at <backend> . The following installed software: Git Python (>=3.9, <3.11) An environment with the following Python packages: PDM","title":"Prerequisites"},{"location":"setup/client/#files","text":"If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> .","title":"Setup File Sharing (Optional)"},{"location":"setup/client/#download","text":"Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as <repo-root> .","title":"Download SimpleUAM"},{"location":"setup/client/#download-github-http","text":"git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 1: Clone from Github (HTTP):"},{"location":"setup/client/#download-github-ssh","text":"git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 2: Clone from Github (SSH):"},{"location":"setup/client/#init","text":"Initialize pdm and packages for client use. Navigate a shell to <repo-root> . Setup PDM environment for this repo: pdm install Test whether setup script was installed: pdm run suam-client --help Result should be a help message showing all of suam-client 's flags and sub-commands.","title":"Initialize SimpleUAM Package"},{"location":"setup/client/#config","text":"The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate a shell to <repo-root> . Print config directory: pdm run suam-config dir Save result as <config-dir> .","title":"Get Configuration Directory"},{"location":"setup/client/#broker","text":"The client process needs to be configured with how to connect to a message broker. These options should be identical to any worker nodes which is why they use the same config file.","title":"Configure Broker Settings"},{"location":"setup/client/#broker-reuse","text":"As long as the IPs or DNS names of the broker and backend resolve to the same machines as the worker, then you can reuse the broker.conf.yaml directly. Have the worker's broker.conf.yaml accessible at <conf-loc> . Make a copy in the configuration directory: pdm run suam-config install --no-symlink --input = <conf-loc> See the config file usage page for other ways to set this up...","title":"Option 1: Use the worker broker.conf.yaml"},{"location":"setup/client/#broker-new","text":"If visibility of the broker and backend differs from the default then a client specific broker.conf.yaml needs to be created. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = broker -r Create or update the config at <config-dir>/broker.conf.yaml : Set up the broker: If you have a <broker.url> : Set url to <broker.url> . Otherwise: Set host to <broker.ip> . Set protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set port to <broker.port> . Set db to <broker.db> if you have one. Set up the backend, if available: Set backend.enabled to true If you have a <backend.url> : Set broker.url to <broker.url> Otherwise: Set backend.host to <backend.ip> . Set backend.port to <broker.port> . Set backend.db to <broker.db> if you have one. See the config file guide for more detailed instructions and information. Further details on configuring the client's broker and backend are here ...","title":"Option 2: Create a new configuration"},{"location":"setup/client/#test","text":"Run a simple test task, generating the info files for a design, in order to test the client node's configuration. Have a valid design file (usually design_swri.json ) at <design-file> . Have the results dir available to client at <results-dir> Have a broker running at the configured location. Have at least one worker running and connected to the broker. Open a shell to <repo-root> . Test generating design info files: pdm run suam-client direct2cad.gen-info-files --design = <design-file> --results = <results-dir> A worker should pick up this task and run it, eventually placing an archive in the <results-dir> with the generated info files. Test generating processing designs: pdm run suam-client direct2cad.process-design --design = <design-file> --results = <results-dir> A worker should pick up this task and run it, eventually placing an archive in the <results-dir> with the generated results. Information on how to run a fill analysis pipeline, through either a CLI or Python code, can be found here ...","title":"Test the Client Node (Optional)"},{"location":"setup/corpus/","text":"Static Corpus Setup \u00b6 The graph database is far too bloated for practical use, instead we can export the relevant information to a small json file and use that directly. This file is needed for many of the tasks that worker nodes will do and needs to be retrieved or generated. Option 1: Use static corpus provided in repo (Recommended) \u00b6 This static corpus was generated from all_schema.graphml on July 15th '22. Prerequisites \u00b6 General Setup has been completed. Install Static Corpus \u00b6 Once configured open admin powershell at <repo-root> . Install the corpus provided with this repo: pdm run craidl static-corpus.copy Option 2: User a user provided static corpus \u00b6 This will just load the user provided file into the install location. Prerequisites \u00b6 General Setup has been completed. The corpus to be installed at <static-corpus-loc> . Install Static Corpus \u00b6 Once configured open admin powershell at <repo-root> . Install a user provided corpus from <static-corpus-loc> pdm run craidl static-corpus.copy --input = <static-corpus-loc> Option 3: Generate Static Corpus from a Corpus Database \u00b6 This will generate a static corpus from a running graph server, using whatever corpus it was configured with. Prerequisites \u00b6 General Setup has been completed. Configure Server Settings \u00b6 The config file at <config-dir>/craidl.conf.yaml stores information for connecting to a corpus DB server. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r The fields under server_host and server_port determine which corpus database this tool will connect to when generating a static corpus. The default options connect to a stub server running on the same machine. (Optional) If you're connecting to a corpus database not running on the current machine then create or update <config-dir>/craidl.conf.yaml . Set server_host to <corpus-db.ip> . Set server_port to <corpus-db.port> . Generate Static Corpus \u00b6 Ensure the server at <corpus-db.ip> is currently running. Generate the static corpus from that server. pdm run craidl static-corpus.generate The stub server seems to hang when out of memory or CPU, halting the serialization process. To mitigate this the generation process periodically saves the serialized component data and skips re-downloading saved data. By default every cluster of 50 components is saved to disk. Just rerun the command (with the same settings) and it will resume generating the corpus from the last saved cluster of components. See the section on using Craidl for information on how to use the generated static corpus...","title":"Static Corpus"},{"location":"setup/corpus/#static-corpus-setup","text":"The graph database is far too bloated for practical use, instead we can export the relevant information to a small json file and use that directly. This file is needed for many of the tasks that worker nodes will do and needs to be retrieved or generated.","title":"Static Corpus Setup"},{"location":"setup/corpus/#repo","text":"This static corpus was generated from all_schema.graphml on July 15th '22.","title":"Option 1: Use static corpus provided in repo (Recommended)"},{"location":"setup/corpus/#repo-prereqs","text":"General Setup has been completed.","title":"Prerequisites"},{"location":"setup/corpus/#repo-install","text":"Once configured open admin powershell at <repo-root> . Install the corpus provided with this repo: pdm run craidl static-corpus.copy","title":"Install Static Corpus"},{"location":"setup/corpus/#copy","text":"This will just load the user provided file into the install location.","title":"Option 2: User a user provided static corpus"},{"location":"setup/corpus/#copy-prereqs","text":"General Setup has been completed. The corpus to be installed at <static-corpus-loc> .","title":"Prerequisites"},{"location":"setup/corpus/#copy-install","text":"Once configured open admin powershell at <repo-root> . Install a user provided corpus from <static-corpus-loc> pdm run craidl static-corpus.copy --input = <static-corpus-loc>","title":"Install Static Corpus"},{"location":"setup/corpus/#generate","text":"This will generate a static corpus from a running graph server, using whatever corpus it was configured with.","title":"Option 3: Generate Static Corpus from a Corpus Database"},{"location":"setup/corpus/#generate-prereqs","text":"General Setup has been completed.","title":"Prerequisites"},{"location":"setup/corpus/#generate-configure","text":"The config file at <config-dir>/craidl.conf.yaml stores information for connecting to a corpus DB server. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r The fields under server_host and server_port determine which corpus database this tool will connect to when generating a static corpus. The default options connect to a stub server running on the same machine. (Optional) If you're connecting to a corpus database not running on the current machine then create or update <config-dir>/craidl.conf.yaml . Set server_host to <corpus-db.ip> . Set server_port to <corpus-db.port> .","title":"Configure Server Settings"},{"location":"setup/corpus/#generate-run","text":"Ensure the server at <corpus-db.ip> is currently running. Generate the static corpus from that server. pdm run craidl static-corpus.generate The stub server seems to hang when out of memory or CPU, halting the serialization process. To mitigate this the generation process periodically saves the serialized component data and skips re-downloading saved data. By default every cluster of 50 components is saved to disk. Just rerun the command (with the same settings) and it will resume generating the corpus from the last saved cluster of components. See the section on using Craidl for information on how to use the generated static corpus...","title":"Generate Static Corpus"},{"location":"setup/general/","text":"General Machine Setup \u00b6 This is setup that needs to be done for multiple types of machines. Prerequisites \u00b6 This machine must be running Windows. If you're using a Linux machine skip to the setup page of whatever component type it's meant to host. You must be okay using chocolatey as your package manager. Install Chocolatey and Minimal Deps \u00b6 This downloads a minimal install script for chocolatey, git, and python. This step is idempotent and will just do nothing if these are all installed. Open admin powershell and run: iwr -Uri https://raw.githubusercontent.com/LOGiCS-Project/swri-simple-uam-pipeline/main/data/setup/bootstrap_win.ps1 | iex Close this powershell terminal and open new ones for future steps. Download SimpleUAM \u00b6 Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as: <repo-root> Option 1 : Clone From Github (HTTP): \u00b6 Clone from Github via HTTP, replacing <repo-root> in the following command: git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Option 2 : Clone From Github (SSH): \u00b6 Clone from Github via SSH, replacing <repo-root> in the following command: git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root> Option 3 : Retrieve From Other Source \u00b6 Details are left to the user. Initialize Setup Package \u00b6 Initialize pdm and packages for worker setup. Navigate an admin powershell to <repo_root> . Setup PDM environment for this repo: pdm install Test whether setup script was installed: pdm run setup-win --help Result should be a help message showing all of setup-win 's flags and subcommands. Get Configuration Directory \u00b6 The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate an admin powershell to <repo_root> . Print config directory: pdm run suam-config dir Save result as <config-dir> . Install Quality of Life Packages (Optional) \u00b6 This installs Firefox, Notepad++, Tess, and other applications that make working on a new windows install more bearable. Navigate an admin powershell to <repo_root> . Install the packages: pdm run setup-win install.qol-deps Close this powershell terminal and open new ones for future steps. One of the default QoL packages, Tess , is significantly nicer to use than the native powershell terminal. Consider using it to create an admin powershell instead of the OS-provided terminal. Setup File Sharing (Optional) \u00b6 If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> . Set up Isis Authentication \u00b6 Various private resources are on git.isis.vanderbilt.edu. So that they aren't made available to the public SimpleUAM will retrieve any non-public information using credentials you provide. Option 1 : Install an API token for git.isis.vanderbilt.edu (Recommended) \u00b6 We can use an API token to automate some repository and file accesses using Isis that would otherwise require manual authentication. Create a Personal Access Token \u00b6 Log into https://git.isis.vanderbilt.edu . Save your isis username as: <isis-auth.user> Go to \"User Settings\" -> \" Access Tokens \". Token Name : <isis-auth.token-name> Expiration Date : 2100-01-01 Select Scopes: read_api : Check read_repository : Check All other others should be unchecked. Click \"Create Personal Access Token\". Save \"Your new personal access token\" as: <isis-auth.token> Configure SimpleUAM to use your token \u00b6 Open <config-dir>/auth.conf.yaml in a text editor. Set the isis_user field to <isis-auth.user> Set the isis_token field to <isis-auth.token> Sample auth.conf.yaml ### auth.conf.yaml ### isis_user : myIsisUsername isis_token : 'glpat-XXXXXXXXXXXXXXXXXXXX' Option 2 : Install SSH keys for git.isis.vanderbilt.edu \u00b6 SSH access to Isis, while not strictly necessary, will make future install steps easier and more secure. This doesn't do anything if an API token is already provided. Follow the instructions here to set up ssh key based access to the isis server. Option 3 : Skip Authentication \u00b6 If you skip authentication here you will prompted for passwords when cloning repositories and asked to manually download files when needed. Next Steps \u00b6 Once this setup is complete you can continue to one or more of the following steps. All of the following nodes can coexist on a single windows instance. Continue to Creo License Server Setup ... Or to Message Broker Setup ... Or to Corpus DB Setup ... Or to Static Corpus Setup ... Or to Worker Node Setup ...","title":"General Setup"},{"location":"setup/general/#general-machine-setup","text":"This is setup that needs to be done for multiple types of machines.","title":"General Machine Setup"},{"location":"setup/general/#prereqs","text":"This machine must be running Windows. If you're using a Linux machine skip to the setup page of whatever component type it's meant to host. You must be okay using chocolatey as your package manager.","title":"Prerequisites"},{"location":"setup/general/#install-choco","text":"This downloads a minimal install script for chocolatey, git, and python. This step is idempotent and will just do nothing if these are all installed. Open admin powershell and run: iwr -Uri https://raw.githubusercontent.com/LOGiCS-Project/swri-simple-uam-pipeline/main/data/setup/bootstrap_win.ps1 | iex Close this powershell terminal and open new ones for future steps.","title":"Install Chocolatey and Minimal Deps"},{"location":"setup/general/#suam","text":"Get this repo onto the machine somehow, cloning is the default method. If you have a shared drive, placing the repo there will allow local development without constant pushing and pulling. Save the repo's final location as: <repo-root>","title":"Download SimpleUAM"},{"location":"setup/general/#suam-github-http","text":"Clone from Github via HTTP, replacing <repo-root> in the following command: git clone https://github.com/LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 1: Clone From Github (HTTP):"},{"location":"setup/general/#suam-github-ssh","text":"Clone from Github via SSH, replacing <repo-root> in the following command: git clone git@github.com:LOGiCS-Project/swri-simple-uam-pipeline.git <repo-root>","title":"Option 2: Clone From Github (SSH):"},{"location":"setup/general/#suam-other","text":"Details are left to the user.","title":"Option 3: Retrieve From Other Source"},{"location":"setup/general/#setup","text":"Initialize pdm and packages for worker setup. Navigate an admin powershell to <repo_root> . Setup PDM environment for this repo: pdm install Test whether setup script was installed: pdm run setup-win --help Result should be a help message showing all of setup-win 's flags and subcommands.","title":"Initialize Setup Package"},{"location":"setup/general/#conf-dir","text":"The configuration directory holds *.conf.yaml files that determine how many aspects of a running SimpleUAM system operate. Navigate an admin powershell to <repo_root> . Print config directory: pdm run suam-config dir Save result as <config-dir> .","title":"Get Configuration Directory"},{"location":"setup/general/#qol","text":"This installs Firefox, Notepad++, Tess, and other applications that make working on a new windows install more bearable. Navigate an admin powershell to <repo_root> . Install the packages: pdm run setup-win install.qol-deps Close this powershell terminal and open new ones for future steps. One of the default QoL packages, Tess , is significantly nicer to use than the native powershell terminal. Consider using it to create an admin powershell instead of the OS-provided terminal.","title":"Install Quality of Life Packages (Optional)"},{"location":"setup/general/#nfs","text":"If you intend to share files (e.g. results) between workers and clients then set that up now, if you haven't done so already. If using AWS the instructions for the FSx file share take care of this. Otherwise shared directory and file server configurations are too varied for us to provide precise instructions. The only real requirement is that worker nodes see the shared file storage as a normal directory. Save the shared results directory as <results-dir> .","title":"Setup File Sharing (Optional)"},{"location":"setup/general/#isis","text":"Various private resources are on git.isis.vanderbilt.edu. So that they aren't made available to the public SimpleUAM will retrieve any non-public information using credentials you provide.","title":"Set up Isis Authentication"},{"location":"setup/general/#isis-token","text":"We can use an API token to automate some repository and file accesses using Isis that would otherwise require manual authentication.","title":"Option 1: Install an API token for git.isis.vanderbilt.edu (Recommended)"},{"location":"setup/general/#isis-token-create","text":"Log into https://git.isis.vanderbilt.edu . Save your isis username as: <isis-auth.user> Go to \"User Settings\" -> \" Access Tokens \". Token Name : <isis-auth.token-name> Expiration Date : 2100-01-01 Select Scopes: read_api : Check read_repository : Check All other others should be unchecked. Click \"Create Personal Access Token\". Save \"Your new personal access token\" as: <isis-auth.token>","title":"Create a Personal Access Token"},{"location":"setup/general/#isis-token-config","text":"Open <config-dir>/auth.conf.yaml in a text editor. Set the isis_user field to <isis-auth.user> Set the isis_token field to <isis-auth.token> Sample auth.conf.yaml ### auth.conf.yaml ### isis_user : myIsisUsername isis_token : 'glpat-XXXXXXXXXXXXXXXXXXXX'","title":"Configure SimpleUAM to use your token"},{"location":"setup/general/#isis-ssh","text":"SSH access to Isis, while not strictly necessary, will make future install steps easier and more secure. This doesn't do anything if an API token is already provided. Follow the instructions here to set up ssh key based access to the isis server.","title":"Option 2: Install SSH keys for git.isis.vanderbilt.edu"},{"location":"setup/general/#isis-skip","text":"If you skip authentication here you will prompted for passwords when cloning repositories and asked to manually download files when needed.","title":"Option 3: Skip Authentication"},{"location":"setup/general/#next","text":"Once this setup is complete you can continue to one or more of the following steps. All of the following nodes can coexist on a single windows instance. Continue to Creo License Server Setup ... Or to Message Broker Setup ... Or to Corpus DB Setup ... Or to Static Corpus Setup ... Or to Worker Node Setup ...","title":"Next Steps"},{"location":"setup/graph/","text":"Corpus Database Setup \u00b6 SWRi distributes their corpus of design data as a graph database that needs to be queried for various tasks. Option 1: Running a Local Stub Server (Recommended) \u00b6 This is a local, minimal version of a corpus DB that uses TinkerPop 's reference graph db implementation. It is not persistent so no changes will be preserved between restarts. Prerequisites \u00b6 General Setup has been completed. If not using the default corpus have the .graphml corpus ready at <corpus-loc> . If using the default corpus have an auth token, SSH keys, or credentials for git.isis.vanderbilt.edu set up. Install Dependencies \u00b6 Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.graph-deps Reboot the machine. Install GraphML Corpus \u00b6 We need to install the GraphML corpus to a default location so future setup steps can find it. Option 1 : Download default from athens-uav-workflows \u00b6 Open an admin powershell at <repo-root> . Automatically download athens-uav-workflows repo and get all_schema_uam.graphml : pdm run craidl corpus.download Follow prompts, entering in git.isis.vanderbilt.edu credentials if needed. Install corpus to default location: pdm run craidl corpus.install Option 2 : Install corpus from user provided file \u00b6 Open admin powershell to <repo-root> . Install user provided corpus from <corpus-loc> : pdm run craidl corpus.install --corpus = <corpus-loc> Configure Corpus DB Server \u00b6 The config file at <config-dir>/craidl.conf.yaml stores information for running SimpleUAM's stub corpus database. Those settings are then used to configure the stub server. Open admin powershell to <repo-root> . (Optional) View currently loaded configuration: pdm run suam-config print --config = craidl -r The fields under stub_server.host and stub_server.port determine how the corpus is served. (Optional) If serving the graph to other machines then update or create the stub_server.host config property. Set stub_server.host to 0.0.0.0 . See the config file guide for more detailed instructions and information. Install and configure corpus DB: pdm run craidl stub-server.configure In the absence of any arguments this uses the configured host, port, and graphml corpus from <corpus-dir>/craid.conf.yaml . Run Corpus DB Server (Optional) \u00b6 This runs the corpus DB stub server as a local process. Use it as needed, especially when generating a static corpus. Run corpus DB: pdm run craidl stub-server.run Kill the process manually (usually with Ctrl-C) when finished. Open Required Ports (Optional) \u00b6 Open the relevant ports up so that non-local worker nodes can connect to the stub database. We do not recommend non-local connections to a stub corpus database. If you only intend to connect to a local worker node or locally generate a static corpus then you don't need to open any ports up. Option 1: Open only port 8182. \u00b6 The instructions for this are too configuration specific for us to provide. Option 2: Disable corpus DB firewalls entirely. \u00b6 We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Do not do this on public or untrusted machines Disable the Windows Server 2019 firewall: pdm run setup-win disable-firewall Note: This might work with other versions of Windows but that hasn't been tested. Preserve Settings \u00b6 If you're connecting from a different machine. Keep this machine's IP as: <corpus-db.ip> Keep the database's open port as: <corpus-db.port> (default: 8182) Option 2: Run a full corpus database. \u00b6 We try to avoid using the SWRi provided graph database and haven't explored alternate options, so this is up to you. Preserve Settings \u00b6 If you're connecting from a different machine. Keep this machine's IP as: <corpus-db.ip> Keep the database's open port as: <corpus-db.port> (default: 8182) Next Steps \u00b6 Generate a static corpus with the instructions here ... See the section on using Craidl for information on how to run and use this server...","title":"Corpus Database"},{"location":"setup/graph/#corpus-database-setup","text":"SWRi distributes their corpus of design data as a graph database that needs to be queried for various tasks.","title":"Corpus Database Setup"},{"location":"setup/graph/#stub","text":"This is a local, minimal version of a corpus DB that uses TinkerPop 's reference graph db implementation. It is not persistent so no changes will be preserved between restarts.","title":"Option 1: Running a Local Stub Server (Recommended)"},{"location":"setup/graph/#stub-prereqs","text":"General Setup has been completed. If not using the default corpus have the .graphml corpus ready at <corpus-loc> . If using the default corpus have an auth token, SSH keys, or credentials for git.isis.vanderbilt.edu set up.","title":"Prerequisites"},{"location":"setup/graph/#stub-deps","text":"Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.graph-deps Reboot the machine.","title":"Install Dependencies"},{"location":"setup/graph/#stub-corpus","text":"We need to install the GraphML corpus to a default location so future setup steps can find it.","title":"Install GraphML Corpus"},{"location":"setup/graph/#stub-corpus-download","text":"Open an admin powershell at <repo-root> . Automatically download athens-uav-workflows repo and get all_schema_uam.graphml : pdm run craidl corpus.download Follow prompts, entering in git.isis.vanderbilt.edu credentials if needed. Install corpus to default location: pdm run craidl corpus.install","title":"Option 1: Download default from athens-uav-workflows"},{"location":"setup/graph/#stub-corpus-file","text":"Open admin powershell to <repo-root> . Install user provided corpus from <corpus-loc> : pdm run craidl corpus.install --corpus = <corpus-loc>","title":"Option 2: Install corpus from user provided file"},{"location":"setup/graph/#stub-configure","text":"The config file at <config-dir>/craidl.conf.yaml stores information for running SimpleUAM's stub corpus database. Those settings are then used to configure the stub server. Open admin powershell to <repo-root> . (Optional) View currently loaded configuration: pdm run suam-config print --config = craidl -r The fields under stub_server.host and stub_server.port determine how the corpus is served. (Optional) If serving the graph to other machines then update or create the stub_server.host config property. Set stub_server.host to 0.0.0.0 . See the config file guide for more detailed instructions and information. Install and configure corpus DB: pdm run craidl stub-server.configure In the absence of any arguments this uses the configured host, port, and graphml corpus from <corpus-dir>/craid.conf.yaml .","title":"Configure Corpus DB Server"},{"location":"setup/graph/#stub-run","text":"This runs the corpus DB stub server as a local process. Use it as needed, especially when generating a static corpus. Run corpus DB: pdm run craidl stub-server.run Kill the process manually (usually with Ctrl-C) when finished.","title":"Run Corpus DB Server (Optional)"},{"location":"setup/graph/#open-required-ports-optional","text":"Open the relevant ports up so that non-local worker nodes can connect to the stub database. We do not recommend non-local connections to a stub corpus database. If you only intend to connect to a local worker node or locally generate a static corpus then you don't need to open any ports up.","title":"Open Required Ports (Optional)"},{"location":"setup/graph/#option-1-open-only-port-8182","text":"The instructions for this are too configuration specific for us to provide.","title":"Option 1: Open only port 8182."},{"location":"setup/graph/#option-2-disable-corpus-db-firewalls-entirely","text":"We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Do not do this on public or untrusted machines Disable the Windows Server 2019 firewall: pdm run setup-win disable-firewall Note: This might work with other versions of Windows but that hasn't been tested.","title":"Option 2: Disable corpus DB firewalls entirely."},{"location":"setup/graph/#preserve-settings","text":"If you're connecting from a different machine. Keep this machine's IP as: <corpus-db.ip> Keep the database's open port as: <corpus-db.port> (default: 8182)","title":"Preserve Settings"},{"location":"setup/graph/#option-2-run-a-full-corpus-database","text":"We try to avoid using the SWRi provided graph database and haven't explored alternate options, so this is up to you.","title":"Option 2: Run a full corpus database."},{"location":"setup/graph/#preserve-settings_1","text":"If you're connecting from a different machine. Keep this machine's IP as: <corpus-db.ip> Keep the database's open port as: <corpus-db.port> (default: 8182)","title":"Preserve Settings"},{"location":"setup/graph/#next-steps","text":"Generate a static corpus with the instructions here ... See the section on using Craidl for information on how to run and use this server...","title":"Next Steps"},{"location":"setup/intro/","text":"SimpleUAM Installation Guide \u00b6 Info We would appreciate assistance in making this guide better. Any notes on issues with the install process, lack of clarity in wording, or other improvements would be appreciated. SimpleUAM Component Structure The core goal of SimpleUAM is allow users to set up a service for processing requests to analyze UAM and UAV designs. Client nodes, such as optimizers or search tools, should be able to queue requests for distribution to worker nodes as they become available. The results of those analyses, packaged as zip files, should then be made available to the clients as they're completed. The key components of a SimpleUAM deployment are: Client Nodes : These make requests to analyze designs and retrieve analysis results once they finish. Client nodes will usually be running some optimization or search process. Message Brokers : These gather analysis requests from client nodes and distribute them to free worker nodes. Worker Nodes : These will perform analyses on designs and store the results somewhere accessible to the clients. License Management : Each worker node needs Creo to perform analysis, and a license for Creo must be made available somehow. License Server : These can provide licenses to a number of worker nodes at once if provided with a floating license. Node-Locked Creo License : This is a worker-specific, static license file that can be placed on a worker. Component Corpus : A corpus of component information that every worker needs in order to analyze designs. Corpus DB : A graph database that the worker can use to look up component information. Static Corpus : A static file containing a dump of the component corpus which is placed on each worker node. Results Storage : A file system, accessible to both worker nodes and clients, where analysis results (in individual zip files) are placed. Results Backends : These notify client nodes when their analysis requests are completed and where the output is in Results Storage. In order to form a complete SimpleUAM deployment some core requirements need to be met: There must be one, and only one, message broker. The broker must be accessible over the network to all worker and client nodes. There needs to be at least one configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to a component corpus, either through an initialized corpus DB or a static corpus file. There must be a results storage accessible to all the worker and client nodes. The results storage should be a directory where workers can place files which are then made accessible to all the clients. Possible Storage Mechanisms Generally the results store will be a mounted network file share or local folder, but could also be: A network file system mounted on workers and clients. (Default) A local folder, if both worker and client are on the same machine. rsync + cron Dropbox Google Drive S3 Drive Anything that looks like a filesystem directory to both the worker and the client. In order for clients to receive analysis completion notifications there must be a single, unique results backend. This backend must be accessible over the network to all worker nodes and any client that wants to receive notifications. A results backend is optional and simply polling the results storage is perfectly viable. With a SimpleUAM deployment meeting those requirements, a client nodes can offload analysis jobs to a pool of workers though simple python and command line interfaces. Choosing a Service Topology \u00b6 It's possible to distribute SimpleUAM components between multiple machines in numerous ways that meet the given requirements. Picking a topology, specifically the components that go on each individual machine, tells you which installation steps are needed for that machine. We'll look at two example topologies, one I use for (semi)local development work and one for a potential production system. Development SimpleUAM System This development setup has a local machine and a single worker. The local machine is set up so it can run a SimpleUAM client and so that any code shared with a worker node can be edited in whatever manner the user is comfortable with. The worker node, running in a VM, then has all the other necessary components of the service, including broker, license, and corpus. The structure is a broad guideline and can be tweaked as needed. For instance, if you're running windows you can just run all the components on your local machine and use a stub message broker that will run analysis requests as blocking calls. Alternately, the worker node can be running on a server somewhere with a NFS shared drive acting as the shared folder. Production SimpleUAM System The production service has significantly more going on. There are one or more clients producing analysis requests, multiple workers processing them, a Creo license server, a message broker, a results backend, and results storage. This setup can scale relatively easily while providing features like completion notifications. In fact this is the intended topology for the AWS instructions, with clients either accessing the other components via a VPN or simply running directly on the cloud. Other topologies are also viable, for instance running a central graph database for all the workers to share instead of relying on a local, static corpus. The most important part of choosing a service topology is knowing what component(s) are going to be running on each individual server or VM. Given that one can just go through the instructions for each component on a machine in sequence, repeating that process for each machine in the deployment. Command Line Interfaces \u00b6 All the command line scripts SimpleUAM provides are made using Invoke and evaluated within a PDM administered python environment. This means that all the SimpleUAM provided commands must be run from <repo-root> and have this format: pdm run <command> All the core SimpleUAM commands suam-config , setup-win , craidl , d2c-workspace , and d2c-client will print a help message when run without arguments. In their base form these commands are safe and will never make change to your system. The help messages also provide a list of subcommands that do perform various tasks. These subcommands are run with: pdm run <command> <sub-command> [ ARGS ] All of these subcommands come with detailed help information that can be accessed with: pdm run <command> <sub-command> --help These help messages are worth checking for available options and notes. Configuration \u00b6 SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. While you can find a more detailed breakdown of the system here , this is a quick overview. Configuration File Directory \u00b6 Once the SimpleUAM project is installed (in General Setup ) you can run the following command to find the config file directory: pdm run suam-config dir Files placed there will be loaded when most SimpleUAM code is started up. The configuration is immutable for the runtime of a program and changes will require a restart to register. Configuration State \u00b6 You can get a printout of the current configuration state with the following: pdm run suam-config print --all Sample Output of pdm run suam-config print --all ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### auth.conf.yaml ### isis_user : null isis_token : null ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - openjdk11 - rsync - nssm worker_pip_packages : - psutil - parea - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 - nssm qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : ${path:data_directory}/craidl_examples stub_server : cache_dir : ${path:cache_directory}/corpus_stub_cache server_dir : ${path:data_directory}/corpus_stub_server graphml_corpus : ${path:data_directory}/corpus_stub.graphml host : localhost port : 8182 read_only : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/craidl_stub_db/stdout.log stderr_file : ${path:log_directory}/craidl_stub_db/stderr.log rotate_io : true auto_start : false console : true interactive : false server_host : localhost server_port : ${stub_server.port} static_corpus : ${path:data_directory}/corpus_static_dump.json static_corpus_cache : ${path:cache_directory}/static_corpus_cache use_static_corpus : true ### corpus.conf.yaml ### trinity : repo : https://git.isis.vanderbilt.edu/SwRI/ta1/sri-ta1/trinity-craidl.git branch : main examples_dir : examples graphml_corpus : repo : https://git.isis.vanderbilt.edu/SwRI/athens-uav-workflows.git branch : uam_corpus graphml_file : ExportedGraphML/all_schema_uam.graphml creopyson : repo : https://git.isis.vanderbilt.edu/SwRI/creoson/creopyson.git branch : main creoson_server : api : https://git.isis.vanderbilt.edu/api/v4/projects/499/jobs/3827/artifacts/out/CreosonServerWithSetup-2.8.0-win64.zip manual : https://git.isis.vanderbilt.edu/SwRI/creoson/creoson-server/-/jobs/artifacts/main/raw/out/CreosonServerWithSetup-2.8.0-win64.zip?job=build-job direct2cad : repo : https://git.isis.vanderbilt.edu/SwRI/uam_direct2cad.git branch : main ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : ${workspaces_dir}/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json workspaces_dir : ${path:work_directory}/d2c_workspaces cache_dir : ${path:cache_directory}/d2c_workspaces max_workspaces : 1 exclude : - .git result_exclude : - .git - workingdir/*.prt ### broker.conf.yaml ### protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : ${.protocol}://${.host}:${.port}${.db} backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : ${.protocol}://${.host}:${.port}/${.db} ### d2c_worker.conf.yaml ### max_processes : ${d2c_workspace:max_workspaces} max_threads : 1 shutdown_timeout : 600000 skip_logging : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/d2c_worker/stdout.log stderr_file : ${path:log_directory}/d2c_worker/stderr.log rotate_io : true auto_start : false console : true interactive : false If you want to see the full expanded version of the configs, with all the interpolations resolved, add the --resolved flag. Sample Output of pdm run suam-config print --all --resolved ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### auth.conf.yaml ### isis_user : null isis_token : null ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - openjdk11 - rsync - nssm worker_pip_packages : - psutil - parea - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 - nssm qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : /usr/share/budgie-desktop/SimpleUAM/data/craidl_examples stub_server : cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/corpus_stub_cache server_dir : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub_server graphml_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub.graphml host : localhost port : 8182 read_only : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : /home/rkr/.cache/SimpleUAM/log/craidl_stub_db/stdout.log stderr_file : /home/rkr/.cache/SimpleUAM/log/craidl_stub_db/stderr.log rotate_io : true auto_start : false console : true interactive : false server_host : localhost server_port : 8182 static_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_static_dump.json static_corpus_cache : /usr/share/budgie-desktop/SimpleUAM/cache/static_corpus_cache use_static_corpus : true ### corpus.conf.yaml ### trinity : repo : https://git.isis.vanderbilt.edu/SwRI/ta1/sri-ta1/trinity-craidl.git branch : main examples_dir : examples graphml_corpus : repo : https://git.isis.vanderbilt.edu/SwRI/athens-uav-workflows.git branch : uam_corpus graphml_file : ExportedGraphML/all_schema_uam.graphml creopyson : repo : https://git.isis.vanderbilt.edu/SwRI/creoson/creopyson.git branch : main creoson_server : api : https://git.isis.vanderbilt.edu/api/v4/projects/499/jobs/3827/artifacts/out/CreosonServerWithSetup-2.8.0-win64.zip manual : https://git.isis.vanderbilt.edu/SwRI/creoson/creoson-server/-/jobs/artifacts/main/raw/out/CreosonServerWithSetup-2.8.0-win64.zip?job=build-job direct2cad : repo : https://git.isis.vanderbilt.edu/SwRI/uam_direct2cad.git branch : main ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json workspaces_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/d2c_workspaces max_workspaces : 1 exclude : - .git result_exclude : - .git - workingdir/*.prt ### broker.conf.yaml ### protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : amqp://127.0.0.1:5672 backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : redis://127.0.0.1:6379/0 ### d2c_worker.conf.yaml ### max_processes : 1 max_threads : 1 shutdown_timeout : 600000 skip_logging : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : /home/rkr/.cache/SimpleUAM/log/d2c_worker/stdout.log stderr_file : /home/rkr/.cache/SimpleUAM/log/d2c_worker/stderr.log rotate_io : true auto_start : false console : true interactive : false Generating Stub Config Files \u00b6 You can also use the write subcommand to write sample config files out to the appropriate locations. Run the following for more info: pdm run suam-config write --help Installing Config Files \u00b6 The install subcommand will symlink or copy config files from another location into the configuration directory for you. This is useful if you want to share config files between worker nodes or rapidly update a deployment. Run the following for more info: pdm run suam-config install --help Config File Format \u00b6 Config files can be partial and do not need to define every possible key. Keys that are missing will just use their default values. Overriding Configuration Fields. Consider the following defaults for example.conf.yaml : ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'default' field-1 : 'default' field-2 : 'default' With the following example.conf.yaml actually on disk: ### example.conf.yaml defaults ### subsection : subfield-2 : 'modified' field-1 : 'modifed' The final loaded values for example.conf.yaml as seen by the application would be: ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'modified' field-1 : 'modified' field-2 : 'default' When describing keys in a config file, we'll use dot notation. Config File Dot Notation Consider the following config file: subsection : subfield-1 : 'sub-1' subfield-2 : 'sub-2' field-1 : 'fld-1' field-2 : 'fld-2' Then field-1 would have value 'fld-1' and subsection.subfield-1 would have value 'sub-1' Likewise, setting foo to 3 and bar.buzz to 4 would leave you with the following file: foo : 3 bar : buzz : 4 Further details are here ... Placeholder Conventions \u00b6 Throughout these install instructions, but especially in the AWS setup, we use placeholders like <this-one> to represent values that will be useful later or which you, the user, should provide. In either case, this information should be saved somewhere for future reference. This guide tries to proactive about asking you to save potentially useful information. We recommend keeping a file open for this. Example placeholder file from partway through AWS setup. aws-vpc : prefix : example name : example-vpc id : vpc-XXXXXXXXXXXXXXXXX aws-public-subnet : name : example-public1-XX-XXXX-XX id : subnet-XXXXXXXXXXXXXXXXX aws-private-subnet : name : example-private1-XX-XXXX-XX id : subnet-XXXXXXXXXXXXXXXXX group : name : example-private-group aws-elastic-ip : name : example-eip-XX-XXXX-XX addr : 0.0.0.0 id : eipassoc-XXXXXXXXXXXXXXXXX We never use this information programmatically, so use whatever format you want, but it does make it easier to keep track of what you're doing during install. This is particularly important if you are setting up multiple machines and don't want to waste time. Installation and Setup \u00b6 Setting up a SimpleUAM requires choosing how components are distributed between the different machines in a deployment. SimpleUAM Deployment Components SimpleUAM Component Structure Client Nodes : These make requests to analyze designs and retrieve analysis results once they finish. Client nodes will usually be running some optimization or search process. Message Brokers : These gather analysis requests from client nodes and distribute them to free worker nodes. Worker Nodes : These will perform analyses on designs and store the results somewhere accessible to the clients. License Management : Each worker node needs Creo to perform analysis, and a license for Creo must be made available somehow. License Server : These can provide licenses to a number of worker nodes at once if provided with a floating license. Node-Locked Creo License : This is a worker-specific, static license file that can be placed on a worker. Component Corpus : A corpus of component information that every worker needs in order to analyze designs. Corpus DB : A graph database that the worker can use to look up component information. Static Corpus : A static file containing a dump of the component corpus which is placed on each worker node. Results Storage : A file system, accessible to both worker nodes and clients, where analysis results (in individual zip files) are placed. Results Backends : These notify client nodes when their analysis requests are completed and where the output is in Results Storage. The assignment between components and machines must follow the following rules. Deployment Topology Rules There must be one, and only one, message broker. The broker must be accessible over the network to all worker and client nodes. There needs to be at least one configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to a component corpus, either through an initialized corpus DB or a static corpus file. There must be a results storage accessible to all the worker and client nodes. The results storage should be a directory where workers can place files which are then made accessible to all the clients. Possible Storage Mechanisms Generally the results store will be a mounted network file share or local folder, but could also be: A network file system mounted on workers and clients. (Default) A local folder, if both worker and client are on the same machine. rsync + cron Dropbox Google Drive S3 Drive Anything that looks like a filesystem directory to both the worker and the client. In order for clients to receive analysis completion notifications there must be a single, unique results backend. This backend must be accessible over the network to all worker nodes and any client that wants to receive notifications. A results backend is optional and simply polling the results storage is perfectly viable. Then follow the steps for network setup once, and the machine setup instructions for each machine in the deployment. Network Setup \u00b6 Once a deployment topology has been chosen, first set up the network so that: All workers and clients can communicate with the message broker and results backend. If using a license server, ensure it can communicate with all the workers. If using a global, shared corpus DB ensure it can communicate with all the workers. If you are using AWS you can start with our instructions for setting up a virtual private cloud (VPC). It sets up a private subnet for non-client machines and a VPN and Network drive for access to that private subnet. AWS (Network) Setup Machine Setup \u00b6 Installation for each machine requires following the other pages in this section in order, skipping any that aren't relevant and always including general setup. Try to setup machines with centralized functions, like the license server and message broker, before the worker nodes. AWS (Instance) Setup General Setup (Required) Creo License Server Message Broker & Results Backend Corpus DB Static Corpus Worker Node Client nodes are less directly dependent on the SimpleUAM infrastructure and their setup can skip directly to the corresponding section: Client Node Example Installation Orders \u00b6 Here are some worked examples of how to proceed through the setup process for our example deployment topologies: Example Local Development Deployment Development SimpleUAM System This deployment places a single worker node in a VM on a host machine. Initial setup would begin with: Creating a Windows VM for the worker. Ensuring that the VM can access the internet. Ensuring the host machine has a route to the VM at some IP address. Ensuring that a folder in the VM is mounted or mirrored on the host machine. Setup for the Worker VM would then go through the instructions for the relevant components: General Setup (Required) Message Broker & Results Backend Static Corpus Worker Node Finally, the host machine would go through the client instructions at: Client Node Example Production Deployment Production SimpleUAM System This deployment spreads work over multiple worker nodes within a subnet. Initial setup on AWS would follow AWS (Network) Setup , otherwise it would begin with: Creating the license server along with the broker, results, and worker nodes. Creating the results storage and ensuring it's mounted on the worker nodes. Ensure that the workers have a route to the license server and the message broker. The license server would go through the following, adding the AWS step if needed: AWS (Instance) Setup General Setup (Required) Creo License Server The message broker would likewise go through: AWS (Instance) Setup General Setup (Required) Message Broker & Results Backend Similarly, each worker node would go through: AWS (Instance) Setup General Setup (Required) Static Corpus Worker Node A clone of a working worker should be fully functional out of the box. Finally, each client would go through the client instructions at: Client Node","title":"Introduction"},{"location":"setup/intro/#simpleuam-installation-guide","text":"Info We would appreciate assistance in making this guide better. Any notes on issues with the install process, lack of clarity in wording, or other improvements would be appreciated. SimpleUAM Component Structure The core goal of SimpleUAM is allow users to set up a service for processing requests to analyze UAM and UAV designs. Client nodes, such as optimizers or search tools, should be able to queue requests for distribution to worker nodes as they become available. The results of those analyses, packaged as zip files, should then be made available to the clients as they're completed. The key components of a SimpleUAM deployment are: Client Nodes : These make requests to analyze designs and retrieve analysis results once they finish. Client nodes will usually be running some optimization or search process. Message Brokers : These gather analysis requests from client nodes and distribute them to free worker nodes. Worker Nodes : These will perform analyses on designs and store the results somewhere accessible to the clients. License Management : Each worker node needs Creo to perform analysis, and a license for Creo must be made available somehow. License Server : These can provide licenses to a number of worker nodes at once if provided with a floating license. Node-Locked Creo License : This is a worker-specific, static license file that can be placed on a worker. Component Corpus : A corpus of component information that every worker needs in order to analyze designs. Corpus DB : A graph database that the worker can use to look up component information. Static Corpus : A static file containing a dump of the component corpus which is placed on each worker node. Results Storage : A file system, accessible to both worker nodes and clients, where analysis results (in individual zip files) are placed. Results Backends : These notify client nodes when their analysis requests are completed and where the output is in Results Storage. In order to form a complete SimpleUAM deployment some core requirements need to be met: There must be one, and only one, message broker. The broker must be accessible over the network to all worker and client nodes. There needs to be at least one configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to a component corpus, either through an initialized corpus DB or a static corpus file. There must be a results storage accessible to all the worker and client nodes. The results storage should be a directory where workers can place files which are then made accessible to all the clients. Possible Storage Mechanisms Generally the results store will be a mounted network file share or local folder, but could also be: A network file system mounted on workers and clients. (Default) A local folder, if both worker and client are on the same machine. rsync + cron Dropbox Google Drive S3 Drive Anything that looks like a filesystem directory to both the worker and the client. In order for clients to receive analysis completion notifications there must be a single, unique results backend. This backend must be accessible over the network to all worker nodes and any client that wants to receive notifications. A results backend is optional and simply polling the results storage is perfectly viable. With a SimpleUAM deployment meeting those requirements, a client nodes can offload analysis jobs to a pool of workers though simple python and command line interfaces.","title":"SimpleUAM Installation Guide"},{"location":"setup/intro/#topology","text":"It's possible to distribute SimpleUAM components between multiple machines in numerous ways that meet the given requirements. Picking a topology, specifically the components that go on each individual machine, tells you which installation steps are needed for that machine. We'll look at two example topologies, one I use for (semi)local development work and one for a potential production system. Development SimpleUAM System This development setup has a local machine and a single worker. The local machine is set up so it can run a SimpleUAM client and so that any code shared with a worker node can be edited in whatever manner the user is comfortable with. The worker node, running in a VM, then has all the other necessary components of the service, including broker, license, and corpus. The structure is a broad guideline and can be tweaked as needed. For instance, if you're running windows you can just run all the components on your local machine and use a stub message broker that will run analysis requests as blocking calls. Alternately, the worker node can be running on a server somewhere with a NFS shared drive acting as the shared folder. Production SimpleUAM System The production service has significantly more going on. There are one or more clients producing analysis requests, multiple workers processing them, a Creo license server, a message broker, a results backend, and results storage. This setup can scale relatively easily while providing features like completion notifications. In fact this is the intended topology for the AWS instructions, with clients either accessing the other components via a VPN or simply running directly on the cloud. Other topologies are also viable, for instance running a central graph database for all the workers to share instead of relying on a local, static corpus. The most important part of choosing a service topology is knowing what component(s) are going to be running on each individual server or VM. Given that one can just go through the instructions for each component on a machine in sequence, repeating that process for each machine in the deployment.","title":"Choosing a Service Topology"},{"location":"setup/intro/#cli","text":"All the command line scripts SimpleUAM provides are made using Invoke and evaluated within a PDM administered python environment. This means that all the SimpleUAM provided commands must be run from <repo-root> and have this format: pdm run <command> All the core SimpleUAM commands suam-config , setup-win , craidl , d2c-workspace , and d2c-client will print a help message when run without arguments. In their base form these commands are safe and will never make change to your system. The help messages also provide a list of subcommands that do perform various tasks. These subcommands are run with: pdm run <command> <sub-command> [ ARGS ] All of these subcommands come with detailed help information that can be accessed with: pdm run <command> <sub-command> --help These help messages are worth checking for available options and notes.","title":"Command Line Interfaces"},{"location":"setup/intro/#config","text":"SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. While you can find a more detailed breakdown of the system here , this is a quick overview.","title":"Configuration"},{"location":"setup/intro/#config-dir","text":"Once the SimpleUAM project is installed (in General Setup ) you can run the following command to find the config file directory: pdm run suam-config dir Files placed there will be loaded when most SimpleUAM code is started up. The configuration is immutable for the runtime of a program and changes will require a restart to register.","title":"Configuration File Directory"},{"location":"setup/intro/#config-state","text":"You can get a printout of the current configuration state with the following: pdm run suam-config print --all Sample Output of pdm run suam-config print --all ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### auth.conf.yaml ### isis_user : null isis_token : null ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - openjdk11 - rsync - nssm worker_pip_packages : - psutil - parea - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 - nssm qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : ${path:data_directory}/craidl_examples stub_server : cache_dir : ${path:cache_directory}/corpus_stub_cache server_dir : ${path:data_directory}/corpus_stub_server graphml_corpus : ${path:data_directory}/corpus_stub.graphml host : localhost port : 8182 read_only : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/craidl_stub_db/stdout.log stderr_file : ${path:log_directory}/craidl_stub_db/stderr.log rotate_io : true auto_start : false console : true interactive : false server_host : localhost server_port : ${stub_server.port} static_corpus : ${path:data_directory}/corpus_static_dump.json static_corpus_cache : ${path:cache_directory}/static_corpus_cache use_static_corpus : true ### corpus.conf.yaml ### trinity : repo : https://git.isis.vanderbilt.edu/SwRI/ta1/sri-ta1/trinity-craidl.git branch : main examples_dir : examples graphml_corpus : repo : https://git.isis.vanderbilt.edu/SwRI/athens-uav-workflows.git branch : uam_corpus graphml_file : ExportedGraphML/all_schema_uam.graphml creopyson : repo : https://git.isis.vanderbilt.edu/SwRI/creoson/creopyson.git branch : main creoson_server : api : https://git.isis.vanderbilt.edu/api/v4/projects/499/jobs/3827/artifacts/out/CreosonServerWithSetup-2.8.0-win64.zip manual : https://git.isis.vanderbilt.edu/SwRI/creoson/creoson-server/-/jobs/artifacts/main/raw/out/CreosonServerWithSetup-2.8.0-win64.zip?job=build-job direct2cad : repo : https://git.isis.vanderbilt.edu/SwRI/uam_direct2cad.git branch : main ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : ${workspaces_dir}/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json workspaces_dir : ${path:work_directory}/d2c_workspaces cache_dir : ${path:cache_directory}/d2c_workspaces max_workspaces : 1 exclude : - .git result_exclude : - .git - workingdir/*.prt ### broker.conf.yaml ### protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : ${.protocol}://${.host}:${.port}${.db} backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : ${.protocol}://${.host}:${.port}/${.db} ### d2c_worker.conf.yaml ### max_processes : ${d2c_workspace:max_workspaces} max_threads : 1 shutdown_timeout : 600000 skip_logging : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/d2c_worker/stdout.log stderr_file : ${path:log_directory}/d2c_worker/stderr.log rotate_io : true auto_start : false console : true interactive : false If you want to see the full expanded version of the configs, with all the interpolations resolved, add the --resolved flag. Sample Output of pdm run suam-config print --all --resolved ### paths.conf.yaml ### config_directory : /etc/xdg/xdg-budgie-desktop/SimpleUAM/config cache_directory : /usr/share/budgie-desktop/SimpleUAM/cache log_directory : /home/rkr/.cache/SimpleUAM/log work_directory : /usr/share/budgie-desktop/SimpleUAM data_directory : /usr/share/budgie-desktop/SimpleUAM/data ### auth.conf.yaml ### isis_user : null isis_token : null ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - openjdk11 - rsync - nssm worker_pip_packages : - psutil - parea - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 - nssm qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad ### craidl.conf.yaml ### example_dir : /usr/share/budgie-desktop/SimpleUAM/data/craidl_examples stub_server : cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/corpus_stub_cache server_dir : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub_server graphml_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_stub.graphml host : localhost port : 8182 read_only : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : /home/rkr/.cache/SimpleUAM/log/craidl_stub_db/stdout.log stderr_file : /home/rkr/.cache/SimpleUAM/log/craidl_stub_db/stderr.log rotate_io : true auto_start : false console : true interactive : false server_host : localhost server_port : 8182 static_corpus : /usr/share/budgie-desktop/SimpleUAM/data/corpus_static_dump.json static_corpus_cache : /usr/share/budgie-desktop/SimpleUAM/cache/static_corpus_cache use_static_corpus : true ### corpus.conf.yaml ### trinity : repo : https://git.isis.vanderbilt.edu/SwRI/ta1/sri-ta1/trinity-craidl.git branch : main examples_dir : examples graphml_corpus : repo : https://git.isis.vanderbilt.edu/SwRI/athens-uav-workflows.git branch : uam_corpus graphml_file : ExportedGraphML/all_schema_uam.graphml creopyson : repo : https://git.isis.vanderbilt.edu/SwRI/creoson/creopyson.git branch : main creoson_server : api : https://git.isis.vanderbilt.edu/api/v4/projects/499/jobs/3827/artifacts/out/CreosonServerWithSetup-2.8.0-win64.zip manual : https://git.isis.vanderbilt.edu/SwRI/creoson/creoson-server/-/jobs/artifacts/main/raw/out/CreosonServerWithSetup-2.8.0-win64.zip?job=build-job direct2cad : repo : https://git.isis.vanderbilt.edu/SwRI/uam_direct2cad.git branch : main ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json workspaces_dir : /usr/share/budgie-desktop/SimpleUAM/d2c_workspaces cache_dir : /usr/share/budgie-desktop/SimpleUAM/cache/d2c_workspaces max_workspaces : 1 exclude : - .git result_exclude : - .git - workingdir/*.prt ### broker.conf.yaml ### protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : amqp://127.0.0.1:5672 backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : '0' url : redis://127.0.0.1:6379/0 ### d2c_worker.conf.yaml ### max_processes : 1 max_threads : 1 shutdown_timeout : 600000 skip_logging : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : /home/rkr/.cache/SimpleUAM/log/d2c_worker/stdout.log stderr_file : /home/rkr/.cache/SimpleUAM/log/d2c_worker/stderr.log rotate_io : true auto_start : false console : true interactive : false","title":"Configuration State"},{"location":"setup/intro/#config-write","text":"You can also use the write subcommand to write sample config files out to the appropriate locations. Run the following for more info: pdm run suam-config write --help","title":"Generating Stub Config Files"},{"location":"setup/intro/#config-install","text":"The install subcommand will symlink or copy config files from another location into the configuration directory for you. This is useful if you want to share config files between worker nodes or rapidly update a deployment. Run the following for more info: pdm run suam-config install --help","title":"Installing Config Files"},{"location":"setup/intro/#config-format","text":"Config files can be partial and do not need to define every possible key. Keys that are missing will just use their default values. Overriding Configuration Fields. Consider the following defaults for example.conf.yaml : ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'default' field-1 : 'default' field-2 : 'default' With the following example.conf.yaml actually on disk: ### example.conf.yaml defaults ### subsection : subfield-2 : 'modified' field-1 : 'modifed' The final loaded values for example.conf.yaml as seen by the application would be: ### example.conf.yaml defaults ### subsection : subfield-1 : 'default' subfield-2 : 'modified' field-1 : 'modified' field-2 : 'default' When describing keys in a config file, we'll use dot notation. Config File Dot Notation Consider the following config file: subsection : subfield-1 : 'sub-1' subfield-2 : 'sub-2' field-1 : 'fld-1' field-2 : 'fld-2' Then field-1 would have value 'fld-1' and subsection.subfield-1 would have value 'sub-1' Likewise, setting foo to 3 and bar.buzz to 4 would leave you with the following file: foo : 3 bar : buzz : 4 Further details are here ...","title":"Config File Format"},{"location":"setup/intro/#placeholder","text":"Throughout these install instructions, but especially in the AWS setup, we use placeholders like <this-one> to represent values that will be useful later or which you, the user, should provide. In either case, this information should be saved somewhere for future reference. This guide tries to proactive about asking you to save potentially useful information. We recommend keeping a file open for this. Example placeholder file from partway through AWS setup. aws-vpc : prefix : example name : example-vpc id : vpc-XXXXXXXXXXXXXXXXX aws-public-subnet : name : example-public1-XX-XXXX-XX id : subnet-XXXXXXXXXXXXXXXXX aws-private-subnet : name : example-private1-XX-XXXX-XX id : subnet-XXXXXXXXXXXXXXXXX group : name : example-private-group aws-elastic-ip : name : example-eip-XX-XXXX-XX addr : 0.0.0.0 id : eipassoc-XXXXXXXXXXXXXXXXX We never use this information programmatically, so use whatever format you want, but it does make it easier to keep track of what you're doing during install. This is particularly important if you are setting up multiple machines and don't want to waste time.","title":"Placeholder Conventions"},{"location":"setup/intro/#setup","text":"Setting up a SimpleUAM requires choosing how components are distributed between the different machines in a deployment. SimpleUAM Deployment Components SimpleUAM Component Structure Client Nodes : These make requests to analyze designs and retrieve analysis results once they finish. Client nodes will usually be running some optimization or search process. Message Brokers : These gather analysis requests from client nodes and distribute them to free worker nodes. Worker Nodes : These will perform analyses on designs and store the results somewhere accessible to the clients. License Management : Each worker node needs Creo to perform analysis, and a license for Creo must be made available somehow. License Server : These can provide licenses to a number of worker nodes at once if provided with a floating license. Node-Locked Creo License : This is a worker-specific, static license file that can be placed on a worker. Component Corpus : A corpus of component information that every worker needs in order to analyze designs. Corpus DB : A graph database that the worker can use to look up component information. Static Corpus : A static file containing a dump of the component corpus which is placed on each worker node. Results Storage : A file system, accessible to both worker nodes and clients, where analysis results (in individual zip files) are placed. Results Backends : These notify client nodes when their analysis requests are completed and where the output is in Results Storage. The assignment between components and machines must follow the following rules. Deployment Topology Rules There must be one, and only one, message broker. The broker must be accessible over the network to all worker and client nodes. There needs to be at least one configured, running worker node to run analyses. Each worker node needs to have a Creo license, either through a node-locked license or a connection to a Creo license server. Each worker node needs to have access to a component corpus, either through an initialized corpus DB or a static corpus file. There must be a results storage accessible to all the worker and client nodes. The results storage should be a directory where workers can place files which are then made accessible to all the clients. Possible Storage Mechanisms Generally the results store will be a mounted network file share or local folder, but could also be: A network file system mounted on workers and clients. (Default) A local folder, if both worker and client are on the same machine. rsync + cron Dropbox Google Drive S3 Drive Anything that looks like a filesystem directory to both the worker and the client. In order for clients to receive analysis completion notifications there must be a single, unique results backend. This backend must be accessible over the network to all worker nodes and any client that wants to receive notifications. A results backend is optional and simply polling the results storage is perfectly viable. Then follow the steps for network setup once, and the machine setup instructions for each machine in the deployment.","title":"Installation and Setup"},{"location":"setup/intro/#setup-net","text":"Once a deployment topology has been chosen, first set up the network so that: All workers and clients can communicate with the message broker and results backend. If using a license server, ensure it can communicate with all the workers. If using a global, shared corpus DB ensure it can communicate with all the workers. If you are using AWS you can start with our instructions for setting up a virtual private cloud (VPC). It sets up a private subnet for non-client machines and a VPN and Network drive for access to that private subnet. AWS (Network) Setup","title":"Network Setup"},{"location":"setup/intro/#setup-machine","text":"Installation for each machine requires following the other pages in this section in order, skipping any that aren't relevant and always including general setup. Try to setup machines with centralized functions, like the license server and message broker, before the worker nodes. AWS (Instance) Setup General Setup (Required) Creo License Server Message Broker & Results Backend Corpus DB Static Corpus Worker Node Client nodes are less directly dependent on the SimpleUAM infrastructure and their setup can skip directly to the corresponding section: Client Node","title":"Machine Setup"},{"location":"setup/intro/#setup-example","text":"Here are some worked examples of how to proceed through the setup process for our example deployment topologies: Example Local Development Deployment Development SimpleUAM System This deployment places a single worker node in a VM on a host machine. Initial setup would begin with: Creating a Windows VM for the worker. Ensuring that the VM can access the internet. Ensuring the host machine has a route to the VM at some IP address. Ensuring that a folder in the VM is mounted or mirrored on the host machine. Setup for the Worker VM would then go through the instructions for the relevant components: General Setup (Required) Message Broker & Results Backend Static Corpus Worker Node Finally, the host machine would go through the client instructions at: Client Node Example Production Deployment Production SimpleUAM System This deployment spreads work over multiple worker nodes within a subnet. Initial setup on AWS would follow AWS (Network) Setup , otherwise it would begin with: Creating the license server along with the broker, results, and worker nodes. Creating the results storage and ensuring it's mounted on the worker nodes. Ensure that the workers have a route to the license server and the message broker. The license server would go through the following, adding the AWS step if needed: AWS (Instance) Setup General Setup (Required) Creo License Server The message broker would likewise go through: AWS (Instance) Setup General Setup (Required) Message Broker & Results Backend Similarly, each worker node would go through: AWS (Instance) Setup General Setup (Required) Static Corpus Worker Node A clone of a working worker should be fully functional out of the box. Finally, each client would go through the client instructions at: Client Node","title":"Example Installation Orders"},{"location":"setup/license_server/","text":"Creo License Server Setup \u00b6 This sets up a floating Creo license that worker nodes can use. A machine or VM with direct access can use a node-locked Creo license instead. However if you need to access the VM through RDP a floating license is required. Prerequisites \u00b6 General Setup has been completed. The IP of this machine is saved as: <license-server.ip> Get License File \u00b6 Get the Host ID and License File. Open an admin powershell to <repo-root> . Get the Host ID / mac address for this machine: pdm run setup-win mac-address Save the result as <creo-license.host-id> . Get the Creo floating license for <creo-license.host-id> . Place the license file at <creo-license.file> , somewhere on this machine. Install Dependencies \u00b6 Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install necessary dependencies: pdm run setup-win install.license-deps Close this powershell instance and open a new one for future steps. Install Flexnet Server \u00b6 Flexnet server is one of the options for hosting the license server and has been reasonably easy to use. Open an admin powershell to <repo-root> . Download and run the installer: pdm run setup-win license-server.flexnet Wait for installer to download and hit enter when prompted. Follow GUI installer's prompts until done, including providing <creo-license.file> when asked. Open Required Ports \u00b6 Open the relevant ports up so instances of Creo on the worker nodes can connect. If you only intend to use Creo locally, on the same machine as the license server, then you don't need to open any ports up. Option 1: Open only port 7788. \u00b6 The instructions for this are too configuration specific for us to provide. Option 2: Disable license server firewalls entirely. \u00b6 We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Do not do this on public or untrusted machines Disable the Windows Server 2019 firewall: pdm run setup-win disable-firewall Note: This might work with other versions of Windows but that hasn't been tested. Configure and Start Server \u00b6 Configure the server with a new admin password and start it. Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the administration page: Default User: admin Default Pass: admin Change the password: Old Pass: admin New Pass: <creo-license.flexnet-password> Start the server: Start Menu -> PTC -> Start FLEXNet Admin Service The server should now automatically start on boot. Preserve Settings \u00b6 Keep track of connection information Save the IP of this server as: <license-server.ip> Save the server's port (usually 7788) as: <license-server.port>","title":"License Server"},{"location":"setup/license_server/#creo-license-server-setup","text":"This sets up a floating Creo license that worker nodes can use. A machine or VM with direct access can use a node-locked Creo license instead. However if you need to access the VM through RDP a floating license is required.","title":"Creo License Server Setup"},{"location":"setup/license_server/#prereqs","text":"General Setup has been completed. The IP of this machine is saved as: <license-server.ip>","title":"Prerequisites"},{"location":"setup/license_server/#license","text":"Get the Host ID and License File. Open an admin powershell to <repo-root> . Get the Host ID / mac address for this machine: pdm run setup-win mac-address Save the result as <creo-license.host-id> . Get the Creo floating license for <creo-license.host-id> . Place the license file at <creo-license.file> , somewhere on this machine.","title":"Get License File"},{"location":"setup/license_server/#deps","text":"Install utilities like wget and rsync. Open an admin powershell to <repo-root> . Install necessary dependencies: pdm run setup-win install.license-deps Close this powershell instance and open a new one for future steps.","title":"Install Dependencies"},{"location":"setup/license_server/#flexnet","text":"Flexnet server is one of the options for hosting the license server and has been reasonably easy to use. Open an admin powershell to <repo-root> . Download and run the installer: pdm run setup-win license-server.flexnet Wait for installer to download and hit enter when prompted. Follow GUI installer's prompts until done, including providing <creo-license.file> when asked.","title":"Install Flexnet Server"},{"location":"setup/license_server/#ports","text":"Open the relevant ports up so instances of Creo on the worker nodes can connect. If you only intend to use Creo locally, on the same machine as the license server, then you don't need to open any ports up.","title":"Open Required Ports"},{"location":"setup/license_server/#ports-single","text":"The instructions for this are too configuration specific for us to provide.","title":"Option 1: Open only port 7788."},{"location":"setup/license_server/#ports-clobber","text":"We can't provide general instructions for this step but if you're using Windows Server 2019 you can use one of our scripts. Do not do this on public or untrusted machines Disable the Windows Server 2019 firewall: pdm run setup-win disable-firewall Note: This might work with other versions of Windows but that hasn't been tested.","title":"Option 2: Disable license server firewalls entirely."},{"location":"setup/license_server/#configure","text":"Configure the server with a new admin password and start it. Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the administration page: Default User: admin Default Pass: admin Change the password: Old Pass: admin New Pass: <creo-license.flexnet-password> Start the server: Start Menu -> PTC -> Start FLEXNet Admin Service The server should now automatically start on boot.","title":"Configure and Start Server"},{"location":"setup/license_server/#settings","text":"Keep track of connection information Save the IP of this server as: <license-server.ip> Save the server's port (usually 7788) as: <license-server.port>","title":"Preserve Settings"},{"location":"setup/worker/","text":"Worker Node Setup \u00b6 A worker can analyze designs on behalf of clients and requires access to a license server and a broker for most tasks. Prerequisites \u00b6 General Setup has been completed. Auth tokens, SSH keys, or credentials for git.isis.vanderbilt.edu A broker running at <broker> . Access to a corpus: Either: Via a corpus DB at <corpus-db> . Or: A static corpus installed as described here . Have a results directory set up at <results-dir> . Ensure that it's readable and writable by the worker. Optionally have a backend running at <backend> . Optionally have design_swri.json files available for testing. Try using the example store as described here if no other source is available. Install Dependencies \u00b6 Install utilities like wget and rsync, as well as global python packages needed by creopyson and direct2cad. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.worker-deps Close powershell and open a new instance for future commands. Get License Information \u00b6 Option 1: License Server \u00b6 Have a license server running at <license-server.ip> on port <license-server.port> . Option 2: Static Creo License \u00b6 Open an admin powershell to <repo-root> . Get your mac address (for generating licenses): pdm run setup-win mac-address Save the result as <creo-license.host-id> . Get a node-locked license for <creo-license.host-id> . If using a local license, have the license file for the above mac address downloaded to <creo-license.file> . Install Creo \u00b6 Downloads and installs PTC Creo 5.6. Open an admin powershell to <repo-root> . Download and run installer: pdm run setup-win worker.creo Read instructions in terminal and hit enter when ready. Follow installer prompts until done. Fix some minor usability issues. If on Windows Server 2019 you can disable the IE Enhanced Security popups that open whenever Creo starts. pdm run setup-win worker.disable-ieesc Install Creopyson \u00b6 Creopyson provides a python interface to Creo. Prepare to connect to git.isis.vanderbilt.edu . Either: Install auth tokens as in General Setup . Or: Install SSH keys for git.isis.vanderbilt.edu . Or: have credentials ready for prompt. Open an admin powershell to <repo-root> . Download creopyson repository and install via pip: pdm run setup-win worker.creopyson Follow prompts. Configure Corpus Settings \u00b6 Performing various analysis tasks requires access to either a Corpus DB or a local static corpus. The config file at <config-dir>/craidl.conf.yaml specifies which of these use and how the connection is configured. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r Option 1: Using a static corpus. \u00b6 No changes should be needed to config files. Option 2: Using a remote corpus DB. \u00b6 Create or update <config-dir>/craidl.md : Set use_static_corpus to False . Set server_host to <corpus-db,host . Set server_port to <corpus-db.port> . See the config file guide for more detailed instructions and information. Further details on using a static or database corpus can be found in this section ... Configure Results Dir \u00b6 The results directory needs to be configured to point at the appropriate directory. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_workspace -r Create or update the config at <config-dir>/d2c_workspace.conf.yaml : Set results_dir to <results-dir> . See the config file guide for more detailed instructions and information. Further details on results storage and local worker node operations are in this page ... Initialize Reference Workspace \u00b6 Each worker needs a pristine copy of the workspace in which designs can be analyzed. This reference workspace will be copied, via rsync for efficiency, whenever the worker performs a new analysis. The results archives are also constructed by gathering all the files which have been changed after an analysis when compared to the reference directory. Open admin powershell at <repo-root> . Set up the reference workspace: pdm run d2c-workspace setup.reference-workspace Follow prompts. For further details on workspace configuration, operation, and running analyses locally see this page ... Test the Worker Node (Optional) \u00b6 Run a simple test task, generating the info files for a design, in order to test the worker node's configuration. Have a valid design file (usually design_swri.json ) at <design-file> . Open a shell to <repo-root> . Test generating design info files: pdm run d2c-workspace tasks.gen-info-files --input = <design-file> When finished this should place an archive in the <results-dir> with the generated info files. Test generating processing designs: pdm run d2c-workspace tasks.process-design --input = <design-file> When finished this should place an archive in the <results-dir> with the processed design. Configure Broker Settings \u00b6 The worker process itself needs to be configured with how to connect to a message broker. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = broker -r Create or update the config at <config-dir>/broker.conf.yaml : Set up the broker: If you have a <broker.url> : Set url to <broker.url> . Otherwise: Set host to <broker.ip> . Set protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set port to <broker.port> . Set db to <broker.db> if you have one. Set up the backend, if available: Set backend.enabled to true If you have a <backend.url> : Set broker.url to <broker.url> Otherwise: Set backend.host to <backend.ip> . Set backend.port to <broker.port> . Set backend.db to <broker.db> if you have one. See the config file guide for more detailed instructions and information. Further details on configuring the worker's backend are here ... Run the Worker \u00b6 In order for the worker node to be able to process tasks a worker must be running, either as a process or a service. Option 1 : Run the Worker Node as a Process \u00b6 You can run the worker node as a process to verify all the above settings function as intended. Open admin powershell to <repo-root> . Run the SimpleUAM worker node process: pdm run suam-worker run If the worker node process is running and the broker is correctly configured then the worker is also a valid client. This means that, if there is only one attached worker, you can run a basic round trip test of the system using the instructions for testing the client node . Option 2 : Run the Worker Node as a Service \u00b6 Warning Running SimpleUAM's worker as a service requires ensuring file system permissions and mounted drives are available with headless accounts. This is out of scope of scope of this guide and generally rather annoying. Contributions or suggestions to simplify this process would be very appreciated. Configure the Worker Service \u00b6 The worker node service needs to configured, in particular a user should be provided that can see the mounted drive. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Create or modify the config at <config-dir>/d2c_worker.conf.yaml : Set service.account to the user you want the service to run as. The default should be fine if the LocalSystem user can read and write to your results directory and the workspaces directory. Set service.password to the password of that user. (Optional) Make the service auto start by setting service.auto_start to True See the config file guide for more detailed instructions and information. Other Configuration Notes Because services don't run as a frontend user paths to network drives can't use drive letters. Make sure all paths in all config files are in UNC format where applicable. Further details on configuring the worker as a service are here ... Set up the Worker Node Service \u00b6 We use Non-Sucking Service Manager to manage the worker node service, and that requires some setup. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run suam-worker service.install Start the Worker Node Service \u00b6 The worker node service can start immediately but we recommend holding off until you've verified configurations while running the node as a process. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run suam-worker service.start Further details on running the worker as a service are here ... Next Steps \u00b6 View information on corpus use here ... View information on local task processing here ... View information on remote task processing here ...","title":"Worker Node"},{"location":"setup/worker/#worker-node-setup","text":"A worker can analyze designs on behalf of clients and requires access to a license server and a broker for most tasks.","title":"Worker Node Setup"},{"location":"setup/worker/#prereqs","text":"General Setup has been completed. Auth tokens, SSH keys, or credentials for git.isis.vanderbilt.edu A broker running at <broker> . Access to a corpus: Either: Via a corpus DB at <corpus-db> . Or: A static corpus installed as described here . Have a results directory set up at <results-dir> . Ensure that it's readable and writable by the worker. Optionally have a backend running at <backend> . Optionally have design_swri.json files available for testing. Try using the example store as described here if no other source is available.","title":"Prerequisites"},{"location":"setup/worker/#deps","text":"Install utilities like wget and rsync, as well as global python packages needed by creopyson and direct2cad. Open an admin powershell to <repo-root> . Install dependencies: pdm run setup-win install.worker-deps Close powershell and open a new instance for future commands.","title":"Install Dependencies"},{"location":"setup/worker/#license","text":"","title":"Get License Information"},{"location":"setup/worker/#license-server","text":"Have a license server running at <license-server.ip> on port <license-server.port> .","title":"Option 1: License Server"},{"location":"setup/worker/#license-static","text":"Open an admin powershell to <repo-root> . Get your mac address (for generating licenses): pdm run setup-win mac-address Save the result as <creo-license.host-id> . Get a node-locked license for <creo-license.host-id> . If using a local license, have the license file for the above mac address downloaded to <creo-license.file> .","title":"Option 2: Static Creo License"},{"location":"setup/worker/#creo","text":"Downloads and installs PTC Creo 5.6. Open an admin powershell to <repo-root> . Download and run installer: pdm run setup-win worker.creo Read instructions in terminal and hit enter when ready. Follow installer prompts until done. Fix some minor usability issues. If on Windows Server 2019 you can disable the IE Enhanced Security popups that open whenever Creo starts. pdm run setup-win worker.disable-ieesc","title":"Install Creo"},{"location":"setup/worker/#creopyson","text":"Creopyson provides a python interface to Creo. Prepare to connect to git.isis.vanderbilt.edu . Either: Install auth tokens as in General Setup . Or: Install SSH keys for git.isis.vanderbilt.edu . Or: have credentials ready for prompt. Open an admin powershell to <repo-root> . Download creopyson repository and install via pip: pdm run setup-win worker.creopyson Follow prompts.","title":"Install Creopyson"},{"location":"setup/worker/#corpus","text":"Performing various analysis tasks requires access to either a Corpus DB or a local static corpus. The config file at <config-dir>/craidl.conf.yaml specifies which of these use and how the connection is configured. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = craidl -r","title":"Configure Corpus Settings"},{"location":"setup/worker/#corpus-static","text":"No changes should be needed to config files.","title":"Option 1: Using a static corpus."},{"location":"setup/worker/#corpus-db","text":"Create or update <config-dir>/craidl.md : Set use_static_corpus to False . Set server_host to <corpus-db,host . Set server_port to <corpus-db.port> . See the config file guide for more detailed instructions and information. Further details on using a static or database corpus can be found in this section ...","title":"Option 2: Using a remote corpus DB."},{"location":"setup/worker/#results","text":"The results directory needs to be configured to point at the appropriate directory. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_workspace -r Create or update the config at <config-dir>/d2c_workspace.conf.yaml : Set results_dir to <results-dir> . See the config file guide for more detailed instructions and information. Further details on results storage and local worker node operations are in this page ...","title":"Configure Results Dir"},{"location":"setup/worker/#reference","text":"Each worker needs a pristine copy of the workspace in which designs can be analyzed. This reference workspace will be copied, via rsync for efficiency, whenever the worker performs a new analysis. The results archives are also constructed by gathering all the files which have been changed after an analysis when compared to the reference directory. Open admin powershell at <repo-root> . Set up the reference workspace: pdm run d2c-workspace setup.reference-workspace Follow prompts. For further details on workspace configuration, operation, and running analyses locally see this page ...","title":"Initialize Reference Workspace"},{"location":"setup/worker/#test","text":"Run a simple test task, generating the info files for a design, in order to test the worker node's configuration. Have a valid design file (usually design_swri.json ) at <design-file> . Open a shell to <repo-root> . Test generating design info files: pdm run d2c-workspace tasks.gen-info-files --input = <design-file> When finished this should place an archive in the <results-dir> with the generated info files. Test generating processing designs: pdm run d2c-workspace tasks.process-design --input = <design-file> When finished this should place an archive in the <results-dir> with the processed design.","title":"Test the Worker Node (Optional)"},{"location":"setup/worker/#broker","text":"The worker process itself needs to be configured with how to connect to a message broker. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = broker -r Create or update the config at <config-dir>/broker.conf.yaml : Set up the broker: If you have a <broker.url> : Set url to <broker.url> . Otherwise: Set host to <broker.ip> . Set protocol to \"amqp\" if using RabbitMQ or \"redis\" if using Redis. Set port to <broker.port> . Set db to <broker.db> if you have one. Set up the backend, if available: Set backend.enabled to true If you have a <backend.url> : Set broker.url to <broker.url> Otherwise: Set backend.host to <backend.ip> . Set backend.port to <broker.port> . Set backend.db to <broker.db> if you have one. See the config file guide for more detailed instructions and information. Further details on configuring the worker's backend are here ...","title":"Configure Broker Settings"},{"location":"setup/worker/#worker","text":"In order for the worker node to be able to process tasks a worker must be running, either as a process or a service.","title":"Run the Worker"},{"location":"setup/worker/#run","text":"You can run the worker node as a process to verify all the above settings function as intended. Open admin powershell to <repo-root> . Run the SimpleUAM worker node process: pdm run suam-worker run If the worker node process is running and the broker is correctly configured then the worker is also a valid client. This means that, if there is only one attached worker, you can run a basic round trip test of the system using the instructions for testing the client node .","title":"Option 1: Run the Worker Node as a Process"},{"location":"setup/worker/#service","text":"Warning Running SimpleUAM's worker as a service requires ensuring file system permissions and mounted drives are available with headless accounts. This is out of scope of scope of this guide and generally rather annoying. Contributions or suggestions to simplify this process would be very appreciated.","title":"Option 2: Run the Worker Node as a Service"},{"location":"setup/worker/#service-config","text":"The worker node service needs to configured, in particular a user should be provided that can see the mounted drive. Open admin powershell to <repo-root> . (Optional) View currently loaded config file: pdm run suam-config print --config = d2c_worker -r Create or modify the config at <config-dir>/d2c_worker.conf.yaml : Set service.account to the user you want the service to run as. The default should be fine if the LocalSystem user can read and write to your results directory and the workspaces directory. Set service.password to the password of that user. (Optional) Make the service auto start by setting service.auto_start to True See the config file guide for more detailed instructions and information. Other Configuration Notes Because services don't run as a frontend user paths to network drives can't use drive letters. Make sure all paths in all config files are in UNC format where applicable. Further details on configuring the worker as a service are here ...","title":"Configure the Worker Service"},{"location":"setup/worker/#service-install","text":"We use Non-Sucking Service Manager to manage the worker node service, and that requires some setup. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run suam-worker service.install","title":"Set up the Worker Node Service"},{"location":"setup/worker/#service-start","text":"The worker node service can start immediately but we recommend holding off until you've verified configurations while running the node as a process. Open admin powershell to <repo-root> . Install the SimpleUAM worker service: pdm run suam-worker service.start Further details on running the worker as a service are here ...","title":"Start the Worker Node Service"},{"location":"setup/worker/#next","text":"View information on corpus use here ... View information on local task processing here ... View information on remote task processing here ...","title":"Next Steps"},{"location":"usage/clients/","text":"SimpleUAM Client Nodes \u00b6 Client nodes create designs in the SRI format and send those designs to a SimpleUAM deployment to be processed. Configuration Instructions \u00b6 See here for instructions on how to set up a client node. TODO: Details Client Tasks via CLI \u00b6 See here for basic interface. There are two available sub-commands: direct2cad.gen-info-files : Generates info files for a given design. direct2cad.process-design : Runs Creo and FDM for a given design. Which can be run using the suam-client entry point, shown with the mandatory arguments for the input design ( <design-file> ) and the results directory ( <results-dir> ): pdm run suam-client <sub-command> --design = <design-file> --results = <results-dir> This print some logs to stderr and the location of newly created results archive to stdout. Arguments: -d <design-file> / --design=<design-file> : (Mandatory) The design file, usually design_swri.json , to use as an input to the command. -r <results-dir> / --results=<results-dir> : (Mandatory) The results directory, as accessible to the client, where the result archives will appear on completion. -m <metadata-file> / --metadata=<metdata-file> : An optional file with JSON-encoded metadata that will be included with the metadata.json in the result archive. The metadata must be a JSON dictionary so that fields like message_info , session_info , and result_archive can be added. -b / --backend : Force the use of a dramatiq backend to detect when a sub-command has been finished by a worker. Will fail if no suitable backend is enabled. -p / --polling : For the use of polling to find completed results. This watches the results directory looking for an archive with the appropriate message id. This method is the default when no backend is available. -t <int> / --timeout=<int> : How long to wait, in seconds, before giving up on the command. -i <int>/ --interval= ` : The interval between checks for a new result. Will check using the backend if used, otherwise will scan the results directory for new zip files. Use the following for additional help: pdm run suam-client <sub-command> --help Client Tasks via Python Interface \u00b6 Look at the example project here . TODO: Details Basic Tasks \u00b6 TODO: Details Custom Metadata \u00b6 TODO: Details Waiting for results with Backends and Polling \u00b6 Results will appear as zip files in a results directory and there are two ways for a client application to find them. Backend : If a backend is configured a function can be used to see if the sent task has been completed. Polling : The process can watch the results dir for new files and check their metadata to see if they correspond with the sent message. See the example client for more details. TODO: Details Example Client \u00b6 Find an example project at this github repo . TODO: More thorough break down","title":"Clients"},{"location":"usage/clients/#simpleuam-client-nodes","text":"Client nodes create designs in the SRI format and send those designs to a SimpleUAM deployment to be processed.","title":"SimpleUAM Client Nodes"},{"location":"usage/clients/#config","text":"See here for instructions on how to set up a client node. TODO: Details","title":"Configuration Instructions"},{"location":"usage/clients/#CLI","text":"See here for basic interface. There are two available sub-commands: direct2cad.gen-info-files : Generates info files for a given design. direct2cad.process-design : Runs Creo and FDM for a given design. Which can be run using the suam-client entry point, shown with the mandatory arguments for the input design ( <design-file> ) and the results directory ( <results-dir> ): pdm run suam-client <sub-command> --design = <design-file> --results = <results-dir> This print some logs to stderr and the location of newly created results archive to stdout. Arguments: -d <design-file> / --design=<design-file> : (Mandatory) The design file, usually design_swri.json , to use as an input to the command. -r <results-dir> / --results=<results-dir> : (Mandatory) The results directory, as accessible to the client, where the result archives will appear on completion. -m <metadata-file> / --metadata=<metdata-file> : An optional file with JSON-encoded metadata that will be included with the metadata.json in the result archive. The metadata must be a JSON dictionary so that fields like message_info , session_info , and result_archive can be added. -b / --backend : Force the use of a dramatiq backend to detect when a sub-command has been finished by a worker. Will fail if no suitable backend is enabled. -p / --polling : For the use of polling to find completed results. This watches the results directory looking for an archive with the appropriate message id. This method is the default when no backend is available. -t <int> / --timeout=<int> : How long to wait, in seconds, before giving up on the command. -i <int>/ --interval= ` : The interval between checks for a new result. Will check using the backend if used, otherwise will scan the results directory for new zip files. Use the following for additional help: pdm run suam-client <sub-command> --help","title":"Client Tasks via CLI"},{"location":"usage/clients/#python","text":"Look at the example project here . TODO: Details","title":"Client Tasks via Python Interface"},{"location":"usage/clients/#python-tasks","text":"TODO: Details","title":"Basic Tasks"},{"location":"usage/clients/#python-metadata","text":"TODO: Details","title":"Custom Metadata"},{"location":"usage/clients/#python-watch","text":"Results will appear as zip files in a results directory and there are two ways for a client application to find them. Backend : If a backend is configured a function can be used to see if the sent task has been completed. Polling : The process can watch the results dir for new files and check their metadata to see if they correspond with the sent message. See the example client for more details. TODO: Details","title":"Waiting for results with Backends and Polling"},{"location":"usage/clients/#python-example","text":"Find an example project at this github repo . TODO: More thorough break down","title":"Example Client"},{"location":"usage/config/","text":"SimpleUAM Configuration System \u00b6 SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. In addition there is a utility, suam-config , provided with this repo to help manage configuration information. All commands are run in <repo-root> . Configuration Semantics \u00b6 The SimpleUAM configuration model is based on a few core principles: Configuration variables are split into two categories: Per-machine: These are unique to a single machine, but are shared between all the processes or tasks on that machine. Per-deployment: These are unique to single deployment are can be shared without modification between all the machines in a deployment. Configuration files are split into per-machine and per-deployment categories as well so that they can respectively be defined locally and linked from a central shared location. At a per-machine level there is a single opinionated config directory where all config files must be found. Config files come with functional defaults whenever possible. Config files can be defined partially, with missing fields being populated by defaults or lower priority config files. Together these guidelines give us a basic model of configuration semantics based on a fixed root location for files, incremental overloading, and linking of shared configuration into the fixed root. Root Directory \u00b6 Config files are all located in an fixed root directory. Run the following to find that directory: pdm run suam-config dir See this section for more information on the command and here for information on the specific config files you can find there. Overloading \u00b6 You can incrementally define configurations with multiple files that overload each other. There are 3 general levels of configuration options, from lowest to highest priority. Default : These are defined in the code as a python class. Attributes in that class correspond to fields of the config file. Base Config : This is the base configuration directory as found with pdm run suam-config dir . CLI Argument Config : This is the config dir added to the stack when a command is given --config-dir=<config-dir-arg> as an argument. For example consider the case for a hypothetical foo.conf.yaml and the raw definitions of each configuration: Default Base Config CLI Argument Config util/config/foo_config.py 1 2 3 4 5 6 7 8 9 from attrs import define @define class FooConfig (): name : str = \"John\" age : int = 23 occupation : str = \"Example Salesman\" hobby : str = \"Sampling\" config-dir/foo.conf.yaml 1 2 name : Andy hobby : Cooking config-dir-arg/foo.conf.yaml 1 2 name : Jane age : 32 And the corresponding config as seen by the program in each case. Default Base Config CLI Argument Config These are configuration values when: No config-dir/foo.conf.yaml exists No --config-dir=config-dir-arg argument has been used Loaded, In-Memory Configuration 1 2 3 4 name : John age : 23 occupation : Example Salesman hobby : Sampling These are the loaded values when: config-dir/foo.conf.yaml exists --config-dir=config-dir-arg isn't an argument Loaded, In-Memory Configuration 1 2 3 4 name : Andy age : 23 occupation : Example Salesman hobby : Cooking Note how only the values in <config-dir>/foo.conf.yaml change with the rest remaining at their defaults. These are the loaded values when config-dir/foo.conf.yaml exists config-dir-arg/foo.conf.yaml exists A --config-dir=config-dir-arg argument was given Loaded, In-Memory Configuration 1 2 3 4 name : Jane age : 32 occupation : Example Salesman hobby : Cooking Look here for information on the specific config files, their source locations, expected filenames, and key fields. Linking \u00b6 The ideal way to work with per-deployment configs is to put them all in a single location, accessible from all the nodes in your deployment, and symlink them into the root configuration directories of each machine. See the install command , which makes that process easier. Also see the individual config files for information on whether each file is per-machine or per-deployment. Configuration File Format \u00b6 All SimpleUAM configuration files are in YAML . See this guide for a quick introduction to the format. Variable Interpolation \u00b6 We support OmegaConf's variable interpolation to allow referencing values in one config file from another. Terms between ${ and } in a configuration variable are interpreted as a reference to another field in the current configuration or, with the appropriate key, a field in another configuration file. Interpolation happens after all config files are loaded and all overloading is finished. Basic Interpolation \u00b6 Within a single file you can interpolate variables at the same level: Raw Config 1 2 3 4 5 6 7 field1 : Test field2 : Field1 is '${field1}' field3 : 123 field4 : ${field3} subsection : field1 : Other Value field2 : Sibling Field1 is '${field1}' Which resolves to: Resolved Config 1 2 3 4 5 6 7 field1 : Test field2 : Field1 is 'Test' field3 : 123 field4 : 123 subsection : field1 : Other Value field2 : Sibling Field1 is 'Other Value' Advanced Interpolation \u00b6 You can also look at parent and child fields with interpolation: Raw Config 1 2 3 4 5 field1 : Test field2 : Child Field1 is '${subsection.field1}' subsection : field1 : Other Value field2 : Parent Field1 is '${.field1}' Which resolves to: Resolved Config 1 2 3 4 5 field1 : Test field2 : Child Field1 is 'Other Value' subsection : field1 : Other Value field2 : Parent Field1 is 'Test' Cross-File Interpolation \u00b6 Each SimpleUAM config file also comes with an interpolation key, which can be used to reference its fields in other locations. Consider the following config with the interpolation key \"example\" : Config w/ Key 'example' 1 2 3 parent-field : Example Parent subsection : sub-field : Example Child And the following raw config: Raw Config 1 2 field1 : Parent is '${example:parent-field}' field2 : Child is '${example:subsection.sub-field}' Which resolves to: Resolved Config 1 2 field1 : Parent is 'Example Parent' field2 : Child is 'Example Child' See each config file's info to find out their interpolation keys. Using the suam-config CLI \u00b6 There is an suam-config script included with SimpleUAM for manipulating configuration files. You can get a list of all of its sub-commands by running: pdm run suam-config --list Each sub-command has help output that can be accessed with: pdm run suam-config <sub-command> --help More details on each sub-command follow. The dir sub-command \u00b6 The dir command prints out the current, highest-priority config directory. It can be run as follows: pdm run suam-config dir Arguments : --all / -a : This prints out a list of all the config directories that are being read from lowest to highest priority. Only useful when called with --config-dir= . The list-* sub-commands \u00b6 The various list-* commands print out information on each of the configuration files. The outputs can be used to specify individual config files for the file , print , write , and install commands. list-files \u00b6 Prints a list of recognized configuration files that are searched for in each configuration directory. It can be run as follows: pdm run suam-config list-files Sample Output of pdm run suam-config list-files paths.conf.yaml auth.conf.yaml win_setup.conf.yaml craidl.conf.yaml corpus.conf.yaml d2c_workspace.conf.yaml broker.conf.yaml d2c_worker.conf.yaml Each output filename can be used as a config file identifier in other commands. list-keys \u00b6 Prints a list of recognized interpolation keys. It can be run as follows: pdm run suam-config list-keys Sample Output of pdm run suam-config list-keys path auth win_setup craidl corpus d2c_workspace broker d2c_worker Each interpolation key can be used in configuration files and as a config file identifier in other commands. list-classes \u00b6 Prints a list of python classes corresponding to each configuration file. It can be run as follows: pdm run suam-config list-classes Sample Output of pdm run suam-config list-classes PathConfig AuthConfig WinSetupConfig CraidlConfig CorpusConfig D2CWorkspaceConfig BrokerConfig D2CWorkerConfig Each python class name can be used as a config file identifier in other commands. The file sub-command \u00b6 The file sub-command is the single-file counterpart to the dir sub-command. It prints out where SimpleUAM is looking for a configuration file. It can be run as follows: pdm run suam-config file -c <config-id> Arguments : --config=STRING / -c STRING (Mandatory) : The config file whose search location you're asking for. \" STRING \" can be any of the output lines from the list-* subcommands. --all / -a : This prints out a list of all the config file that are being read from lowest to highest priority. Only useful when called with --config-dir= . The print sub-command \u00b6 The print sub-command will print out the currently loaded configuration after all input files have been read and processed. This is the configuration as other parts of SimpleUAM see it if run in similar circumstances. To print a specific configuration use: pdm run suam-config print -c <config-id> With <config-id> as one of the outputs from a list-* command. This flag can be given multiple times in order to print multiple config files, as follows: pdm run suam-config print -c path -c auth To print all the configurations use: pdm run suam-config print --all Add the --resolved flag to print the configurations after interpolations have been applied. Arguments : --config=STRING / -c STRING (Mandatory) : The config file to be printed out. \" STRING \" can be any of the output lines from the list-* subcommands. Can be given multiple times to print out multiple configs. --all / -a : This prints out a list of all the config file that are being read from lowest to highest priority. Only useful when called with --config-dir= . --resolved / -r : Instead of printing out the unevaluated interpolations, the default, resolve all the fields before printing. The write sub-command \u00b6 the write sub-command will write the currently loaded configuration to the default configuration directory. This is a good way to initialize the configuration files for a machine. By default this will only write fully commented configuration files to disk. To write out all config files to the default configuration directory use: pdm run suam-config write Individual and multiple config files can be specified in the same way as print . Arguments : --config=STRING / -c STRING : The config file to be written out. \" STRING \" can be any of the output lines from the list-* subcommands. Can be given multiple times to print out multiple configs. --no-comment : Don't comment the output files. --overwrite / -o : Don't skip files that exist, overwrite them with the new versions instead. --output=DIR / -u DIR : Write config files out to DIR instead of the default config directory. The install sub-command \u00b6 The install sub-command links or copies config files from a provided directory to the default config directory. Note that config files being copied must have names that match the ones from the list-files sub-command . To symlink all the config files from <dir> into the config directory run: pdm run suam-config install -i <dir> To copy all the config files from <dir> into the config directory run: pdm run suam-config install -i <dir> --no-symlink To symlink individual files run: pdm run suam-config install -i <file> Arguments : --input=STRING / -i STRING (Mandatory): A config file or directory to copy into the default search location. If STRING is a directory the argument can only be given once, if a file then it can given multiple times. --no-symlink : Don't create symlinks, just copy the files over. --overwrite / -o : Don't skip files that exist, overwrite them with the new versions instead. Configuration Files \u00b6 Some basic information on each config file paths.conf.yaml \u00b6 Defines paths that SimpleUAM will use for caches, logs, data, etc... Most people shouldn't need to change this. Locations for more specific tasks are defined in their respective configuration files and changing those should suffice. Changes to this config will require reinstallation or rebuilding large portions of a worker node. Try to keep this static after initial setup. Properties \u00b6 Python Dataclass : simple_uam.util.config.path_config.PathConfig ( API ) Purview Category : Per-Machine Config File Name : paths.conf.yaml Interpolation Key : path Contents \u00b6 Default paths.conf.yaml ### paths.conf.yaml ### ### NOTE: All defaults are system specific config_directory : ... cache_directory : ... log_directory : ... work_directory : ... data_directory : ... Key Fields \u00b6 config_directory : This will add another configuration directory to the chain of overloaded config directories . Try to avoid setting this without good reason. cache_directory : Location for files which are used in install and execution but can be regenerated after deletion. log_directory : Location to place any logs that are generated by SimpleUAM. work_directory : Location for files that are used as various SimpleUAM commands and programs run. data_directory : Location for static files that need to present at runtime but which aren't, usually, directly modified. auth.conf.yaml \u00b6 Authentication keys or tokens needed by a SimpleUAM install. Try not to check this into a repo or make it public. Properties \u00b6 Python Dataclass : simple_uam.util.config.auth_config.AuthConfig ( API ) Purview Category : Per-Deployment Config File Name : auth.conf.yaml Interpolation Key : auth Contents \u00b6 Default auth.conf.yaml ### auth.conf.yaml ### isis_user : null isis_token : null Example auth.conf.yaml ### auth.conf.yaml ### isis_user : myIsisUsername isis_token : 'glpat-XXXXXXXXXXXXXXXXXXXX' Key Fields \u00b6 isis_user : Either null or your username for the Isis GitLab instance. isis_token : Either null or an API token for the Isis GitLab instance. corpus.conf.yaml \u00b6 The repositories and git refspecs used in an install of SimpleUAM. These can be changed to specify the versions of any SWRi code used and make a deployment more repeatable. Properties \u00b6 Python Dataclass : simple_uam.util.config.corpus_config.CorpusConfig ( API ) Purview Category : Per-Deployment Config File Name : corpus.conf.yaml Interpolation Key : corpus Contents \u00b6 Default corpus.conf.yaml ### corpus.conf.yaml ### trinity : repo : https://git.isis.vanderbilt.edu/SwRI/ta1/sri-ta1/trinity-craidl.git branch : main examples_dir : examples graphml_corpus : repo : https://git.isis.vanderbilt.edu/SwRI/athens-uav-workflows.git branch : uam_corpus graphml_file : ExportedGraphML/all_schema_uam.graphml creopyson : repo : https://git.isis.vanderbilt.edu/SwRI/creoson/creopyson.git branch : main creoson_server : api : https://git.isis.vanderbilt.edu/api/v4/projects/499/jobs/3827/artifacts/out/CreosonServerWithSetup-2.8.0-win64.zip manual : https://git.isis.vanderbilt.edu/SwRI/creoson/creoson-server/-/jobs/artifacts/main/raw/out/CreosonServerWithSetup-2.8.0-win64.zip?job=build-job direct2cad : repo : https://git.isis.vanderbilt.edu/SwRI/uam_direct2cad.git branch : main Key Fields \u00b6 trinity : Defines the location and version of the trinity-craidl repo to copy example designs from. graphml_corpus : Defines where to find the graphml dump of the component and design corpus. creopyson : Defines where to find the python package with Creoson bindings. creoson_server : URLs for both API (GitLab Token) based and manual downloads of the modified creoson server used by direct2cad. direct2cad : Defines the location of the core direct2cad repo used for a deployment. win_setup.conf.yaml \u00b6 Options for packages to install on various windows nodes. Should be left unchanged by most, this mainly exists to make larger scale deployment easier to customize. Properties \u00b6 Python Dataclass : simple_uam.util.config.win_setup_config.WinSetupConfig ( API ) Purview Category : Per-Deployment Config File Name : win_setup.conf.yaml Interpolation Key : win_setup Contents \u00b6 Default win_setup.conf.yaml ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - openjdk11 - rsync - nssm worker_pip_packages : - psutil - parea - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 - nssm qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad Key Fields \u00b6 global_dep_packages : Chocolatey packages to install on all node, please only add to this list. qol_dep_packages : QoL chocolatey packages. *_dep_packages : A list of chocolatey packages to install on the corresponding type of node. Please only add to this list. worker_pip_packages : Pip packages to be installed globally on the worker (as opposed to within a venv via PDM), needed for direct2cad. craidl.conf.yaml \u00b6 Settings for working with a corpus DB or static corpus. Properties \u00b6 Python Dataclass : simple_uam.util.config.craidl_config.CraidlConfig ( API ) Purview Category : Per-Machine Config File Name : craidl.conf.yaml Interpolation Key : craidl Contents \u00b6 Default craidl.conf.yaml ### craidl.conf.yaml ### example_dir : ${path:data_directory}/craidl_examples stub_server : cache_dir : ${path:cache_directory}/corpus_stub_cache server_dir : ${path:data_directory}/corpus_stub_server graphml_corpus : ${path:data_directory}/corpus_stub.graphml host : localhost port : 8182 read_only : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/craidl_stub_db/stdout.log stderr_file : ${path:log_directory}/craidl_stub_db/stderr.log rotate_io : true auto_start : false console : true interactive : false server_host : localhost server_port : ${stub_server.port} static_corpus : ${path:data_directory}/corpus_static_dump.json static_corpus_cache : ${path:cache_directory}/static_corpus_cache use_static_corpus : true Key Fields \u00b6 example_dir : Location for saved example designs. Possibly better thought of as a static design corpus to live alongside the static component corpus. stub_server : Settings for running the TinkerPop server reference implementation with a provided graphml corpus dump. graphml_corpus : The graphml corpus file to load on server start. host : The host to serve the graph database to. localhost will only serve to clients on the same machine. 0.0.0.0 will serve to any client. port : Port to server the database on. read_only : Should the stub server allow writes? Note that even if false no writes will be persisted, the state of the DB is reset on each run. server_host : The corpus DB to connect to when generating info files or creating a static component corpus. server_port : The port of the corpus DB to connect to for various tasks. static_corpus : The static corpus to use when performing various tasks. static_corpus_cache : The location of the cache used when generating a new static corpus from the corpus DB. use_static_corpus : Use the static corpus when possible if true otherwise default to using a running corpus DB d2c_workspace.conf.yaml \u00b6 Settings for the execution of direct2cad tasks on a worker node Properties \u00b6 Python Dataclass : simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig ( API ) Purview Category : Per-Deployment (mostly) Config File Name : d2c_workspace.conf.yaml Interpolation Key : d2c_workspace Contents \u00b6 Default d2c_workspace.conf.yaml ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : ${workspaces_dir}/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json workspaces_dir : ${path:work_directory}/d2c_workspaces cache_dir : ${path:cache_directory}/d2c_workspaces max_workspaces : 1 exclude : - .git result_exclude : - .git - workingdir/*.prt Key Fields \u00b6 results_dir : The location to place analysis result zip files. Should be set to point to the results storage. If workers are different then use a symlink to the intended directory. results : Options on how to generate and store results. max_count : Number of results to allow in results_dir . The oldest zip files will be deleted until the count is low enough. -1 disables pruning entirely. min_staletime : Time after last access to prevent the deletion of a record in seconds. Results that aren't stale enough will not be deleted even if there are more than max_count . broker.conf.yaml \u00b6 Settings that define how to connect to a message broker and backend. It is used by both worker nodes and clients, and is very important to get correct in order for a SimpleUAM deployment to function. Thankfully, this is set up such that all the workers and clients should be able to share a single file unless there's network or DNS weirdness. Properties \u00b6 Python Dataclass : simple_uam.util.config.broker_config.BrokerConfig ( API ) Purview Category : Per-Deployment Config File Name : broker.conf.yaml Interpolation Key : broker Contents \u00b6 Default broker.conf.yaml ### broker.conf.yaml ### protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : ${.protocol}://${.host}:${.port}${.db} backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : /0 url : ${.protocol}://${.host}:${.port}${.db} Key Fields \u00b6 protocol : The protocol to use for the broker. Generally either amqp or amqps for RabbitMQ and redis for Redis. host : The IP or URL of the message broker. port : The port to connect to on the message broker. db : The Redis db if needed, this should include a beginning / if provided at all. (e.g. use /0 instead of just 0 ) url : The full url of the message broker including protocol host, port, and the like. If provided then protocol , host , port , and db are ignored. Make sure that url contains all necessary information. backend : Settings for an optional Message Backend (used for completion notifications) enabled : Set to true in order to enable the backend. protocol : Protocol to use for the backend, the only supported option is redis . host : The IP or URL of the message backend. port : The port to connect to on the message backend. db : The Redis db this should include a beginning / if provided at all. (e.g. use /0 instead of just 0 ) url : The full url of the messag backend including protocol host, port, and the like. If provided then backend.protocol , backend.host , backend.port , and backend.db are ignored. Make sure that backend.url contains all necessary information. d2c_worker.conf.yaml \u00b6 Settings for running the process that listens to the message broker for new tasks, executes them via the workspace code, and potentially returns confirmations or other information to a backend. Properties \u00b6 Python Dataclass : simple_uam.util.config.d2c_worker_config.D2CWorkerConfig ( API ) Purview Category : Per-Machine Config File Name : d2c_worker.conf.yaml Interpolation Key : d2c_worker Contents \u00b6 Default d2c_worker.conf.yaml ### d2c_worker.conf.yaml ### max_processes : ${d2c_workspace:max_workspaces} max_threads : 1 shutdown_timeout : 600000 skip_logging : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/d2c_worker/stdout.log stderr_file : ${path:log_directory}/d2c_worker/stderr.log rotate_io : true auto_start : false console : true interactive : false Key Fields \u00b6 max_processes : The number of simultaneously running direct2cad processes. Due to limitations with Creoson the only supported number is 1 . max_threads : The number of threads, per-process, on which to run direct2cad tasks. The only currently supported value is 1 . shutdown_timeout : How long to wait for a worker to shutdown in milliseconds. skip_logging : Do we preserve the logs that dramatiq produces? Note that this does not effect the usual structlog based logging, just the logging from the library that manages a worker process. service : Settings for running the direct2cad worker as a service. These correspond closely to the corresponding settings in NSSM .","title":"Configuration"},{"location":"usage/config/#simpleuam-configuration-system","text":"SimpleUAM uses an internal configuration system based on OmegaConf . It reads YAML files in a platform specific directory for settings that are used throughout the system. In addition there is a utility, suam-config , provided with this repo to help manage configuration information. All commands are run in <repo-root> .","title":"SimpleUAM Configuration System"},{"location":"usage/config/#semantics","text":"The SimpleUAM configuration model is based on a few core principles: Configuration variables are split into two categories: Per-machine: These are unique to a single machine, but are shared between all the processes or tasks on that machine. Per-deployment: These are unique to single deployment are can be shared without modification between all the machines in a deployment. Configuration files are split into per-machine and per-deployment categories as well so that they can respectively be defined locally and linked from a central shared location. At a per-machine level there is a single opinionated config directory where all config files must be found. Config files come with functional defaults whenever possible. Config files can be defined partially, with missing fields being populated by defaults or lower priority config files. Together these guidelines give us a basic model of configuration semantics based on a fixed root location for files, incremental overloading, and linking of shared configuration into the fixed root.","title":"Configuration Semantics"},{"location":"usage/config/#semantics-root","text":"Config files are all located in an fixed root directory. Run the following to find that directory: pdm run suam-config dir See this section for more information on the command and here for information on the specific config files you can find there.","title":"Root Directory"},{"location":"usage/config/#semantics-overloading","text":"You can incrementally define configurations with multiple files that overload each other. There are 3 general levels of configuration options, from lowest to highest priority. Default : These are defined in the code as a python class. Attributes in that class correspond to fields of the config file. Base Config : This is the base configuration directory as found with pdm run suam-config dir . CLI Argument Config : This is the config dir added to the stack when a command is given --config-dir=<config-dir-arg> as an argument. For example consider the case for a hypothetical foo.conf.yaml and the raw definitions of each configuration: Default Base Config CLI Argument Config util/config/foo_config.py 1 2 3 4 5 6 7 8 9 from attrs import define @define class FooConfig (): name : str = \"John\" age : int = 23 occupation : str = \"Example Salesman\" hobby : str = \"Sampling\" config-dir/foo.conf.yaml 1 2 name : Andy hobby : Cooking config-dir-arg/foo.conf.yaml 1 2 name : Jane age : 32 And the corresponding config as seen by the program in each case. Default Base Config CLI Argument Config These are configuration values when: No config-dir/foo.conf.yaml exists No --config-dir=config-dir-arg argument has been used Loaded, In-Memory Configuration 1 2 3 4 name : John age : 23 occupation : Example Salesman hobby : Sampling These are the loaded values when: config-dir/foo.conf.yaml exists --config-dir=config-dir-arg isn't an argument Loaded, In-Memory Configuration 1 2 3 4 name : Andy age : 23 occupation : Example Salesman hobby : Cooking Note how only the values in <config-dir>/foo.conf.yaml change with the rest remaining at their defaults. These are the loaded values when config-dir/foo.conf.yaml exists config-dir-arg/foo.conf.yaml exists A --config-dir=config-dir-arg argument was given Loaded, In-Memory Configuration 1 2 3 4 name : Jane age : 32 occupation : Example Salesman hobby : Cooking Look here for information on the specific config files, their source locations, expected filenames, and key fields.","title":"Overloading"},{"location":"usage/config/#semantics-linking","text":"The ideal way to work with per-deployment configs is to put them all in a single location, accessible from all the nodes in your deployment, and symlink them into the root configuration directories of each machine. See the install command , which makes that process easier. Also see the individual config files for information on whether each file is per-machine or per-deployment.","title":"Linking"},{"location":"usage/config/#format","text":"All SimpleUAM configuration files are in YAML . See this guide for a quick introduction to the format.","title":"Configuration File Format"},{"location":"usage/config/#format-inter","text":"We support OmegaConf's variable interpolation to allow referencing values in one config file from another. Terms between ${ and } in a configuration variable are interpreted as a reference to another field in the current configuration or, with the appropriate key, a field in another configuration file. Interpolation happens after all config files are loaded and all overloading is finished.","title":"Variable Interpolation"},{"location":"usage/config/#format-inter-basic","text":"Within a single file you can interpolate variables at the same level: Raw Config 1 2 3 4 5 6 7 field1 : Test field2 : Field1 is '${field1}' field3 : 123 field4 : ${field3} subsection : field1 : Other Value field2 : Sibling Field1 is '${field1}' Which resolves to: Resolved Config 1 2 3 4 5 6 7 field1 : Test field2 : Field1 is 'Test' field3 : 123 field4 : 123 subsection : field1 : Other Value field2 : Sibling Field1 is 'Other Value'","title":"Basic Interpolation"},{"location":"usage/config/#format-inter-advanced","text":"You can also look at parent and child fields with interpolation: Raw Config 1 2 3 4 5 field1 : Test field2 : Child Field1 is '${subsection.field1}' subsection : field1 : Other Value field2 : Parent Field1 is '${.field1}' Which resolves to: Resolved Config 1 2 3 4 5 field1 : Test field2 : Child Field1 is 'Other Value' subsection : field1 : Other Value field2 : Parent Field1 is 'Test'","title":"Advanced Interpolation"},{"location":"usage/config/#format-inter-cross-file","text":"Each SimpleUAM config file also comes with an interpolation key, which can be used to reference its fields in other locations. Consider the following config with the interpolation key \"example\" : Config w/ Key 'example' 1 2 3 parent-field : Example Parent subsection : sub-field : Example Child And the following raw config: Raw Config 1 2 field1 : Parent is '${example:parent-field}' field2 : Child is '${example:subsection.sub-field}' Which resolves to: Resolved Config 1 2 field1 : Parent is 'Example Parent' field2 : Child is 'Example Child' See each config file's info to find out their interpolation keys.","title":"Cross-File Interpolation"},{"location":"usage/config/#cli","text":"There is an suam-config script included with SimpleUAM for manipulating configuration files. You can get a list of all of its sub-commands by running: pdm run suam-config --list Each sub-command has help output that can be accessed with: pdm run suam-config <sub-command> --help More details on each sub-command follow.","title":"Using the suam-config CLI"},{"location":"usage/config/#cli-dir","text":"The dir command prints out the current, highest-priority config directory. It can be run as follows: pdm run suam-config dir Arguments : --all / -a : This prints out a list of all the config directories that are being read from lowest to highest priority. Only useful when called with --config-dir= .","title":"The dir sub-command"},{"location":"usage/config/#cli-list","text":"The various list-* commands print out information on each of the configuration files. The outputs can be used to specify individual config files for the file , print , write , and install commands.","title":"The list-* sub-commands"},{"location":"usage/config/#cli-list-files","text":"Prints a list of recognized configuration files that are searched for in each configuration directory. It can be run as follows: pdm run suam-config list-files Sample Output of pdm run suam-config list-files paths.conf.yaml auth.conf.yaml win_setup.conf.yaml craidl.conf.yaml corpus.conf.yaml d2c_workspace.conf.yaml broker.conf.yaml d2c_worker.conf.yaml Each output filename can be used as a config file identifier in other commands.","title":"list-files"},{"location":"usage/config/#cli-list-keys","text":"Prints a list of recognized interpolation keys. It can be run as follows: pdm run suam-config list-keys Sample Output of pdm run suam-config list-keys path auth win_setup craidl corpus d2c_workspace broker d2c_worker Each interpolation key can be used in configuration files and as a config file identifier in other commands.","title":"list-keys"},{"location":"usage/config/#cli-list-classes","text":"Prints a list of python classes corresponding to each configuration file. It can be run as follows: pdm run suam-config list-classes Sample Output of pdm run suam-config list-classes PathConfig AuthConfig WinSetupConfig CraidlConfig CorpusConfig D2CWorkspaceConfig BrokerConfig D2CWorkerConfig Each python class name can be used as a config file identifier in other commands.","title":"list-classes"},{"location":"usage/config/#cli-file","text":"The file sub-command is the single-file counterpart to the dir sub-command. It prints out where SimpleUAM is looking for a configuration file. It can be run as follows: pdm run suam-config file -c <config-id> Arguments : --config=STRING / -c STRING (Mandatory) : The config file whose search location you're asking for. \" STRING \" can be any of the output lines from the list-* subcommands. --all / -a : This prints out a list of all the config file that are being read from lowest to highest priority. Only useful when called with --config-dir= .","title":"The file sub-command"},{"location":"usage/config/#cli-print","text":"The print sub-command will print out the currently loaded configuration after all input files have been read and processed. This is the configuration as other parts of SimpleUAM see it if run in similar circumstances. To print a specific configuration use: pdm run suam-config print -c <config-id> With <config-id> as one of the outputs from a list-* command. This flag can be given multiple times in order to print multiple config files, as follows: pdm run suam-config print -c path -c auth To print all the configurations use: pdm run suam-config print --all Add the --resolved flag to print the configurations after interpolations have been applied. Arguments : --config=STRING / -c STRING (Mandatory) : The config file to be printed out. \" STRING \" can be any of the output lines from the list-* subcommands. Can be given multiple times to print out multiple configs. --all / -a : This prints out a list of all the config file that are being read from lowest to highest priority. Only useful when called with --config-dir= . --resolved / -r : Instead of printing out the unevaluated interpolations, the default, resolve all the fields before printing.","title":"The print sub-command"},{"location":"usage/config/#cli-write","text":"the write sub-command will write the currently loaded configuration to the default configuration directory. This is a good way to initialize the configuration files for a machine. By default this will only write fully commented configuration files to disk. To write out all config files to the default configuration directory use: pdm run suam-config write Individual and multiple config files can be specified in the same way as print . Arguments : --config=STRING / -c STRING : The config file to be written out. \" STRING \" can be any of the output lines from the list-* subcommands. Can be given multiple times to print out multiple configs. --no-comment : Don't comment the output files. --overwrite / -o : Don't skip files that exist, overwrite them with the new versions instead. --output=DIR / -u DIR : Write config files out to DIR instead of the default config directory.","title":"The write sub-command"},{"location":"usage/config/#cli-install","text":"The install sub-command links or copies config files from a provided directory to the default config directory. Note that config files being copied must have names that match the ones from the list-files sub-command . To symlink all the config files from <dir> into the config directory run: pdm run suam-config install -i <dir> To copy all the config files from <dir> into the config directory run: pdm run suam-config install -i <dir> --no-symlink To symlink individual files run: pdm run suam-config install -i <file> Arguments : --input=STRING / -i STRING (Mandatory): A config file or directory to copy into the default search location. If STRING is a directory the argument can only be given once, if a file then it can given multiple times. --no-symlink : Don't create symlinks, just copy the files over. --overwrite / -o : Don't skip files that exist, overwrite them with the new versions instead.","title":"The install sub-command"},{"location":"usage/config/#files","text":"Some basic information on each config file","title":"Configuration Files"},{"location":"usage/config/#files-path","text":"Defines paths that SimpleUAM will use for caches, logs, data, etc... Most people shouldn't need to change this. Locations for more specific tasks are defined in their respective configuration files and changing those should suffice. Changes to this config will require reinstallation or rebuilding large portions of a worker node. Try to keep this static after initial setup.","title":"paths.conf.yaml"},{"location":"usage/config/#properties","text":"Python Dataclass : simple_uam.util.config.path_config.PathConfig ( API ) Purview Category : Per-Machine Config File Name : paths.conf.yaml Interpolation Key : path","title":"Properties"},{"location":"usage/config/#contents","text":"Default paths.conf.yaml ### paths.conf.yaml ### ### NOTE: All defaults are system specific config_directory : ... cache_directory : ... log_directory : ... work_directory : ... data_directory : ...","title":"Contents"},{"location":"usage/config/#key-fields","text":"config_directory : This will add another configuration directory to the chain of overloaded config directories . Try to avoid setting this without good reason. cache_directory : Location for files which are used in install and execution but can be regenerated after deletion. log_directory : Location to place any logs that are generated by SimpleUAM. work_directory : Location for files that are used as various SimpleUAM commands and programs run. data_directory : Location for static files that need to present at runtime but which aren't, usually, directly modified.","title":"Key Fields"},{"location":"usage/config/#files-auth","text":"Authentication keys or tokens needed by a SimpleUAM install. Try not to check this into a repo or make it public.","title":"auth.conf.yaml"},{"location":"usage/config/#properties_1","text":"Python Dataclass : simple_uam.util.config.auth_config.AuthConfig ( API ) Purview Category : Per-Deployment Config File Name : auth.conf.yaml Interpolation Key : auth","title":"Properties"},{"location":"usage/config/#contents_1","text":"Default auth.conf.yaml ### auth.conf.yaml ### isis_user : null isis_token : null Example auth.conf.yaml ### auth.conf.yaml ### isis_user : myIsisUsername isis_token : 'glpat-XXXXXXXXXXXXXXXXXXXX'","title":"Contents"},{"location":"usage/config/#key-fields_1","text":"isis_user : Either null or your username for the Isis GitLab instance. isis_token : Either null or an API token for the Isis GitLab instance.","title":"Key Fields"},{"location":"usage/config/#files-corpus","text":"The repositories and git refspecs used in an install of SimpleUAM. These can be changed to specify the versions of any SWRi code used and make a deployment more repeatable.","title":"corpus.conf.yaml"},{"location":"usage/config/#properties_2","text":"Python Dataclass : simple_uam.util.config.corpus_config.CorpusConfig ( API ) Purview Category : Per-Deployment Config File Name : corpus.conf.yaml Interpolation Key : corpus","title":"Properties"},{"location":"usage/config/#contents_2","text":"Default corpus.conf.yaml ### corpus.conf.yaml ### trinity : repo : https://git.isis.vanderbilt.edu/SwRI/ta1/sri-ta1/trinity-craidl.git branch : main examples_dir : examples graphml_corpus : repo : https://git.isis.vanderbilt.edu/SwRI/athens-uav-workflows.git branch : uam_corpus graphml_file : ExportedGraphML/all_schema_uam.graphml creopyson : repo : https://git.isis.vanderbilt.edu/SwRI/creoson/creopyson.git branch : main creoson_server : api : https://git.isis.vanderbilt.edu/api/v4/projects/499/jobs/3827/artifacts/out/CreosonServerWithSetup-2.8.0-win64.zip manual : https://git.isis.vanderbilt.edu/SwRI/creoson/creoson-server/-/jobs/artifacts/main/raw/out/CreosonServerWithSetup-2.8.0-win64.zip?job=build-job direct2cad : repo : https://git.isis.vanderbilt.edu/SwRI/uam_direct2cad.git branch : main","title":"Contents"},{"location":"usage/config/#key-fields_2","text":"trinity : Defines the location and version of the trinity-craidl repo to copy example designs from. graphml_corpus : Defines where to find the graphml dump of the component and design corpus. creopyson : Defines where to find the python package with Creoson bindings. creoson_server : URLs for both API (GitLab Token) based and manual downloads of the modified creoson server used by direct2cad. direct2cad : Defines the location of the core direct2cad repo used for a deployment.","title":"Key Fields"},{"location":"usage/config/#files-win-setup","text":"Options for packages to install on various windows nodes. Should be left unchanged by most, this mainly exists to make larger scale deployment easier to customize.","title":"win_setup.conf.yaml"},{"location":"usage/config/#properties_3","text":"Python Dataclass : simple_uam.util.config.win_setup_config.WinSetupConfig ( API ) Purview Category : Per-Deployment Config File Name : win_setup.conf.yaml Interpolation Key : win_setup","title":"Properties"},{"location":"usage/config/#contents_3","text":"Default win_setup.conf.yaml ### win_setup.conf.yaml ### global_dep_packages : - checksum - wget - 7zip broker_dep_packages : - rabbitmq worker_dep_packages : - openjdk11 - rsync - nssm worker_pip_packages : - psutil - parea - numpy license_dep_packages : [] graph_dep_packages : - openjdk11 - nssm qol_packages : - firefox - notepadplusplus - foxitreader - tess - freecad","title":"Contents"},{"location":"usage/config/#key-fields_3","text":"global_dep_packages : Chocolatey packages to install on all node, please only add to this list. qol_dep_packages : QoL chocolatey packages. *_dep_packages : A list of chocolatey packages to install on the corresponding type of node. Please only add to this list. worker_pip_packages : Pip packages to be installed globally on the worker (as opposed to within a venv via PDM), needed for direct2cad.","title":"Key Fields"},{"location":"usage/config/#files-craidl","text":"Settings for working with a corpus DB or static corpus.","title":"craidl.conf.yaml"},{"location":"usage/config/#properties_4","text":"Python Dataclass : simple_uam.util.config.craidl_config.CraidlConfig ( API ) Purview Category : Per-Machine Config File Name : craidl.conf.yaml Interpolation Key : craidl","title":"Properties"},{"location":"usage/config/#contents_4","text":"Default craidl.conf.yaml ### craidl.conf.yaml ### example_dir : ${path:data_directory}/craidl_examples stub_server : cache_dir : ${path:cache_directory}/corpus_stub_cache server_dir : ${path:data_directory}/corpus_stub_server graphml_corpus : ${path:data_directory}/corpus_stub.graphml host : localhost port : 8182 read_only : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/craidl_stub_db/stdout.log stderr_file : ${path:log_directory}/craidl_stub_db/stderr.log rotate_io : true auto_start : false console : true interactive : false server_host : localhost server_port : ${stub_server.port} static_corpus : ${path:data_directory}/corpus_static_dump.json static_corpus_cache : ${path:cache_directory}/static_corpus_cache use_static_corpus : true","title":"Contents"},{"location":"usage/config/#key-fields_4","text":"example_dir : Location for saved example designs. Possibly better thought of as a static design corpus to live alongside the static component corpus. stub_server : Settings for running the TinkerPop server reference implementation with a provided graphml corpus dump. graphml_corpus : The graphml corpus file to load on server start. host : The host to serve the graph database to. localhost will only serve to clients on the same machine. 0.0.0.0 will serve to any client. port : Port to server the database on. read_only : Should the stub server allow writes? Note that even if false no writes will be persisted, the state of the DB is reset on each run. server_host : The corpus DB to connect to when generating info files or creating a static component corpus. server_port : The port of the corpus DB to connect to for various tasks. static_corpus : The static corpus to use when performing various tasks. static_corpus_cache : The location of the cache used when generating a new static corpus from the corpus DB. use_static_corpus : Use the static corpus when possible if true otherwise default to using a running corpus DB","title":"Key Fields"},{"location":"usage/config/#files-d2c-workspace","text":"Settings for the execution of direct2cad tasks on a worker node","title":"d2c_workspace.conf.yaml"},{"location":"usage/config/#properties_5","text":"Python Dataclass : simple_uam.util.config.d2c_workspace_config.D2CWorkspaceConfig ( API ) Purview Category : Per-Deployment (mostly) Config File Name : d2c_workspace.conf.yaml Interpolation Key : d2c_workspace","title":"Properties"},{"location":"usage/config/#contents_5","text":"Default d2c_workspace.conf.yaml ### d2c_workspace.conf.yaml ### workspace_subdir_pattern : workspace_{} reference_subdir : reference_workspace assets_subdir : assets locks_subdir : workspace_locks results_dir : ${workspaces_dir}/results results : max_count : -1 min_staletime : 3600 metadata_file : metadata.json log_file : log.json workspaces_dir : ${path:work_directory}/d2c_workspaces cache_dir : ${path:cache_directory}/d2c_workspaces max_workspaces : 1 exclude : - .git result_exclude : - .git - workingdir/*.prt","title":"Contents"},{"location":"usage/config/#key-fields_5","text":"results_dir : The location to place analysis result zip files. Should be set to point to the results storage. If workers are different then use a symlink to the intended directory. results : Options on how to generate and store results. max_count : Number of results to allow in results_dir . The oldest zip files will be deleted until the count is low enough. -1 disables pruning entirely. min_staletime : Time after last access to prevent the deletion of a record in seconds. Results that aren't stale enough will not be deleted even if there are more than max_count .","title":"Key Fields"},{"location":"usage/config/#files-broker","text":"Settings that define how to connect to a message broker and backend. It is used by both worker nodes and clients, and is very important to get correct in order for a SimpleUAM deployment to function. Thankfully, this is set up such that all the workers and clients should be able to share a single file unless there's network or DNS weirdness.","title":"broker.conf.yaml"},{"location":"usage/config/#properties_6","text":"Python Dataclass : simple_uam.util.config.broker_config.BrokerConfig ( API ) Purview Category : Per-Deployment Config File Name : broker.conf.yaml Interpolation Key : broker","title":"Properties"},{"location":"usage/config/#contents_6","text":"Default broker.conf.yaml ### broker.conf.yaml ### protocol : amqp host : 127.0.0.1 port : 5672 db : '' url : ${.protocol}://${.host}:${.port}${.db} backend : enabled : false protocol : redis host : 127.0.0.1 port : 6379 db : /0 url : ${.protocol}://${.host}:${.port}${.db}","title":"Contents"},{"location":"usage/config/#key-fields_6","text":"protocol : The protocol to use for the broker. Generally either amqp or amqps for RabbitMQ and redis for Redis. host : The IP or URL of the message broker. port : The port to connect to on the message broker. db : The Redis db if needed, this should include a beginning / if provided at all. (e.g. use /0 instead of just 0 ) url : The full url of the message broker including protocol host, port, and the like. If provided then protocol , host , port , and db are ignored. Make sure that url contains all necessary information. backend : Settings for an optional Message Backend (used for completion notifications) enabled : Set to true in order to enable the backend. protocol : Protocol to use for the backend, the only supported option is redis . host : The IP or URL of the message backend. port : The port to connect to on the message backend. db : The Redis db this should include a beginning / if provided at all. (e.g. use /0 instead of just 0 ) url : The full url of the messag backend including protocol host, port, and the like. If provided then backend.protocol , backend.host , backend.port , and backend.db are ignored. Make sure that backend.url contains all necessary information.","title":"Key Fields"},{"location":"usage/config/#files-d2c-worker","text":"Settings for running the process that listens to the message broker for new tasks, executes them via the workspace code, and potentially returns confirmations or other information to a backend.","title":"d2c_worker.conf.yaml"},{"location":"usage/config/#properties_7","text":"Python Dataclass : simple_uam.util.config.d2c_worker_config.D2CWorkerConfig ( API ) Purview Category : Per-Machine Config File Name : d2c_worker.conf.yaml Interpolation Key : d2c_worker","title":"Properties"},{"location":"usage/config/#contents_7","text":"Default d2c_worker.conf.yaml ### d2c_worker.conf.yaml ### max_processes : ${d2c_workspace:max_workspaces} max_threads : 1 shutdown_timeout : 600000 skip_logging : false service : priority : NORMAL exit_action : Restart restart_throttle : 5000 restart_delay : 1000 redirect_io : false stdout_file : ${path:log_directory}/d2c_worker/stdout.log stderr_file : ${path:log_directory}/d2c_worker/stderr.log rotate_io : true auto_start : false console : true interactive : false","title":"Contents"},{"location":"usage/config/#key-fields_7","text":"max_processes : The number of simultaneously running direct2cad processes. Due to limitations with Creoson the only supported number is 1 . max_threads : The number of threads, per-process, on which to run direct2cad tasks. The only currently supported value is 1 . shutdown_timeout : How long to wait for a worker to shutdown in milliseconds. skip_logging : Do we preserve the logs that dramatiq produces? Note that this does not effect the usual structlog based logging, just the logging from the library that manages a worker process. service : Settings for running the direct2cad worker as a service. These correspond closely to the corresponding settings in NSSM .","title":"Key Fields"},{"location":"usage/craidl/","text":"Craidl Tools \u00b6 The craidl module and CLI tool are designed to support generating a info files for a design from SRI's compressed representation. These info files are an expanded version of a design that pulls in specific component information from the corpus so that SWRi's direct2cad pipeline can process it. The core of this is based on work by SRI, albeit heavily modified for a number of reasons. As implemented the Craidl module in SimpleUAM has a few major components/concepts: Corpus : This is the SWRi provided .graphml file that contains information on components and designs within the current challenge problem. Stub Server : A small graph server that can serve the corpus to tools which expect to access it via Gremlin graph queries. Static Corpus : This is component information from the corpus extracted into a much smaller, more efficient, distributable form. Examples : This is a list of designs, in SRI's design representation, that can serve as a pool of seeds or examples for testing. This should probably be called the 'static design corpus' or something. The basic install procedure can be found here . This page covers other things. Updating Craidl \u00b6 When the corpus or various config files change you'll need to propagate those changes to the stub-server. The easiest way to do this is to go through the install instructions again. The operations are all functionally idempotent and make backups of things they would overwrite. Note that corpus.install must be run to update the active corpus before running stub-server.configure which copies that corpus to the appropriate location for the stub server. Running gen_info_files \u00b6 The craidl CLI allows you to generate the direct2cad info files directly using either a static corpus or a corpus DB. This uses the craidl.conf.yaml config information and the gen-info-files sub-command. Run the following for details on the sub-command: pdm run craidl gen-info-files --help This is a functional standalone command that doesn't require additional setup of workspaces or a worker node. Working With Examples \u00b6 We can store a set of examples designs in a local corpus. This is mainly for convenience during testing as there isn't much call for such a store otherwise. The example store contains individual folders for each example's design_swri,json file. The main commands to work with examples are all of the form: pdm run craidl <sub-command> <args> The main sub-commands are: examples.dir : Prints the directory where examples are stored. examples.list : Lists all available examples. examples.add : Will add an example to the example directory. examples.clean : Deletes all stored examples. examples.sri.install : Downloads and installs examples from the trinity-craidl repo. Use the --help argument to get more details on flags and parameters.","title":"Craidl"},{"location":"usage/craidl/#craidl-tools","text":"The craidl module and CLI tool are designed to support generating a info files for a design from SRI's compressed representation. These info files are an expanded version of a design that pulls in specific component information from the corpus so that SWRi's direct2cad pipeline can process it. The core of this is based on work by SRI, albeit heavily modified for a number of reasons. As implemented the Craidl module in SimpleUAM has a few major components/concepts: Corpus : This is the SWRi provided .graphml file that contains information on components and designs within the current challenge problem. Stub Server : A small graph server that can serve the corpus to tools which expect to access it via Gremlin graph queries. Static Corpus : This is component information from the corpus extracted into a much smaller, more efficient, distributable form. Examples : This is a list of designs, in SRI's design representation, that can serve as a pool of seeds or examples for testing. This should probably be called the 'static design corpus' or something. The basic install procedure can be found here . This page covers other things.","title":"Craidl Tools"},{"location":"usage/craidl/#update","text":"When the corpus or various config files change you'll need to propagate those changes to the stub-server. The easiest way to do this is to go through the install instructions again. The operations are all functionally idempotent and make backups of things they would overwrite. Note that corpus.install must be run to update the active corpus before running stub-server.configure which copies that corpus to the appropriate location for the stub server.","title":"Updating Craidl"},{"location":"usage/craidl/#gen-info-files","text":"The craidl CLI allows you to generate the direct2cad info files directly using either a static corpus or a corpus DB. This uses the craidl.conf.yaml config information and the gen-info-files sub-command. Run the following for details on the sub-command: pdm run craidl gen-info-files --help This is a functional standalone command that doesn't require additional setup of workspaces or a worker node.","title":"Running gen_info_files"},{"location":"usage/craidl/#examples","text":"We can store a set of examples designs in a local corpus. This is mainly for convenience during testing as there isn't much call for such a store otherwise. The example store contains individual folders for each example's design_swri,json file. The main commands to work with examples are all of the form: pdm run craidl <sub-command> <args> The main sub-commands are: examples.dir : Prints the directory where examples are stored. examples.list : Lists all available examples. examples.add : Will add an example to the example directory. examples.clean : Deletes all stored examples. examples.sri.install : Downloads and installs examples from the trinity-craidl repo. Use the --help argument to get more details on flags and parameters.","title":"Working With Examples"},{"location":"usage/license/","text":"License Server \u00b6 The license server will provide floating licenses to Creo on each of your worker nodes. Updating Creo License \u00b6 Update the creo license on a running server with a new one. Prerequisites \u00b6 Have your new license available at <creo-license.file> Import to Flexnet Server \u00b6 Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the administration page: User: admin Pass: <creo-license.flexnet-password> Open the 'Vendor Deamon Adminstration' page: Click 'Import License': License File From Your Local Machine : <creo-license.file> Overwrite License File on License Server : Yes Reboot the server. Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the 'Vendor Deamon Adminstration' page: Click 'Administer' for the 'ptc_d' license Click 'Start'","title":"License Server"},{"location":"usage/license/#license-server","text":"The license server will provide floating licenses to Creo on each of your worker nodes.","title":"License Server"},{"location":"usage/license/#update-creo","text":"Update the creo license on a running server with a new one.","title":"Updating Creo License"},{"location":"usage/license/#update-creo-prereqs","text":"Have your new license available at <creo-license.file>","title":"Prerequisites"},{"location":"usage/license/#update-creo-import","text":"Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the administration page: User: admin Pass: <creo-license.flexnet-password> Open the 'Vendor Deamon Adminstration' page: Click 'Import License': License File From Your Local Machine : <creo-license.file> Overwrite License File on License Server : Yes Reboot the server. Open the web interface: Start Menu -> PTC -> Flexnet Admin Web Interface Open the 'Vendor Deamon Adminstration' page: Click 'Administer' for the 'ptc_d' license Click 'Start'","title":"Import to Flexnet Server"},{"location":"usage/workers/","text":"Direct2Cad Worker \u00b6 Section Incomplete Worker runs as a service and listens for tasks from msg broker. When task arrives uses the D2C Workspaces code to run the analysis and produce a result archive. Talk about broker vs backend. Broker gets message from client to worker. Backend gets notification of completion from worker to client. (Really just tells you the name of the zip file from a particular request.) RabbitMQ: Broker only Redis: Both Broker and Backend Supports running worker node as normal process: Normal Process: pdm run suam-worker run Supports running worker node as a service managed by NSSM. The commands do the obvious things. pdm run suam-worker service.install pdm run suam-worker service.uninstall pdm run suam-worker service.configure : Converts config file settings to NSSM settings. pdm run suam-worker service.start pdm run suam-worker service.stop pdm run suam-worker service.restart pdm run suam-worker service.status Talk about config file broker.conf.yaml and fields: Broker config Backend config Talk about config file d2c_worker.conf.yaml and fields: Worker Node Opts Service config Below Info Out of Date reqs Workers: Configuring broker and backend Configure use as service Remote task processing A worker will listen for tasks from a message broker and then run those tasks. Configure Worker Settings \u00b6 Configure the message broker settings in <config-dir>/broker.conf.yaml . Configure the settings in <config-dir>/d2c_worker.conf.yaml . Examine loaded config with pdm run suam-config print --config=d2c_worker -r Run Worker Process \u00b6 A worker node, which listens to a broker and performs tasks as requests come in must have been set up as here . Make sure that direct2cad workspaces have been properly set up on this node as described here . That code is what the worker process uses to evaluate a task, so if it doesn't function locally it won't function when a remote client asks. Make sure that a message broker is running at the configured host and port. Actually run the worker process. In admin powershell at <repo-root> . Run: pdm run suam-worker worker.run Note that this is a process not a service. If it shuts down it needs to be restarted.","title":"Workers"},{"location":"usage/workers/#direct2cad-worker","text":"Section Incomplete Worker runs as a service and listens for tasks from msg broker. When task arrives uses the D2C Workspaces code to run the analysis and produce a result archive. Talk about broker vs backend. Broker gets message from client to worker. Backend gets notification of completion from worker to client. (Really just tells you the name of the zip file from a particular request.) RabbitMQ: Broker only Redis: Both Broker and Backend Supports running worker node as normal process: Normal Process: pdm run suam-worker run Supports running worker node as a service managed by NSSM. The commands do the obvious things. pdm run suam-worker service.install pdm run suam-worker service.uninstall pdm run suam-worker service.configure : Converts config file settings to NSSM settings. pdm run suam-worker service.start pdm run suam-worker service.stop pdm run suam-worker service.restart pdm run suam-worker service.status Talk about config file broker.conf.yaml and fields: Broker config Backend config Talk about config file d2c_worker.conf.yaml and fields: Worker Node Opts Service config Below Info Out of Date reqs Workers: Configuring broker and backend Configure use as service Remote task processing A worker will listen for tasks from a message broker and then run those tasks.","title":"Direct2Cad Worker"},{"location":"usage/workers/#configure-worker-settings","text":"Configure the message broker settings in <config-dir>/broker.conf.yaml . Configure the settings in <config-dir>/d2c_worker.conf.yaml . Examine loaded config with pdm run suam-config print --config=d2c_worker -r","title":"Configure Worker Settings"},{"location":"usage/workers/#run-worker-process","text":"A worker node, which listens to a broker and performs tasks as requests come in must have been set up as here . Make sure that direct2cad workspaces have been properly set up on this node as described here . That code is what the worker process uses to evaluate a task, so if it doesn't function locally it won't function when a remote client asks. Make sure that a message broker is running at the configured host and port. Actually run the worker process. In admin powershell at <repo-root> . Run: pdm run suam-worker worker.run Note that this is a process not a service. If it shuts down it needs to be restarted.","title":"Run Worker Process"},{"location":"usage/workspaces/","text":"Direct2Cad Workspace \u00b6 A workspace is an isolated environment for SWRi's direct2cad scripts to run in. Various tasks will run in a workspace and have their results bundled into a zip file for use by clients. Basic Workspace Operation \u00b6 In order to describe how operations occur in a workspace we need to define a few concepts: Workspace : An isolated environment, in practice a file folder, in which direct2cad operations are run. Session : A set of operations, python and console commands, that are run within a workspace to do something, usually processing a design with direct2cad. Reference Workspace : This is a static, read-only workspace which is used to initialize a session. Active Workspace : The workspace that is currently running a session. Result Archive : A zip file that contains all the output from a session. The flow is as follows: sequenceDiagram autonumber participant S as Session participant A as Active<br/>Workspace participant R as Reference<br/>Workspace participant O as Results<br/>Directory S --> A : Acquire<br/>Lock activate A R ->> A : Reset Active<br>Workspace Note over S,A: Perform session operations, e.g:<br/>gen info files, start Creo,<br/>run FDM, etc.. S --> A : Write \"metadata.json\"<br/>to Active Workspace R ->> A : Compare<br/>Workspaces A ->> O : Write Changed Files<br/>to Result Archive A --> S : Release<br/>Lock deactivate A Acquire Lock : Each active workspace has a lock that prevents multiple processes and threads from using it at simultaneously. The session acquires the lock for the first available workspace or fails with an error if none are available. Reset Workspace : We use rsync to make sure the active workspace is a perfect copy of the reference workspace for the various session operations. Write metadata.json : The metadata.json file will end up in the result archive, with information about the worker node, the specific workspace being used, and whatever metadata the session was given when it was initialized. Compare Workspaces : Use rsync to get a list of files that have changed relative to the reference workspace. Write Results : Make zip archive with all the changes and place it in the results directory. Release Lock : Allow other sessions or processes to use the workspace again. The Reference Workspace \u00b6 Run the following to initialize or update the reference workspace: pdm run d2c-workspace setup.reference-workspace This shouldn't be run when there are any ongoing sessions. Provided Sessions \u00b6 There are three basic sessions provided with SimpleUAM, though more can be added by modifying simple_uam.direct2cad.D2CSession . Start Creo \u00b6 Starts Creo and the creoson server on the worker. This is mostly useful for testing, the other sessions that need to start Creo will do it automatically. Run with the following: pdm run d2c-workspace tasks.start-creo A mostly empty result archive will be written to the results directory when the session is finished. Arguments: --output=FILE : Write the output metadata to FILE . Metadata will be printed to console if not provided. --workspace=NUM : Use workspace NUM if available. The first available workspace will be used if not provided. Generate Info Files \u00b6 Uses the craidl code to generate info files for a design albeit within a session that writes the results to an archive in the results directory. With a design_swri.json at <design-loc> , run with: pdm run d2c-workspace tasks.gen-info-files --input = <design-loc> The output result archive will contain: metadata.json : Assorted metadata from this session. design_swri.json : The design file. info_*.json : Input files generated by craidl. Arguments: --output=FILE : Write the output metadata to FILE . Metadata will be printed to console if not provided. --workspace=NUM : Use workspace NUM if available. The first available workspace will be used if not provided. Process Design \u00b6 This processes an input design with the direct2cad pipeline. With a design_swri.json at <design-loc> , run with: pdm run d2c-workspace tasks.gen-info-files --input = <design-loc> The output result archive will contain: The additional files from gen-info-files . Various CAD files and outputs from the direct2cad pipeline. buildcad.stdout : The output stream from buildCad.py . buildcad.stderr : The error stream from buildCad.py . Arguments: --output=FILE : Write the output metadata to FILE . Metadata will be printed to console if not provided. --workspace=NUM : Use workspace NUM if available. The first available workspace will be used if not provided. Workspace Management \u00b6 Get the cache directory for these workspaces: pdm run d2c-workspace manage.cache-dir Get the root directory of the reference workspace and the live workspaces: pdm run d2c-workspace manage.workspaces-dir Get the records directory for these workspaces. This is where the zip files with data from each session will go. Run with: pdm run d2c-workspace manage.records-dir Prune the records directory by deleting old records until only the configured maximum are present. It will not delete records that are newer than the configured min_staletime . Run with: pdm run d2c-workspace manage.prune-records Delete any file system locks that might have gotten left behind. These usually prevent multiple processes from taking control of the same live workspace. Run with: pdm run d2c-workspace manage.delete-locks `","title":"Workspaces"},{"location":"usage/workspaces/#direct2cad-workspace","text":"A workspace is an isolated environment for SWRi's direct2cad scripts to run in. Various tasks will run in a workspace and have their results bundled into a zip file for use by clients.","title":"Direct2Cad Workspace"},{"location":"usage/workspaces/#operation","text":"In order to describe how operations occur in a workspace we need to define a few concepts: Workspace : An isolated environment, in practice a file folder, in which direct2cad operations are run. Session : A set of operations, python and console commands, that are run within a workspace to do something, usually processing a design with direct2cad. Reference Workspace : This is a static, read-only workspace which is used to initialize a session. Active Workspace : The workspace that is currently running a session. Result Archive : A zip file that contains all the output from a session. The flow is as follows: sequenceDiagram autonumber participant S as Session participant A as Active<br/>Workspace participant R as Reference<br/>Workspace participant O as Results<br/>Directory S --> A : Acquire<br/>Lock activate A R ->> A : Reset Active<br>Workspace Note over S,A: Perform session operations, e.g:<br/>gen info files, start Creo,<br/>run FDM, etc.. S --> A : Write \"metadata.json\"<br/>to Active Workspace R ->> A : Compare<br/>Workspaces A ->> O : Write Changed Files<br/>to Result Archive A --> S : Release<br/>Lock deactivate A Acquire Lock : Each active workspace has a lock that prevents multiple processes and threads from using it at simultaneously. The session acquires the lock for the first available workspace or fails with an error if none are available. Reset Workspace : We use rsync to make sure the active workspace is a perfect copy of the reference workspace for the various session operations. Write metadata.json : The metadata.json file will end up in the result archive, with information about the worker node, the specific workspace being used, and whatever metadata the session was given when it was initialized. Compare Workspaces : Use rsync to get a list of files that have changed relative to the reference workspace. Write Results : Make zip archive with all the changes and place it in the results directory. Release Lock : Allow other sessions or processes to use the workspace again.","title":"Basic Workspace Operation"},{"location":"usage/workspaces/#reference","text":"Run the following to initialize or update the reference workspace: pdm run d2c-workspace setup.reference-workspace This shouldn't be run when there are any ongoing sessions.","title":"The Reference Workspace"},{"location":"usage/workspaces/#sessions","text":"There are three basic sessions provided with SimpleUAM, though more can be added by modifying simple_uam.direct2cad.D2CSession .","title":"Provided Sessions"},{"location":"usage/workspaces/#session-creo","text":"Starts Creo and the creoson server on the worker. This is mostly useful for testing, the other sessions that need to start Creo will do it automatically. Run with the following: pdm run d2c-workspace tasks.start-creo A mostly empty result archive will be written to the results directory when the session is finished. Arguments: --output=FILE : Write the output metadata to FILE . Metadata will be printed to console if not provided. --workspace=NUM : Use workspace NUM if available. The first available workspace will be used if not provided.","title":"Start Creo"},{"location":"usage/workspaces/#session-gen-info","text":"Uses the craidl code to generate info files for a design albeit within a session that writes the results to an archive in the results directory. With a design_swri.json at <design-loc> , run with: pdm run d2c-workspace tasks.gen-info-files --input = <design-loc> The output result archive will contain: metadata.json : Assorted metadata from this session. design_swri.json : The design file. info_*.json : Input files generated by craidl. Arguments: --output=FILE : Write the output metadata to FILE . Metadata will be printed to console if not provided. --workspace=NUM : Use workspace NUM if available. The first available workspace will be used if not provided.","title":"Generate Info Files"},{"location":"usage/workspaces/#session-process-design","text":"This processes an input design with the direct2cad pipeline. With a design_swri.json at <design-loc> , run with: pdm run d2c-workspace tasks.gen-info-files --input = <design-loc> The output result archive will contain: The additional files from gen-info-files . Various CAD files and outputs from the direct2cad pipeline. buildcad.stdout : The output stream from buildCad.py . buildcad.stderr : The error stream from buildCad.py . Arguments: --output=FILE : Write the output metadata to FILE . Metadata will be printed to console if not provided. --workspace=NUM : Use workspace NUM if available. The first available workspace will be used if not provided.","title":"Process Design"},{"location":"usage/workspaces/#manage","text":"Get the cache directory for these workspaces: pdm run d2c-workspace manage.cache-dir Get the root directory of the reference workspace and the live workspaces: pdm run d2c-workspace manage.workspaces-dir Get the records directory for these workspaces. This is where the zip files with data from each session will go. Run with: pdm run d2c-workspace manage.records-dir Prune the records directory by deleting old records until only the configured maximum are present. It will not delete records that are newer than the configured min_staletime . Run with: pdm run d2c-workspace manage.prune-records Delete any file system locks that might have gotten left behind. These usually prevent multiple processes from taking control of the same live workspace. Run with: pdm run d2c-workspace manage.delete-locks `","title":"Workspace Management"}]}